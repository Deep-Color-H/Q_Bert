{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qbert_model(vocab_size, num_layers, dff, d_model, num_heads, dropout, name = 'qbert'):\n",
    "\n",
    "    input = tf.keras.layers.Input(shape = (None, ), name = 'inputs')\n",
    "\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = 'padding_mask')\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(input)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    print(embeddings.shape)\n",
    "    pos_layer = tf.keras.layers.Po\n",
    "    embeddings += tf.kears.layers.add(embeddings, pos_layer)\n",
    "    outputs = tf.keras.layers.Dropout(rate = dropout)(embeddings)\n",
    "\n",
    "\n",
    "#     for i in range(num_layers) :\n",
    "\n",
    "#         outputs = encoder_layer(dff = dff\n",
    "#                                 , d_model=d_model\n",
    "#                                 , num_heads = num_heads\n",
    "#                                 , dropout = dropout\n",
    "#                                 , name = 'encoding_layer_{}'.format(i))([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs = [input, padding_mask], outputs = outputs, name = name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"Creates a positional embedding.\n",
    "    Example:\n",
    "    ```python\n",
    "    position_embedding = PositionEmbedding(max_length=100)\n",
    "    inputs = tf.keras.Input((100, 32), dtype=tf.float32)\n",
    "    outputs = position_embedding(inputs)\n",
    "    ```\n",
    "    Args:\n",
    "    max_length: The maximum size of the dynamic sequence.\n",
    "    initializer: The initializer to use for the embedding weights. Defaults to\n",
    "      \"glorot_uniform\".\n",
    "    seq_axis: The axis of the input tensor where we add the embeddings.\n",
    "    Reference: This layer creates a positional embedding as described in\n",
    "    [BERT: Pre-training of Deep Bidirectional Transformers for Language\n",
    "    Understanding](https://arxiv.org/abs/1810.04805).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_length, initializer=\"glorot_uniform\", seq_axis=1,  **kwargs):\n",
    "\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        \n",
    "        if max_length is None:\n",
    "            raise ValueError(\"`max_length` must be an Integer, not `None`.\")\n",
    "        \n",
    "        self._max_length = max_length\n",
    "        self._initializer = tf.keras.initializers.get(initializer)\n",
    "        self._seq_axis = seq_axis\n",
    "\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"max_length\": self._max_length,\n",
    "            \"initializer\": tf.keras.initializers.serialize(self._initializer),\n",
    "            \"seq_axis\": self._seq_axis,\n",
    "        }\n",
    "        base_config = super(PositionEmbedding, self).get_config()\n",
    "    \n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "#         print(input_shape)\n",
    "        dimension_list = input_shape.as_list()\n",
    "\n",
    "        seq_length = dimension_list[self._seq_axis]\n",
    "        width = dimension_list[-1]\n",
    "\n",
    "        if self._max_length is not None:\n",
    "            weight_sequence_length = self._max_length\n",
    "        else:\n",
    "            weight_sequence_length = seq_length\n",
    "\n",
    "        self._position_embeddings = self.add_weight(\"embeddings\", shape=[weight_sequence_length, width], initializer=self._initializer)\n",
    "\n",
    "        super(PositionEmbedding, self).build(input_shape)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "#         print(\"CALL\")\n",
    "        print(inputs)\n",
    "        input_shape = tf.shape(inputs)\n",
    "        print(input_shape)\n",
    "        print(input_shape[1])\n",
    "        actual_seq_len = input_shape[self._seq_axis]\n",
    "        position_embeddings = self._position_embeddings[:actual_seq_len, :]\n",
    "        \n",
    "        new_shape = [1 for _ in inputs.get_shape().as_list()]\n",
    "        new_shape[self._seq_axis] = actual_seq_len\n",
    "        new_shape[-1] = position_embeddings.get_shape().as_list()[-1]\n",
    "        \n",
    "        print(new_shape)\n",
    "        \n",
    "        position_embeddings = tf.reshape(position_embeddings, new_shape)\n",
    "        \n",
    "        return tf.broadcast_to(position_embeddings, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_16:0\", shape=(None, 255, 50), dtype=float32)\n",
      "Tensor(\"position_embedding_11/Shape:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"position_embedding_11/strided_slice:0\", shape=(), dtype=int32)\n",
      "[1, <tf.Tensor 'position_embedding_11/strided_slice_1:0' shape=() dtype=int32>, 50]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'position_embedding_11/Identity:0' shape=(None, 255, 50) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PositionEmbedding(255)(keras.layers.Input((255, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
