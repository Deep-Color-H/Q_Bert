{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"Creates a positional embedding.\n",
    "    Example:\n",
    "    ```python\n",
    "    position_embedding = PositionEmbedding(max_length=100)\n",
    "    inputs = tf.keras.Input((100, 32), dtype=tf.float32)\n",
    "    outputs = position_embedding(inputs)\n",
    "    ```\n",
    "    Args:\n",
    "    max_length: The maximum size of the dynamic sequence.\n",
    "    initializer: The initializer to use for the embedding weights. Defaults to\n",
    "      \"glorot_uniform\".\n",
    "    seq_axis: The axis of the input tensor where we add the embeddings.\n",
    "    Reference: This layer creates a positional embedding as described in\n",
    "    [BERT: Pre-training of Deep Bidirectional Transformers for Language\n",
    "    Understanding](https://arxiv.org/abs/1810.04805).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_length, initializer=\"glorot_uniform\", seq_axis=1,  **kwargs):\n",
    "\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        \n",
    "        if max_length is None:\n",
    "            raise ValueError(\"`max_length` must be an Integer, not `None`.\")\n",
    "        \n",
    "        self._max_length = max_length\n",
    "        self._initializer = tf.keras.initializers.get(initializer)\n",
    "        self._seq_axis = seq_axis\n",
    "\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"max_length\": self._max_length,\n",
    "            \"initializer\": tf.keras.initializers.serialize(self._initializer),\n",
    "            \"seq_axis\": self._seq_axis,\n",
    "        }\n",
    "        base_config = super(PositionEmbedding, self).get_config()\n",
    "    \n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        dimension_list = input_shape.as_list()\n",
    "\n",
    "        seq_length = dimension_list[self._seq_axis]\n",
    "        width = dimension_list[-1]\n",
    "\n",
    "        if self._max_length is not None:\n",
    "            weight_sequence_length = self._max_length\n",
    "        else:\n",
    "            weight_sequence_length = seq_length\n",
    "\n",
    "        self._position_embeddings = self.add_weight(\"embeddings\", shape=[weight_sequence_length, width], initializer=self._initializer)\n",
    "\n",
    "        super(PositionEmbedding, self).build(input_shape)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        actual_seq_len = input_shape[self._seq_axis]\n",
    "        position_embeddings = self._position_embeddings[:actual_seq_len, :]\n",
    "        \n",
    "        new_shape = [1 for _ in inputs.get_shape().as_list()]\n",
    "        new_shape[self._seq_axis] = actual_seq_len\n",
    "        new_shape[-1] = position_embeddings.get_shape().as_list()[-1]\n",
    "        \n",
    "        position_embeddings = tf.reshape(position_embeddings, new_shape)\n",
    "        \n",
    "        return tf.broadcast_to(position_embeddings, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer) :\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, name = 'multi_head_attention') :\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__(name = name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.depth = d_model // num_heads\n",
    "        assert d_model == (num_heads * self.depth)\n",
    "        \n",
    "        self.w_q = tf.keras.layers.Dense(self.d_model)\n",
    "        self.w_k = tf.keras.layers.Dense(self.d_model)\n",
    "        self.w_v = tf.keras.layers.Dense(self.d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_head(self, l, batch_size) :\n",
    "        outputs = tf.reshape(l, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(outputs, perm = [0, 2, 1, 3])\n",
    "    \n",
    "    def scaled_dot_product(self, query, key, value, mask) :\n",
    "        \n",
    "        d_k = tf.cast(self.depth, dtype = tf.float32)\n",
    "        \n",
    "        dot_score = tf.matmul(query, key, transpose_b = True) / tf.math.sqrt(d_k / self.num_heads)\n",
    "        \n",
    "        if mask is not None :\n",
    "            dot_score += mask * -1e9\n",
    "        \n",
    "        attention_score = tf.nn.softmax(dot_score)\n",
    "        outputs = tf.matmul(attention_score, value)\n",
    "        \n",
    "        return outputs, attention_score\n",
    "    \n",
    "    \n",
    "    def call(self, inputs) :\n",
    "        \n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        \n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        \n",
    "        # inputs : (batch, seq_len, d_model)\n",
    "        query = self.w_q(query)\n",
    "        key   = self.w_k(key)\n",
    "        value = self.w_v(value)\n",
    "        \n",
    "        # q, k, v\n",
    "        # (batch, seq_len, d_model) -> (batch, num_heads, seq_len, depth)\n",
    "        query = self.split_head(query, batch_size)\n",
    "        key = self.split_head(key, batch_size)\n",
    "        value = self.split_head(value, batch_size)\n",
    "        \n",
    "        # scaled_dot_product\n",
    "        outputs, _ = self.scaled_dot_product(query, key, value, mask)\n",
    "        \n",
    "        outputs = tf.reshape(outputs, (batch_size, -1, self.d_model))\n",
    "        outputs = self.dense(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name = 'encoder_layer') :\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name = 'inputs')\n",
    "\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = 'padding_mask')\n",
    "\n",
    "    attention = MultiHeadAttention(d_model = d_model, num_heads = num_heads)({\n",
    "        'query' : inputs, 'key' : inputs, 'value' : inputs,\n",
    "        'mask': padding_mask\n",
    "    })\n",
    "\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units = dff, activation = custom_gelu)(attention)\n",
    "    outputs = tf.keras.layers.Dense(units = d_model)(outputs)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs = [inputs, padding_mask], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qbert_model(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name = 'qbert'):\n",
    "\n",
    "    input = tf.keras.Input(shape = (None, ), name = 'inputs')\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = 'padding_mask')\n",
    "    segments = tf.keras.Input(shape = (None, ), name = 'segments')\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(input)\n",
    "    embeddings += PositionEmbedding(max_seq_len)(embeddings)\n",
    "    embeddings += tf.keras.layers.Embedding(3, d_model)(segments) # sentence A or sentence B\n",
    "    \n",
    "    output = tf.keras.layers.Dropout(rate = dropout)(embeddings)\n",
    "    # LayerNorm +\n",
    "    \n",
    "    encode_outputs = []\n",
    "    for i in range(num_layers) :\n",
    "\n",
    "        output = encoder_layer(dff = dff\n",
    "                                , d_model=d_model\n",
    "                                , num_heads = num_heads\n",
    "                                , dropout = dropout\n",
    "                                , name = 'encoding_layer_{}'.format(i))([output, padding_mask])\n",
    "        \n",
    "        # pooler_layer \n",
    "        if i == 0 :\n",
    "            pooler_output = tf.keras.layers.Dense(d_model)(output)\n",
    "            outputs['pooled_output'] = pooler_output\n",
    "            \n",
    "        encode_outputs.append(output)\n",
    "    \n",
    "    outputs['sequence_output'] = encode_outputs[-1]\n",
    "    outputs['hidden_states'] = encode_outputs\n",
    "    \n",
    "    return tf.keras.Model(inputs = [input, padding_mask, segments], outputs = outputs, name = name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST CODE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def save_pkl(df, file_path) :\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_padding_mask(x):\n",
    "    init_shape = x.shape\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, key의 문장 길이)\n",
    "    return np.array(mask).reshape(init_shape[0], 1,1, init_shape[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train = load_pkl('./dt/train_set_under_255.pkl')\n",
    "train = load_pkl('./dt/train_set-maksed-position-sample-10000.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vocab_size = 32000\n",
    "max_seq_len = 130\n",
    "num_layers = 3\n",
    "dff = 256\n",
    "d_model = 100\n",
    "num_heads = 5\n",
    "dropout = .1\n",
    "name = 'qbert_210603'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def make_segments(inputs) :\n",
    "    \n",
    "    segment = []\n",
    "    segment_num = 0\n",
    "    \n",
    "    for i, x in enumerate(inputs) :\n",
    "        \n",
    "        segment.append(segment_num)\n",
    "        \n",
    "        if x == 3 :\n",
    "            segment_num+=1\n",
    "            \n",
    "    return np.array(segment)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "segments = np.array([make_segments(x['x']) for x in train ])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "segments = make_segments(train[0]['x'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sg = tf.keras.layers.Embedding(2, 50)(segments)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bert_model = qbert_model(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_input = tf.reshape(train[0]['x'], (1, -1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_mask = create_padding_mask(test_input)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_segments = make_segments(train[0]['x'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = bert_model([test_input, test_mask, test_segments])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
