{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import train_utils, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 256 # 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['./Test_Examples2.tfrecords' ]\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "# keys = feature_description.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "def _select_data_from_record(record):\n",
    "    \"\"\"Filter out features to use for pretraining.\"\"\"\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids'],\n",
    "        'masked_lm_positions': record['masked_lm_positions'],\n",
    "        'masked_lm_ids': record['masked_lm_ids'],\n",
    "        'masked_lm_weights': record['masked_lm_weights'],\n",
    "    }\n",
    "    if use_next_sentence_label:\n",
    "        x['next_sentence_labels'] = record['next_sentence_labels']\n",
    "    if use_position_id:\n",
    "        x['position_ids'] = record['position_ids']\n",
    "\n",
    "    # TODO(hongkuny): Remove the fake labels after migrating bert pretraining.\n",
    "    if output_fake_labels:\n",
    "        return (x, record['masked_lm_weights'])\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "GLOBAL_BATCH_SIZE = 4\n",
    "#BATCH_SIZE_PER_REPLICA = np.ceil(GLOBAL_BATCH_SIZE // strategy.num_replicas_in_sync)\n",
    "\n",
    "use_next_sentence_label = True\n",
    "output_fake_labels = True\n",
    "use_position_id = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "train_dataset = train_dataset.interleave(tf.data.TFRecordDataset, cycle_length = -1)\n",
    "\n",
    "dataset_inputs = train_dataset.map(_parse_function) # String to Example\n",
    "dataset_inputs_with_labels = dataset_inputs.map(_select_data_from_record) # Example to InputData\n",
    "## 본래대로라면 그냥 써도 되지만, 현재 Label이 없는 데이터이기 때문에\n",
    "## max_predictions_per_seq 길이의 허위 정답 (Fake_y)를 삽입하는 mapping function이다.\n",
    "\n",
    "dataset = dataset_inputs_with_labels\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(10000, reshuffle_each_iteration = True)\n",
    "dataset = dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3130,  2357,  3052,  6005,   738, 24311,  3113, 13649,  3191,\n",
       "       13370,    17, 31622,  5741,  3091, 11627,  3209,  5777,  5579,\n",
       "        2987,  6275,  4189,  6007,  8667, 29452,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,    17, 24783,    15,  6997,  5621,\n",
       "       11406,  7612,    15,  9892, 28545,  9892,   170,  5795, 17950,\n",
       "        9892,  8686,  1779,  3214,  7719,  1779,  3053,  7090, 14161,\n",
       "         560, 13864, 19648,    15, 14883, 14883,  3121, 14883,  3130,\n",
       "       17329,     0,     0,     0,     0,     0,     0,     0,  2009,\n",
       "        5651,  3238, 20114, 16151,  5726, 17192,    17,  6731,  6879,\n",
       "        5603,  3130,  9855, 10815,  9064,  1850, 21296,   178,  6192,\n",
       "        4150,  6516, 24908,  6616,  1573,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,  6703,  5584, 17898,  7471,    15,  5651,\n",
       "        3130,  7070, 11367,  3012, 16545,   610,  3147,  3130,  6344,\n",
       "        5863,   544, 23045, 22071,    13, 19330, 18191,  1836,    17,\n",
       "          82,  6703,  5590, 20366, 15760,    15, 31806,    12,  3236,\n",
       "       10703, 20091,    15,  6119,     6,     0,     0], dtype=int64)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(data['masked_lm_ids'], (-1)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, datas in enumerate(dataset) :\n",
    "    \n",
    "    data, labels = datas\n",
    "    \n",
    "    word_count.update(Counter(tf.reshape(data['masked_lm_ids'], (-1)).numpy()))\n",
    "    \n",
    "    if i == 100 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 73\n"
     ]
    }
   ],
   "source": [
    "for key, values in word_count.items():\n",
    "    \n",
    "    if values == 73 :\n",
    "        print(key, values)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6927,\n",
       " 436,\n",
       " 293,\n",
       " 93,\n",
       " 73,\n",
       " 68,\n",
       " 64,\n",
       " 63,\n",
       " 57,\n",
       " 53,\n",
       " 49,\n",
       " 43,\n",
       " 41,\n",
       " 39,\n",
       " 37,\n",
       " 36,\n",
       " 35,\n",
       " 34,\n",
       " 33,\n",
       " 32,\n",
       " 30,\n",
       " 28,\n",
       " 28,\n",
       " 27,\n",
       " 26,\n",
       " 24,\n",
       " 23,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_count.values())[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-3000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ds[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "ina = {}\n",
    "for key in inputs.keys():\n",
    "    ina[key] = inputs[key][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_input = tokenizer_for_load.convert_ids_to_tokens(ina['input_ids'])\n",
    "t_output = tokenizer_for_load.convert_ids_to_tokens(ina['masked_lm_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '여름에', '열린', '고시엔', '대회의', '준', '##준', '##결승', '##전에서는', '하타', '##야마', '히토', '##시', ',', '미즈', '##노', '가쓰', '##히토', '[MASK]', '소속된', '[MASK]', '현립', '[MASK]', '고등학교', '##와', '대결', '##하였지만', '선발', '투수', '##인', '아라', '##키와', '구원', '투수', '##인', '이시', '##이가', '모두', '부진', '##하면서', '2대', '14', '##로', '크게', '패했다', '.', '학', '##년을', '늘', '##릴', '때마다', '고시엔', '대회에서', '성적이', '저하', '##했던', '것에', '대해', '아라', '##키는', '\"', '1학년', '때에는', '힘이', '없었기', '때문에', '타자', '##의', '손', '[MASK]', '##림에서', '성장', '##하지', '[MASK]', '자연스럽게', '[MASK]', '##은', '공이', '3학년', '[MASK]', '그대로', '성장', '##해', '타자', '##에게', '있어서의', '치', '[MASK]', '좋은', '무렵', '##이', '되었다', '[MASK]', '라고', '분석', '##했다', '.', '프로', '입단', '후', '.', '1982년', '가을에', '[MASK]', '프로', '야구', '드래프트', '회의에서', '##는', '야쿠르트', '스', '##왈', '##로스', '##와', '[MASK]', '[MASK]', '##가', '[MASK]', '##순위로', '지명', '##하면서', '추첨', '결과', '야쿠르트', '##가', '교섭', '##권을', '획득했다', '[SEP]', '[MASK]', '인기를', '얻', '##으면서', '아라', '##키를', '둘러싸', '##는', '팬들의', '혼란을', '##소에', '위해', '홈구', '##장인', '메이지', '[MASK]', '##구', '야구장', '[MASK]', '클럽', '하우스', '[MASK]', '구장', '##을', '연결하는', '[MASK]', '##도가', '[MASK]', '.', '이것은', '일명', \"'\", '[MASK]', '##키', '[MASK]', \"'\", '이라고', '불리는', '등', '여의', '야쿠르트', '##의', '선수가', '구장으로', '향', '##할', '때에', '사용하는', '경우가', '있다', '.', '프로', '[MASK]', '##차', '##인', '1985년', '후반부터', '선발', '로', '[MASK]', '들어가', '1986년에', '##는', '개막전', '선발', '투수', '##를', '##슬링', '.', '올스타전', '팬', '[MASK]', '1위로', '선정된', '같은', '해', '[MASK]', '##에서는', '[MASK]', '##전에', '[MASK]', '등판해', '타자', '대동', '##명과', '[MASK]', '##하면서', '1', '##안타', '무실', '##점으로', '호', '##투', '##했다', '.', '이듬해인', '1987년에', '##는', '[MASK]', '##와', '마찬가지로', '개막전', '선발', '투수로', '등판', '##한', '것', '외에도', '시즌', '10', '##승을', '올리는', '등', '야쿠르트', '##의', '중심', '투수로', '[MASK]', '[MASK]', '##했지만', '다음해', '##인', '1988년', '시즌', '중반에', '팔꿈치', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(ina['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 \t:[MASK]\t vs\t제1차\n",
      "144 \t:[MASK]\t vs\t진\n",
      "247 \t:[MASK]\t vs\t활약\n",
      "129 \t:[MASK]\t vs\t절대적인\n",
      "139 \t:##소에\t vs\t피하기\n",
      "246 \t:[MASK]\t vs\t##서\n",
      "79 \t:[MASK]\t vs\t때는\n",
      "150 \t:[MASK]\t vs\t##와\n",
      "22 \t:[MASK]\t vs\t이케다\n",
      "117 \t:[MASK]\t vs\t1\n",
      "214 \t:[MASK]\t vs\t상대\n",
      "196 \t:##슬링\t vs\t맡았다\n",
      "69 \t:[MASK]\t vs\t##놀\n",
      "250 \t:##인\t vs\t##인\n",
      "188 \t:[MASK]\t vs\t##테이션\n",
      "168 \t:여의\t vs\t지금도\n",
      "163 \t:[MASK]\t vs\t터널\n",
      "181 \t:[MASK]\t vs\t3년\n",
      "115 \t:[MASK]\t vs\t자이언츠\n",
      "114 \t:[MASK]\t vs\t요미우리\n",
      "209 \t:[MASK]\t vs\t선발\n",
      "75 \t:[MASK]\t vs\t가라앉\n",
      "161 \t:[MASK]\t vs\t아라\n",
      "212 \t:대동\t vs\t10\n",
      "205 \t:[MASK]\t vs\t올스타전\n",
      "154 \t:[MASK]\t vs\t지하\n",
      "19 \t:소속된\t vs\t소속된\n",
      "20 \t:[MASK]\t vs\t도쿠시마\n",
      "87 \t:[MASK]\t vs\t##기\n",
      "147 \t:[MASK]\t vs\t##의\n",
      "73 \t:[MASK]\t vs\t못하고\n",
      "200 \t:[MASK]\t vs\t투표\n",
      "80 \t:그대로\t vs\t그대로\n",
      "227 \t:[MASK]\t vs\t전년도\n",
      "92 \t:[MASK]\t vs\t\"\n",
      "18 \t:[MASK]\t vs\t등이\n",
      "103 \t:[MASK]\t vs\t열린\n",
      "156 \t:[MASK]\t vs\t만들어졌다\n",
      "0 \t:[CLS]\t vs\t[PAD]\n",
      "0 \t:[CLS]\t vs\t[PAD]\n"
     ]
    }
   ],
   "source": [
    "for i, m_p in enumerate(ina['masked_lm_positions']) :\n",
    "    print(\"{} \\t:{}\\t vs\\t{}\".format(m_p,t_input[m_p], t_output[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 94 100  76 122  99  87  65  54  67  32  21  18   5 116  43  11 102   1\n",
      " 108   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0], shape=(40,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(ina['masked_lm_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['남부의', '하여', '전쟁', '.', '수도로', '##탱', '그리하여', '아르', '대성', '보냈다', '이때', '공방', '는', '##시', '##수', '이름', '프랑스', '\"', '11월', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(ina['masked_lm_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(ina['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "## Learning Rate Decay\n",
    "\n",
    "# lr = 1e-4 warmup stage (step <= 10000)\n",
    "# Decay linearly\n",
    "\n",
    "init_lr = 1e-4\n",
    "warmup_steps = 10000\n",
    "num_train_steps = 20 * 50000\n",
    "end_lr = 0\n",
    "\n",
    "# decay_schedule_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "#       initial_learning_rate=init_lr,\n",
    "#       decay_steps=num_train_steps,\n",
    "#       end_learning_rate=end_lr)\n",
    "\n",
    "# lr_schedule = train_utils.WarmUp(\n",
    "#         initial_learning_rate=init_lr,\n",
    "#         decay_schedule_fn=decay_schedule_fn,\n",
    "#         warmup_steps=warmup_steps)\n",
    "\n",
    "# optimizer = train_utils.AdamWeightDecay( \n",
    "#     learning_rate=lr_schedule,\n",
    "#     weight_decay_rate=0.01,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-6,\n",
    "#     exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = init_lr, beta_1 = 0.9, beta_2 = 0.999, epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def loss_fn(fake_y, losses, **unused_args) :\n",
    "    \n",
    "    return tf.reduce_mean(losses, axis = -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, sub_model = models.get_bert_models_fn(vocab_size\n",
    "                                         , hidden_size\n",
    "                                         , type_vocab_size\n",
    "                                         , num_layers\n",
    "                                         , num_attention_heads\n",
    "                                         , max_seq_length\n",
    "                                         , max_predictions_per_seq\n",
    "                                         , dropout_rate\n",
    "                                         , inner_dim \n",
    "                                         , initializer)\n",
    "model.compile(optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25988eb6860>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(filepath='./training_checkpoints_load/ckpt_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_res = sub_model([inputs['input_ids'], inputs['input_mask'][:, tf.newaxis, tf.newaxis, :], inputs['segment_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_x = model.layers[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = layer_x([sub_res['sequence_output'], tf.cast(inputs['masked_lm_positions'], dtype = tf.int32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_output, masked_lm_positions = sub_res['sequence_output'], tf.cast(inputs['masked_lm_positions'], dtype = tf.int32)\n",
    "        \n",
    "sequence_shape = tf.shape(seg_output) # [batch_size, seq_length, dff]\n",
    "batch_size, seq_length = sequence_shape[0], sequence_shape[1] \n",
    "width = seg_output.shape.as_list()[2] or sequence_shape[2]\n",
    "\n",
    "flat_offsets = tf.reshape(\n",
    "    tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "flat_positions = tf.reshape(masked_lm_positions + flat_offsets, [-1])\n",
    "flat_sequence_tensor = tf.reshape(seg_output,\n",
    "                                  [batch_size * seq_length, width])\n",
    "gathered_tensor = tf.gather(flat_sequence_tensor, flat_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(160, 768), dtype=float32, numpy=\n",
       "array([[ 0.05825796, -0.15728351,  0.03383664, ...,  0.21044591,\n",
       "        -0.20374396,  0.01895263],\n",
       "       [ 0.04630796, -0.35663825, -0.02367768, ...,  0.2625608 ,\n",
       "        -0.26293308,  0.00389194],\n",
       "       [ 0.02822801, -0.03218177,  0.09385708, ...,  0.10875528,\n",
       "        -0.03896865,  0.0520727 ],\n",
       "       ...,\n",
       "       [ 0.03363404, -0.9710863 ,  0.1194402 , ...,  0.60961324,\n",
       "         0.01628826, -0.10343488],\n",
       "       [ 0.01762544, -0.1050678 , -0.0690262 , ...,  0.04092298,\n",
       "        -0.02420309, -0.04515424],\n",
       "       [ 0.01762544, -0.1050678 , -0.0690262 , ...,  0.04092298,\n",
       "        -0.02420309, -0.04515424]], dtype=float32)>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gathered_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 256, 768), dtype=float32, numpy=\n",
       "array([[[ 0.02781948, -0.02764396,  0.09445661, ...,  0.10266508,\n",
       "         -0.03675129,  0.05366931],\n",
       "        [ 0.02822801, -0.03218177,  0.09385708, ...,  0.10875528,\n",
       "         -0.03896865,  0.0520727 ],\n",
       "        [ 0.02795909, -0.03357469,  0.09306844, ...,  0.11037768,\n",
       "         -0.04007384,  0.05098892],\n",
       "        ...,\n",
       "        [ 0.45052794, -0.60000634,  0.5118584 , ...,  0.76278085,\n",
       "         -0.37980098,  0.02797634],\n",
       "        [ 0.47552156, -0.56079006,  0.47169533, ...,  0.7253324 ,\n",
       "         -0.41338626, -0.02545325],\n",
       "        [ 0.47995764, -0.39457887,  0.56018215, ...,  0.7901278 ,\n",
       "         -0.5410525 ,  0.01307989]],\n",
       "\n",
       "       [[ 0.02781946, -0.02764396,  0.09445661, ...,  0.10266499,\n",
       "         -0.0367513 ,  0.05366932],\n",
       "        [ 0.02822802, -0.03218175,  0.09385711, ...,  0.10875521,\n",
       "         -0.03896853,  0.05207273],\n",
       "        [ 0.02795907, -0.03357475,  0.09306846, ...,  0.11037759,\n",
       "         -0.04007381,  0.05098896],\n",
       "        ...,\n",
       "        [ 0.5158172 , -0.45099688,  0.44273677, ...,  0.67426884,\n",
       "         -0.475579  ,  0.02444944],\n",
       "        [ 0.47736195, -0.51312053,  0.35453063, ...,  0.5745472 ,\n",
       "         -0.42173067, -0.06269072],\n",
       "        [ 0.48732874, -0.40879905,  0.5462488 , ...,  0.78439087,\n",
       "         -0.5344793 ,  0.00543145]],\n",
       "\n",
       "       [[ 0.01762544, -0.1050678 , -0.06902619, ...,  0.04092296,\n",
       "         -0.02420309, -0.04515423],\n",
       "        [ 0.00901157, -0.15018499, -0.06452435, ...,  0.06070383,\n",
       "         -0.00874564, -0.04072388],\n",
       "        [ 0.00632706, -0.16400456, -0.06474699, ...,  0.0651137 ,\n",
       "         -0.00589067, -0.04075022],\n",
       "        ...,\n",
       "        [ 0.62462765,  0.344931  ,  0.15895101, ...,  0.59220797,\n",
       "         -0.82652384,  0.01159092],\n",
       "        [ 0.4602872 , -0.40293294,  0.1130278 , ...,  0.44226593,\n",
       "         -0.46194685, -0.19209872],\n",
       "        [ 0.4724029 , -0.39222723,  0.56310916, ...,  0.8482854 ,\n",
       "         -0.6145802 , -0.03269086]],\n",
       "\n",
       "       [[ 0.01762544, -0.1050678 , -0.0690262 , ...,  0.04092298,\n",
       "         -0.02420309, -0.04515424],\n",
       "        [ 0.00901157, -0.15018496, -0.06452432, ...,  0.06070383,\n",
       "         -0.0087457 , -0.04072389],\n",
       "        [ 0.00632705, -0.16400452, -0.06474701, ...,  0.06511365,\n",
       "         -0.00589067, -0.04075022],\n",
       "        ...,\n",
       "        [ 0.6246277 ,  0.34493095,  0.15895106, ...,  0.59220797,\n",
       "         -0.82652384,  0.01159092],\n",
       "        [ 0.46028718, -0.4029329 ,  0.1130278 , ...,  0.4422659 ,\n",
       "         -0.46194667, -0.1920987 ],\n",
       "        [ 0.47240284, -0.39222717,  0.5631091 , ...,  0.84828526,\n",
       "         -0.6145801 , -0.03269086]]], dtype=float32)>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(768,), dtype=float32, numpy=\n",
       "array([ 2.82280147e-02, -3.21817696e-02,  9.38570797e-02,  5.64307794e-02,\n",
       "       -5.62167168e-02, -4.53090817e-02,  1.57649189e-01,  3.94005105e-02,\n",
       "        2.54391372e-01,  3.69562387e-01,  8.11151266e-02,  8.10427666e-02,\n",
       "        1.01128034e-01, -1.24113932e-01,  1.88320726e-02, -2.26577967e-02,\n",
       "       -6.03011549e-02, -6.14533797e-02, -4.82205078e-02, -7.14849681e-03,\n",
       "       -1.13110252e-01, -2.57278860e-01,  1.38709605e-01,  5.41408807e-02,\n",
       "       -5.95906079e-01,  1.54145792e-01,  4.34713066e-03, -7.52546340e-02,\n",
       "       -3.77097577e-02, -4.28135097e-02,  3.97741646e-02, -2.14224383e-02,\n",
       "       -2.95710802e-01, -4.20654202e+00, -1.30429447e-01, -6.88063502e-02,\n",
       "        1.29382089e-01,  1.59032136e-01, -1.14017189e-01,  4.67470199e-01,\n",
       "       -1.06012955e-01,  4.08833838e+00,  9.22491401e-02,  7.57985413e-02,\n",
       "       -3.10274288e-02,  4.93434966e-02, -3.73269320e-02,  1.56765908e-01,\n",
       "       -1.74741969e-02, -2.23300606e-03, -5.87098598e-02, -1.43531501e-01,\n",
       "       -1.50624737e-02, -5.19720018e-02, -1.02772236e-01,  1.07619859e-01,\n",
       "       -1.75408721e-02, -1.81991607e-03,  8.49366188e-02, -1.21714950e-01,\n",
       "        1.06355086e-01, -1.09820023e-01,  1.12613156e-01, -2.35492274e-01,\n",
       "       -2.49870270e-02, -7.21038878e-02,  8.72565359e-02, -2.19856024e-01,\n",
       "       -1.54306412e-01, -1.43044710e-01, -1.51177198e-02, -7.27587581e-01,\n",
       "        9.32851434e-03, -6.14927864e+00, -1.28186196e-02,  7.06046522e-02,\n",
       "       -8.49915743e-01, -9.56345648e-02, -3.17586869e-01,  1.95543170e-02,\n",
       "        7.78961480e-02,  7.99848884e-02,  3.98447067e-02, -1.57797232e-01,\n",
       "        1.01675928e-01, -3.51875931e-01, -8.41314197e-02,  2.88891912e-01,\n",
       "        3.38378176e-02,  7.72559643e-03, -3.09281424e-02, -1.59703344e-02,\n",
       "       -1.92614496e-02, -7.25802779e-03,  7.59210885e-02, -3.39110494e-02,\n",
       "        4.92624491e-02,  9.64038298e-02,  2.59936720e-01, -7.55082220e-02,\n",
       "        2.64483035e-01,  7.43885785e-02, -5.07181734e-02,  1.35974839e-01,\n",
       "       -8.77533257e-02, -5.33878207e-02,  4.73156720e-02, -4.79920506e-02,\n",
       "       -2.50379324e-01,  6.60674721e-02, -6.46973848e-01, -2.78264642e-01,\n",
       "        8.33337307e-02,  9.54487324e-02,  5.01879081e-02,  9.35808420e-02,\n",
       "        6.17606789e-02,  7.58233666e-03,  6.62401095e-02, -4.59138572e-01,\n",
       "       -2.23559096e-01,  1.01906762e-01,  2.76228637e-02,  8.84706527e-02,\n",
       "        2.02334374e-02,  5.22996560e-02,  1.15338340e-01,  4.17283103e-02,\n",
       "       -2.19378725e-01,  3.46821472e-02,  7.03475773e-02, -3.28365117e-02,\n",
       "       -7.39338994e-02, -7.51111358e-02, -9.10087526e-02, -1.76471695e-02,\n",
       "        9.30350050e-02, -9.40248072e-02,  2.08007619e-02, -9.76303071e-02,\n",
       "       -3.00549120e-02, -6.63323253e-02, -4.42429781e-02,  2.83163339e-02,\n",
       "        9.99248326e-02,  1.72267124e-01, -5.81646562e-02, -7.23967701e-02,\n",
       "       -1.99493378e-01, -2.27569252e-01, -1.68259293e-02,  3.43432873e-02,\n",
       "       -1.38194785e-01,  2.36961022e-02, -1.67267039e-01,  4.29717749e-02,\n",
       "       -3.90589535e-02,  2.22586736e-01, -8.98852050e-02, -1.62392557e-01,\n",
       "        5.06686047e-02,  6.44398481e-02, -1.23758033e-01,  9.21011791e-02,\n",
       "       -9.15047526e-02, -1.40272081e-02, -1.07318379e-01, -1.59288675e-01,\n",
       "       -4.50867265e-02, -8.21612924e-02, -3.94774973e-02,  7.96440393e-02,\n",
       "        8.42641592e-02,  5.53686172e-03,  2.45816410e-02,  7.71255046e-02,\n",
       "        3.50533873e-02, -2.74334639e-01,  3.81395370e-02,  1.73504144e-01,\n",
       "       -2.36047432e-02, -1.13190517e-01,  2.78749526e-01,  1.66694447e-02,\n",
       "        3.27655151e-02,  5.46365902e-02, -6.94856793e-03, -2.68733501e-02,\n",
       "        8.35034251e-03,  7.83905908e-02, -1.41107008e-01,  9.31080803e-02,\n",
       "        6.07582554e-02, -1.47444308e-02, -2.17631400e-01, -4.22650278e-02,\n",
       "       -1.24512538e-01,  2.45678425e-01,  6.25354201e-02,  1.26705080e-01,\n",
       "       -3.88421118e-01,  2.04760939e-01,  4.17974740e-02,  9.94133204e-03,\n",
       "       -1.98893249e-02, -3.12668383e-01,  1.61616996e-01,  1.19568460e-01,\n",
       "       -1.98707029e-01,  8.62025842e-02, -8.90427828e-02,  4.56304848e-03,\n",
       "        2.17397436e-02, -1.96620554e-01, -7.64501989e-02, -6.42929375e-02,\n",
       "        5.06449640e-02,  4.01589274e-03,  8.23094547e-02, -5.06155342e-02,\n",
       "        1.23967588e-01,  1.46971241e-01, -2.11190432e-02,  3.46950889e-01,\n",
       "       -3.33361328e-02,  9.46512818e-02, -1.40132606e-02, -2.47311264e-01,\n",
       "       -5.05886376e-02, -3.15310508e-01, -2.26900354e-02, -6.65390193e-02,\n",
       "       -1.74292475e-02,  9.63384584e-02,  8.70529860e-02,  1.14032857e-01,\n",
       "        9.53480080e-02, -3.05750966e-02, -1.54998541e-01,  1.39096320e-01,\n",
       "        1.03786103e-01, -9.06835049e-02,  2.43285671e-02, -7.45934248e-02,\n",
       "        1.44638270e-02,  9.54412669e-02, -1.89549431e-01, -6.54051900e-02,\n",
       "        1.58125907e-03,  7.08884192e+00, -2.87004977e-01, -1.95426375e-01,\n",
       "        1.04887113e-02, -1.01862147e-01, -1.21541321e-04,  4.27371711e-02,\n",
       "       -8.93619135e-02, -5.09735346e-02, -1.42141879e-01,  6.44080639e-02,\n",
       "       -3.64885554e-02, -2.44297311e-01, -3.14378738e-02, -1.99580505e-01,\n",
       "       -2.78192312e-02,  1.41526088e-02, -2.52200663e-02, -8.93821716e-02,\n",
       "        3.27090919e-02,  9.12145525e-03, -4.99935746e-02, -9.97905880e-02,\n",
       "        2.65509725e-01,  4.67423573e-02,  9.95619744e-02,  6.93608075e-02,\n",
       "        5.45560420e-02,  6.67060167e-03, -1.70418844e-02, -6.36527389e-02,\n",
       "        9.48510319e-02, -4.64208424e-02, -4.13031504e-02, -8.82632509e-02,\n",
       "        8.98061320e-02, -5.82064539e-02, -2.86336243e-03, -1.58504635e-01,\n",
       "        1.76033974e-02, -3.05180788e-01,  1.55460089e-02, -5.84798604e-02,\n",
       "       -4.39236835e-02, -1.04251385e-01, -3.36828828e-03,  4.58737835e-02,\n",
       "       -1.12262592e-01, -1.67932302e-01,  3.94643396e-02, -2.95938998e-02,\n",
       "       -6.06893748e-02,  2.20921293e-01, -2.29589194e-02,  1.29068971e-01,\n",
       "       -7.93149620e-02,  1.01901457e-01,  1.78513765e-01,  4.42255884e-02,\n",
       "        2.22367167e-01, -5.71136773e-02,  1.28056467e-01,  2.47028917e-02,\n",
       "        7.28440434e-02,  3.96205842e-01,  4.22214448e-01, -2.68094420e-01,\n",
       "       -3.60087454e-02, -1.67606443e-01, -4.50454354e-02,  4.84947935e-02,\n",
       "        2.30082822e+00,  7.53123313e-02, -3.06670368e-01,  4.24945951e-02,\n",
       "       -8.39193463e-02, -2.19397694e-01,  7.72201866e-02,  1.00784004e-02,\n",
       "       -6.66904747e-02,  1.35145605e-01,  3.25264409e-02,  1.84485227e-01,\n",
       "        5.51973060e-02,  5.13904467e-02, -1.60237044e-01,  7.63920471e-02,\n",
       "       -2.47001097e-01, -4.03270498e-02,  5.92439398e-02, -1.94313690e-01,\n",
       "        1.28055260e-01, -6.69740885e-03, -7.18513131e-03,  4.76145968e-02,\n",
       "        5.42324074e-02,  8.88772607e-02, -1.55505240e-02,  3.18717510e-02,\n",
       "        2.26057589e-01,  4.66867536e-03,  9.09113735e-02, -3.31652761e-02,\n",
       "        6.57844245e-02,  1.34649426e-02,  7.16400146e-02, -8.47277939e-02,\n",
       "       -1.59169883e-02, -5.94771922e-01, -2.10613191e-01, -4.63420153e-03,\n",
       "       -1.29988432e-01,  2.52642557e-02,  2.21400037e-01,  1.56377256e-03,\n",
       "       -2.37283856e-02,  4.36369032e-02, -1.67511731e-01,  1.03924572e-02,\n",
       "        1.29897892e-02,  9.18010920e-02, -1.30691081e-02, -6.89714998e-02,\n",
       "       -4.16477025e-03, -1.03861964e+00, -5.78515083e-02, -1.55802965e-01,\n",
       "       -3.44474614e-03,  5.30683771e-02,  5.92065230e-02,  2.83779129e-02,\n",
       "       -7.65137374e-03, -4.97353226e-02, -3.72919440e-03,  3.67233604e-02,\n",
       "       -2.88931429e-01, -3.56034338e-02, -6.19068742e-05, -3.91812921e-02,\n",
       "       -1.01545162e-01, -3.07814702e-02, -6.31496310e-03, -4.89501953e-02,\n",
       "        1.55272037e-01,  7.44082630e-02, -2.70847902e-02, -8.28471482e-02,\n",
       "       -2.49213174e-01, -6.99147433e-02, -1.75530761e-02,  5.31411543e-02,\n",
       "       -3.63527760e-02,  2.50938088e-02,  1.53716087e-01, -6.47525430e-01,\n",
       "        1.11692600e-01, -1.47369891e-01,  7.86894113e-02, -6.98843524e-02,\n",
       "        3.62307504e-02,  1.26903057e-02, -2.09773123e-01, -2.57796675e-01,\n",
       "        1.08181790e-01,  2.12532818e-01,  2.58809060e-01,  9.98311341e-02,\n",
       "       -6.91081956e-02, -9.62167454e+00,  1.04546405e-01,  1.70738026e-02,\n",
       "       -6.28737956e-02, -5.46228811e-02,  1.44491494e-02, -1.85158268e-01,\n",
       "        7.63471648e-02, -2.86513507e-01,  2.67040506e-02,  2.72014216e-02,\n",
       "        1.07686907e-01,  8.96953046e-02,  1.84559435e-01, -7.17909634e-02,\n",
       "       -1.94128305e-02,  3.97608578e-02,  2.25926355e-01, -9.75738913e-02,\n",
       "        1.76789343e-01, -6.01789057e-02,  4.81582507e-02,  1.42947137e-01,\n",
       "        8.81566256e-02,  4.81110811e-03,  2.19531059e-01,  2.13012919e-01,\n",
       "        1.80334777e-01,  2.04110652e-01, -3.37645411e-03,  5.18192612e-02,\n",
       "       -9.38816369e-03, -7.43411481e-02,  5.81906065e-02,  1.12465173e-01,\n",
       "        9.02250409e-03, -2.68907845e-01,  1.24228910e-01,  2.03325212e-01,\n",
       "       -5.66409528e-02,  8.55190009e-02,  1.69394448e-01,  5.98699898e-02,\n",
       "       -2.34286234e-01, -2.37933338e-01, -4.74982113e-02,  1.36016995e-01,\n",
       "       -2.04128534e-01,  2.38992691e-01, -1.17467791e-02,  2.10813582e-02,\n",
       "        2.82539800e-02, -1.89876020e-01,  2.15374053e-01, -2.24107206e-02,\n",
       "        1.04231313e-01,  3.07620287e-01,  5.64959669e+00, -2.10851803e-02,\n",
       "        1.06464028e-02, -3.71021569e-01,  6.70800284e-02,  1.71955198e-01,\n",
       "        6.13538995e-02, -8.86641443e-02,  2.35699639e-02, -2.39929631e-02,\n",
       "        1.26713932e-01,  5.68222776e-02,  2.76174769e-02, -2.37398818e-01,\n",
       "       -4.71137613e-02,  1.34681523e-01, -2.98562825e-01,  1.57990545e-01,\n",
       "       -2.96753570e-02, -4.05380353e-02, -3.54445547e-01, -3.01487297e-02,\n",
       "        4.29680049e-02, -5.56575954e-02,  2.13598460e-03,  7.04908848e-01,\n",
       "        3.93005347e+00,  3.38223577e-03,  8.02005380e-02, -2.19285205e-01,\n",
       "        6.76947832e-02, -7.29747415e-02,  6.60324097e-02, -7.06017017e-05,\n",
       "       -4.20176089e-02,  5.56721650e-02, -1.88391477e-01, -3.08856070e-02,\n",
       "       -8.07680726e-01, -2.76865289e-02, -8.16123188e-03, -2.71613300e-01,\n",
       "       -4.53045666e-02,  6.93619102e-02,  7.90390745e-02, -1.79638922e-01,\n",
       "        9.89793390e-02,  2.30456799e-01,  1.83292434e-01, -9.57344621e-02,\n",
       "       -1.90485269e-02,  5.28639518e-02,  9.80578884e-02,  1.47100985e-02,\n",
       "       -7.51232207e-02,  3.93156111e-01,  8.95578414e-03, -1.45433739e-01,\n",
       "       -2.26747006e-01,  1.76750943e-02,  9.44698453e-02, -1.60982624e-01,\n",
       "        7.95187801e-02, -1.13281250e-01,  3.63729000e-02, -1.50456935e-01,\n",
       "        6.38708994e-02,  1.95183292e-01, -5.78123108e-02,  1.38022453e-01,\n",
       "       -8.60482454e-02, -1.22325860e-01,  3.47356200e-02, -1.80184871e-01,\n",
       "       -2.22866982e-02, -4.67048883e-02,  8.16800892e-02, -5.16696125e-02,\n",
       "       -3.07996899e-01,  1.44759089e-01, -1.15834519e-01,  1.49732560e-01,\n",
       "       -3.99254262e-02, -1.41819596e-01,  4.95007277e-01,  9.20786560e-02,\n",
       "       -1.40833348e-01,  7.23419935e-02, -1.96636021e-02, -2.17523545e-01,\n",
       "       -1.56580615e+00,  1.06781833e-01, -2.80854344e-01, -9.48411226e-03,\n",
       "        4.37189713e-02,  1.68853253e-02,  6.82138801e-02, -3.88607830e-02,\n",
       "       -1.20486133e-01,  1.05698816e-01,  5.61210439e-02, -1.27713487e-01,\n",
       "       -9.63485241e-03,  2.79239267e-02, -9.02154595e-02,  2.08874047e-02,\n",
       "        1.85290337e-01, -4.40162212e-01, -1.37173459e-02,  5.21893576e-02,\n",
       "        1.11480877e-02,  3.92188430e-01,  3.32581997e-02,  6.82868809e-02,\n",
       "       -2.21395791e-02,  1.17935531e-01,  2.47270539e-02,  9.25006792e-02,\n",
       "       -7.31103599e-01, -1.13351405e-01,  7.90063381e-01,  6.17807359e-03,\n",
       "        3.82854044e-03,  7.35228509e-02,  9.35008377e-02, -5.75888604e-02,\n",
       "        1.27106011e-02, -3.14952433e-02, -2.12809145e-01,  1.79478750e-02,\n",
       "        1.39420897e-01, -2.49392837e-01, -3.52650881e-04, -3.02430779e-01,\n",
       "        6.70434535e-02,  1.48778796e+00,  8.14256817e-02, -1.66971311e-02,\n",
       "       -2.42140859e-01,  7.30241090e-03,  3.22215170e-01,  2.01219380e-01,\n",
       "        1.76704675e-03, -2.12306485e-01,  7.71280378e-03, -2.29459658e-01,\n",
       "        1.12705186e-01, -2.69276381e-01,  4.38885763e-02,  1.94405034e-01,\n",
       "       -2.97011584e-02, -2.14975536e-01, -3.48869771e-01,  1.46047235e-01,\n",
       "        6.45025894e-02, -7.90751725e-02, -5.23620099e-03,  1.26216337e-02,\n",
       "        1.00878179e-01, -1.48543760e-01,  2.17495859e-01, -1.16841912e-01,\n",
       "       -6.33752942e-02, -1.81115806e-01, -5.03756776e-02, -7.24328831e-02,\n",
       "        1.73123479e-01,  1.14393957e-01, -2.34046146e-01, -3.27995092e-01,\n",
       "        9.48542416e-01,  6.11418635e-02,  5.93760237e-02,  6.92470148e-02,\n",
       "        1.58761591e-02,  1.53169855e-02, -2.73226738e-01,  9.39478204e-02,\n",
       "        1.66505188e-01,  1.82507932e-03,  3.73728797e-02,  4.63539809e-02,\n",
       "       -3.63940716e-01,  7.64667988e-03, -4.83855009e-02,  5.83370328e-02,\n",
       "       -1.58719629e-01,  2.98200905e-01, -3.39906454e-01,  6.04795404e-02,\n",
       "       -2.36374021e-01,  1.20554641e-01,  1.25598416e-01,  2.32820511e-02,\n",
       "       -3.25847343e-02,  9.67382193e-02, -5.43004647e-02, -4.63016927e-02,\n",
       "        1.30620450e-02,  6.69644326e-02,  3.66901010e-02,  2.83600017e-02,\n",
       "        1.42067865e-01,  1.02001384e-01,  8.91482830e-03, -6.54725879e-02,\n",
       "       -7.83035755e-02, -3.95662725e-01,  1.76435560e-01,  6.51695728e-02,\n",
       "       -1.61987618e-01,  8.89389217e-03, -2.31061131e-02, -1.52584404e-01,\n",
       "        5.89490831e-02, -1.06859878e-02,  5.46219051e-02,  2.20712602e-01,\n",
       "       -6.73708469e-02, -1.06568605e-01, -4.82591838e-01, -1.38334870e-01,\n",
       "       -5.63872457e-02, -1.14008203e-01, -5.09424210e-01,  7.40429461e-02,\n",
       "       -2.38716021e-01, -5.13873994e-03, -1.10749006e-02, -7.54642189e-02,\n",
       "       -8.17915797e-03,  1.15139030e-01, -5.97481802e-02,  3.52409557e-02,\n",
       "       -1.04988590e-02,  2.75141746e-03,  3.76910120e-02, -3.65639180e-02,\n",
       "       -3.56707126e-02, -6.18496239e-02, -2.79584825e-02, -2.11061239e-02,\n",
       "        1.82508588e-01,  6.04524687e-02,  1.94561100e+00,  1.23547390e-02,\n",
       "       -4.44188863e-02,  9.30045098e-02,  2.93263078e-01,  1.10907602e+00,\n",
       "       -2.13454232e-01,  1.00893036e-01, -7.45347887e-02, -7.63147026e-02,\n",
       "       -5.66515565e-01,  1.73506886e-03,  1.68588907e-02,  2.97430038e+00,\n",
       "        9.18797106e-02,  1.17823653e-01, -5.00971824e-02, -1.24736458e-01,\n",
       "       -1.25242099e-01, -1.48561448e-02,  9.21029598e-03,  7.98027441e-02,\n",
       "       -1.59711763e-02, -1.53725311e-01, -7.79676363e-02,  1.14297703e-01,\n",
       "        3.73968482e-03, -5.28066278e-01,  4.29487154e-02,  2.50155702e-02,\n",
       "       -1.05993822e-01, -3.67086306e-02, -6.68496266e-02, -2.43859977e-01,\n",
       "        9.11740810e-02,  5.44198900e-02, -1.19847760e-01,  5.89034930e-02,\n",
       "       -2.13027418e-01, -2.12837309e-02, -4.40465689e-01,  1.44285530e-01,\n",
       "        1.51509523e-01, -3.15586329e-02,  7.54080191e-02,  5.86564764e-02,\n",
       "        4.39579263e-02,  1.08755276e-01, -3.89686525e-02,  5.20727001e-02],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_output[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=int32, numpy=\n",
       "array([21, 26,  1, 25, 38, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0])>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_positions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(768,), dtype=float32, numpy=\n",
       "array([ 2.82280147e-02, -3.21817696e-02,  9.38570797e-02,  5.64307794e-02,\n",
       "       -5.62167168e-02, -4.53090817e-02,  1.57649189e-01,  3.94005105e-02,\n",
       "        2.54391372e-01,  3.69562387e-01,  8.11151266e-02,  8.10427666e-02,\n",
       "        1.01128034e-01, -1.24113932e-01,  1.88320726e-02, -2.26577967e-02,\n",
       "       -6.03011549e-02, -6.14533797e-02, -4.82205078e-02, -7.14849681e-03,\n",
       "       -1.13110252e-01, -2.57278860e-01,  1.38709605e-01,  5.41408807e-02,\n",
       "       -5.95906079e-01,  1.54145792e-01,  4.34713066e-03, -7.52546340e-02,\n",
       "       -3.77097577e-02, -4.28135097e-02,  3.97741646e-02, -2.14224383e-02,\n",
       "       -2.95710802e-01, -4.20654202e+00, -1.30429447e-01, -6.88063502e-02,\n",
       "        1.29382089e-01,  1.59032136e-01, -1.14017189e-01,  4.67470199e-01,\n",
       "       -1.06012955e-01,  4.08833838e+00,  9.22491401e-02,  7.57985413e-02,\n",
       "       -3.10274288e-02,  4.93434966e-02, -3.73269320e-02,  1.56765908e-01,\n",
       "       -1.74741969e-02, -2.23300606e-03, -5.87098598e-02, -1.43531501e-01,\n",
       "       -1.50624737e-02, -5.19720018e-02, -1.02772236e-01,  1.07619859e-01,\n",
       "       -1.75408721e-02, -1.81991607e-03,  8.49366188e-02, -1.21714950e-01,\n",
       "        1.06355086e-01, -1.09820023e-01,  1.12613156e-01, -2.35492274e-01,\n",
       "       -2.49870270e-02, -7.21038878e-02,  8.72565359e-02, -2.19856024e-01,\n",
       "       -1.54306412e-01, -1.43044710e-01, -1.51177198e-02, -7.27587581e-01,\n",
       "        9.32851434e-03, -6.14927864e+00, -1.28186196e-02,  7.06046522e-02,\n",
       "       -8.49915743e-01, -9.56345648e-02, -3.17586869e-01,  1.95543170e-02,\n",
       "        7.78961480e-02,  7.99848884e-02,  3.98447067e-02, -1.57797232e-01,\n",
       "        1.01675928e-01, -3.51875931e-01, -8.41314197e-02,  2.88891912e-01,\n",
       "        3.38378176e-02,  7.72559643e-03, -3.09281424e-02, -1.59703344e-02,\n",
       "       -1.92614496e-02, -7.25802779e-03,  7.59210885e-02, -3.39110494e-02,\n",
       "        4.92624491e-02,  9.64038298e-02,  2.59936720e-01, -7.55082220e-02,\n",
       "        2.64483035e-01,  7.43885785e-02, -5.07181734e-02,  1.35974839e-01,\n",
       "       -8.77533257e-02, -5.33878207e-02,  4.73156720e-02, -4.79920506e-02,\n",
       "       -2.50379324e-01,  6.60674721e-02, -6.46973848e-01, -2.78264642e-01,\n",
       "        8.33337307e-02,  9.54487324e-02,  5.01879081e-02,  9.35808420e-02,\n",
       "        6.17606789e-02,  7.58233666e-03,  6.62401095e-02, -4.59138572e-01,\n",
       "       -2.23559096e-01,  1.01906762e-01,  2.76228637e-02,  8.84706527e-02,\n",
       "        2.02334374e-02,  5.22996560e-02,  1.15338340e-01,  4.17283103e-02,\n",
       "       -2.19378725e-01,  3.46821472e-02,  7.03475773e-02, -3.28365117e-02,\n",
       "       -7.39338994e-02, -7.51111358e-02, -9.10087526e-02, -1.76471695e-02,\n",
       "        9.30350050e-02, -9.40248072e-02,  2.08007619e-02, -9.76303071e-02,\n",
       "       -3.00549120e-02, -6.63323253e-02, -4.42429781e-02,  2.83163339e-02,\n",
       "        9.99248326e-02,  1.72267124e-01, -5.81646562e-02, -7.23967701e-02,\n",
       "       -1.99493378e-01, -2.27569252e-01, -1.68259293e-02,  3.43432873e-02,\n",
       "       -1.38194785e-01,  2.36961022e-02, -1.67267039e-01,  4.29717749e-02,\n",
       "       -3.90589535e-02,  2.22586736e-01, -8.98852050e-02, -1.62392557e-01,\n",
       "        5.06686047e-02,  6.44398481e-02, -1.23758033e-01,  9.21011791e-02,\n",
       "       -9.15047526e-02, -1.40272081e-02, -1.07318379e-01, -1.59288675e-01,\n",
       "       -4.50867265e-02, -8.21612924e-02, -3.94774973e-02,  7.96440393e-02,\n",
       "        8.42641592e-02,  5.53686172e-03,  2.45816410e-02,  7.71255046e-02,\n",
       "        3.50533873e-02, -2.74334639e-01,  3.81395370e-02,  1.73504144e-01,\n",
       "       -2.36047432e-02, -1.13190517e-01,  2.78749526e-01,  1.66694447e-02,\n",
       "        3.27655151e-02,  5.46365902e-02, -6.94856793e-03, -2.68733501e-02,\n",
       "        8.35034251e-03,  7.83905908e-02, -1.41107008e-01,  9.31080803e-02,\n",
       "        6.07582554e-02, -1.47444308e-02, -2.17631400e-01, -4.22650278e-02,\n",
       "       -1.24512538e-01,  2.45678425e-01,  6.25354201e-02,  1.26705080e-01,\n",
       "       -3.88421118e-01,  2.04760939e-01,  4.17974740e-02,  9.94133204e-03,\n",
       "       -1.98893249e-02, -3.12668383e-01,  1.61616996e-01,  1.19568460e-01,\n",
       "       -1.98707029e-01,  8.62025842e-02, -8.90427828e-02,  4.56304848e-03,\n",
       "        2.17397436e-02, -1.96620554e-01, -7.64501989e-02, -6.42929375e-02,\n",
       "        5.06449640e-02,  4.01589274e-03,  8.23094547e-02, -5.06155342e-02,\n",
       "        1.23967588e-01,  1.46971241e-01, -2.11190432e-02,  3.46950889e-01,\n",
       "       -3.33361328e-02,  9.46512818e-02, -1.40132606e-02, -2.47311264e-01,\n",
       "       -5.05886376e-02, -3.15310508e-01, -2.26900354e-02, -6.65390193e-02,\n",
       "       -1.74292475e-02,  9.63384584e-02,  8.70529860e-02,  1.14032857e-01,\n",
       "        9.53480080e-02, -3.05750966e-02, -1.54998541e-01,  1.39096320e-01,\n",
       "        1.03786103e-01, -9.06835049e-02,  2.43285671e-02, -7.45934248e-02,\n",
       "        1.44638270e-02,  9.54412669e-02, -1.89549431e-01, -6.54051900e-02,\n",
       "        1.58125907e-03,  7.08884192e+00, -2.87004977e-01, -1.95426375e-01,\n",
       "        1.04887113e-02, -1.01862147e-01, -1.21541321e-04,  4.27371711e-02,\n",
       "       -8.93619135e-02, -5.09735346e-02, -1.42141879e-01,  6.44080639e-02,\n",
       "       -3.64885554e-02, -2.44297311e-01, -3.14378738e-02, -1.99580505e-01,\n",
       "       -2.78192312e-02,  1.41526088e-02, -2.52200663e-02, -8.93821716e-02,\n",
       "        3.27090919e-02,  9.12145525e-03, -4.99935746e-02, -9.97905880e-02,\n",
       "        2.65509725e-01,  4.67423573e-02,  9.95619744e-02,  6.93608075e-02,\n",
       "        5.45560420e-02,  6.67060167e-03, -1.70418844e-02, -6.36527389e-02,\n",
       "        9.48510319e-02, -4.64208424e-02, -4.13031504e-02, -8.82632509e-02,\n",
       "        8.98061320e-02, -5.82064539e-02, -2.86336243e-03, -1.58504635e-01,\n",
       "        1.76033974e-02, -3.05180788e-01,  1.55460089e-02, -5.84798604e-02,\n",
       "       -4.39236835e-02, -1.04251385e-01, -3.36828828e-03,  4.58737835e-02,\n",
       "       -1.12262592e-01, -1.67932302e-01,  3.94643396e-02, -2.95938998e-02,\n",
       "       -6.06893748e-02,  2.20921293e-01, -2.29589194e-02,  1.29068971e-01,\n",
       "       -7.93149620e-02,  1.01901457e-01,  1.78513765e-01,  4.42255884e-02,\n",
       "        2.22367167e-01, -5.71136773e-02,  1.28056467e-01,  2.47028917e-02,\n",
       "        7.28440434e-02,  3.96205842e-01,  4.22214448e-01, -2.68094420e-01,\n",
       "       -3.60087454e-02, -1.67606443e-01, -4.50454354e-02,  4.84947935e-02,\n",
       "        2.30082822e+00,  7.53123313e-02, -3.06670368e-01,  4.24945951e-02,\n",
       "       -8.39193463e-02, -2.19397694e-01,  7.72201866e-02,  1.00784004e-02,\n",
       "       -6.66904747e-02,  1.35145605e-01,  3.25264409e-02,  1.84485227e-01,\n",
       "        5.51973060e-02,  5.13904467e-02, -1.60237044e-01,  7.63920471e-02,\n",
       "       -2.47001097e-01, -4.03270498e-02,  5.92439398e-02, -1.94313690e-01,\n",
       "        1.28055260e-01, -6.69740885e-03, -7.18513131e-03,  4.76145968e-02,\n",
       "        5.42324074e-02,  8.88772607e-02, -1.55505240e-02,  3.18717510e-02,\n",
       "        2.26057589e-01,  4.66867536e-03,  9.09113735e-02, -3.31652761e-02,\n",
       "        6.57844245e-02,  1.34649426e-02,  7.16400146e-02, -8.47277939e-02,\n",
       "       -1.59169883e-02, -5.94771922e-01, -2.10613191e-01, -4.63420153e-03,\n",
       "       -1.29988432e-01,  2.52642557e-02,  2.21400037e-01,  1.56377256e-03,\n",
       "       -2.37283856e-02,  4.36369032e-02, -1.67511731e-01,  1.03924572e-02,\n",
       "        1.29897892e-02,  9.18010920e-02, -1.30691081e-02, -6.89714998e-02,\n",
       "       -4.16477025e-03, -1.03861964e+00, -5.78515083e-02, -1.55802965e-01,\n",
       "       -3.44474614e-03,  5.30683771e-02,  5.92065230e-02,  2.83779129e-02,\n",
       "       -7.65137374e-03, -4.97353226e-02, -3.72919440e-03,  3.67233604e-02,\n",
       "       -2.88931429e-01, -3.56034338e-02, -6.19068742e-05, -3.91812921e-02,\n",
       "       -1.01545162e-01, -3.07814702e-02, -6.31496310e-03, -4.89501953e-02,\n",
       "        1.55272037e-01,  7.44082630e-02, -2.70847902e-02, -8.28471482e-02,\n",
       "       -2.49213174e-01, -6.99147433e-02, -1.75530761e-02,  5.31411543e-02,\n",
       "       -3.63527760e-02,  2.50938088e-02,  1.53716087e-01, -6.47525430e-01,\n",
       "        1.11692600e-01, -1.47369891e-01,  7.86894113e-02, -6.98843524e-02,\n",
       "        3.62307504e-02,  1.26903057e-02, -2.09773123e-01, -2.57796675e-01,\n",
       "        1.08181790e-01,  2.12532818e-01,  2.58809060e-01,  9.98311341e-02,\n",
       "       -6.91081956e-02, -9.62167454e+00,  1.04546405e-01,  1.70738026e-02,\n",
       "       -6.28737956e-02, -5.46228811e-02,  1.44491494e-02, -1.85158268e-01,\n",
       "        7.63471648e-02, -2.86513507e-01,  2.67040506e-02,  2.72014216e-02,\n",
       "        1.07686907e-01,  8.96953046e-02,  1.84559435e-01, -7.17909634e-02,\n",
       "       -1.94128305e-02,  3.97608578e-02,  2.25926355e-01, -9.75738913e-02,\n",
       "        1.76789343e-01, -6.01789057e-02,  4.81582507e-02,  1.42947137e-01,\n",
       "        8.81566256e-02,  4.81110811e-03,  2.19531059e-01,  2.13012919e-01,\n",
       "        1.80334777e-01,  2.04110652e-01, -3.37645411e-03,  5.18192612e-02,\n",
       "       -9.38816369e-03, -7.43411481e-02,  5.81906065e-02,  1.12465173e-01,\n",
       "        9.02250409e-03, -2.68907845e-01,  1.24228910e-01,  2.03325212e-01,\n",
       "       -5.66409528e-02,  8.55190009e-02,  1.69394448e-01,  5.98699898e-02,\n",
       "       -2.34286234e-01, -2.37933338e-01, -4.74982113e-02,  1.36016995e-01,\n",
       "       -2.04128534e-01,  2.38992691e-01, -1.17467791e-02,  2.10813582e-02,\n",
       "        2.82539800e-02, -1.89876020e-01,  2.15374053e-01, -2.24107206e-02,\n",
       "        1.04231313e-01,  3.07620287e-01,  5.64959669e+00, -2.10851803e-02,\n",
       "        1.06464028e-02, -3.71021569e-01,  6.70800284e-02,  1.71955198e-01,\n",
       "        6.13538995e-02, -8.86641443e-02,  2.35699639e-02, -2.39929631e-02,\n",
       "        1.26713932e-01,  5.68222776e-02,  2.76174769e-02, -2.37398818e-01,\n",
       "       -4.71137613e-02,  1.34681523e-01, -2.98562825e-01,  1.57990545e-01,\n",
       "       -2.96753570e-02, -4.05380353e-02, -3.54445547e-01, -3.01487297e-02,\n",
       "        4.29680049e-02, -5.56575954e-02,  2.13598460e-03,  7.04908848e-01,\n",
       "        3.93005347e+00,  3.38223577e-03,  8.02005380e-02, -2.19285205e-01,\n",
       "        6.76947832e-02, -7.29747415e-02,  6.60324097e-02, -7.06017017e-05,\n",
       "       -4.20176089e-02,  5.56721650e-02, -1.88391477e-01, -3.08856070e-02,\n",
       "       -8.07680726e-01, -2.76865289e-02, -8.16123188e-03, -2.71613300e-01,\n",
       "       -4.53045666e-02,  6.93619102e-02,  7.90390745e-02, -1.79638922e-01,\n",
       "        9.89793390e-02,  2.30456799e-01,  1.83292434e-01, -9.57344621e-02,\n",
       "       -1.90485269e-02,  5.28639518e-02,  9.80578884e-02,  1.47100985e-02,\n",
       "       -7.51232207e-02,  3.93156111e-01,  8.95578414e-03, -1.45433739e-01,\n",
       "       -2.26747006e-01,  1.76750943e-02,  9.44698453e-02, -1.60982624e-01,\n",
       "        7.95187801e-02, -1.13281250e-01,  3.63729000e-02, -1.50456935e-01,\n",
       "        6.38708994e-02,  1.95183292e-01, -5.78123108e-02,  1.38022453e-01,\n",
       "       -8.60482454e-02, -1.22325860e-01,  3.47356200e-02, -1.80184871e-01,\n",
       "       -2.22866982e-02, -4.67048883e-02,  8.16800892e-02, -5.16696125e-02,\n",
       "       -3.07996899e-01,  1.44759089e-01, -1.15834519e-01,  1.49732560e-01,\n",
       "       -3.99254262e-02, -1.41819596e-01,  4.95007277e-01,  9.20786560e-02,\n",
       "       -1.40833348e-01,  7.23419935e-02, -1.96636021e-02, -2.17523545e-01,\n",
       "       -1.56580615e+00,  1.06781833e-01, -2.80854344e-01, -9.48411226e-03,\n",
       "        4.37189713e-02,  1.68853253e-02,  6.82138801e-02, -3.88607830e-02,\n",
       "       -1.20486133e-01,  1.05698816e-01,  5.61210439e-02, -1.27713487e-01,\n",
       "       -9.63485241e-03,  2.79239267e-02, -9.02154595e-02,  2.08874047e-02,\n",
       "        1.85290337e-01, -4.40162212e-01, -1.37173459e-02,  5.21893576e-02,\n",
       "        1.11480877e-02,  3.92188430e-01,  3.32581997e-02,  6.82868809e-02,\n",
       "       -2.21395791e-02,  1.17935531e-01,  2.47270539e-02,  9.25006792e-02,\n",
       "       -7.31103599e-01, -1.13351405e-01,  7.90063381e-01,  6.17807359e-03,\n",
       "        3.82854044e-03,  7.35228509e-02,  9.35008377e-02, -5.75888604e-02,\n",
       "        1.27106011e-02, -3.14952433e-02, -2.12809145e-01,  1.79478750e-02,\n",
       "        1.39420897e-01, -2.49392837e-01, -3.52650881e-04, -3.02430779e-01,\n",
       "        6.70434535e-02,  1.48778796e+00,  8.14256817e-02, -1.66971311e-02,\n",
       "       -2.42140859e-01,  7.30241090e-03,  3.22215170e-01,  2.01219380e-01,\n",
       "        1.76704675e-03, -2.12306485e-01,  7.71280378e-03, -2.29459658e-01,\n",
       "        1.12705186e-01, -2.69276381e-01,  4.38885763e-02,  1.94405034e-01,\n",
       "       -2.97011584e-02, -2.14975536e-01, -3.48869771e-01,  1.46047235e-01,\n",
       "        6.45025894e-02, -7.90751725e-02, -5.23620099e-03,  1.26216337e-02,\n",
       "        1.00878179e-01, -1.48543760e-01,  2.17495859e-01, -1.16841912e-01,\n",
       "       -6.33752942e-02, -1.81115806e-01, -5.03756776e-02, -7.24328831e-02,\n",
       "        1.73123479e-01,  1.14393957e-01, -2.34046146e-01, -3.27995092e-01,\n",
       "        9.48542416e-01,  6.11418635e-02,  5.93760237e-02,  6.92470148e-02,\n",
       "        1.58761591e-02,  1.53169855e-02, -2.73226738e-01,  9.39478204e-02,\n",
       "        1.66505188e-01,  1.82507932e-03,  3.73728797e-02,  4.63539809e-02,\n",
       "       -3.63940716e-01,  7.64667988e-03, -4.83855009e-02,  5.83370328e-02,\n",
       "       -1.58719629e-01,  2.98200905e-01, -3.39906454e-01,  6.04795404e-02,\n",
       "       -2.36374021e-01,  1.20554641e-01,  1.25598416e-01,  2.32820511e-02,\n",
       "       -3.25847343e-02,  9.67382193e-02, -5.43004647e-02, -4.63016927e-02,\n",
       "        1.30620450e-02,  6.69644326e-02,  3.66901010e-02,  2.83600017e-02,\n",
       "        1.42067865e-01,  1.02001384e-01,  8.91482830e-03, -6.54725879e-02,\n",
       "       -7.83035755e-02, -3.95662725e-01,  1.76435560e-01,  6.51695728e-02,\n",
       "       -1.61987618e-01,  8.89389217e-03, -2.31061131e-02, -1.52584404e-01,\n",
       "        5.89490831e-02, -1.06859878e-02,  5.46219051e-02,  2.20712602e-01,\n",
       "       -6.73708469e-02, -1.06568605e-01, -4.82591838e-01, -1.38334870e-01,\n",
       "       -5.63872457e-02, -1.14008203e-01, -5.09424210e-01,  7.40429461e-02,\n",
       "       -2.38716021e-01, -5.13873994e-03, -1.10749006e-02, -7.54642189e-02,\n",
       "       -8.17915797e-03,  1.15139030e-01, -5.97481802e-02,  3.52409557e-02,\n",
       "       -1.04988590e-02,  2.75141746e-03,  3.76910120e-02, -3.65639180e-02,\n",
       "       -3.56707126e-02, -6.18496239e-02, -2.79584825e-02, -2.11061239e-02,\n",
       "        1.82508588e-01,  6.04524687e-02,  1.94561100e+00,  1.23547390e-02,\n",
       "       -4.44188863e-02,  9.30045098e-02,  2.93263078e-01,  1.10907602e+00,\n",
       "       -2.13454232e-01,  1.00893036e-01, -7.45347887e-02, -7.63147026e-02,\n",
       "       -5.66515565e-01,  1.73506886e-03,  1.68588907e-02,  2.97430038e+00,\n",
       "        9.18797106e-02,  1.17823653e-01, -5.00971824e-02, -1.24736458e-01,\n",
       "       -1.25242099e-01, -1.48561448e-02,  9.21029598e-03,  7.98027441e-02,\n",
       "       -1.59711763e-02, -1.53725311e-01, -7.79676363e-02,  1.14297703e-01,\n",
       "        3.73968482e-03, -5.28066278e-01,  4.29487154e-02,  2.50155702e-02,\n",
       "       -1.05993822e-01, -3.67086306e-02, -6.68496266e-02, -2.43859977e-01,\n",
       "        9.11740810e-02,  5.44198900e-02, -1.19847760e-01,  5.89034930e-02,\n",
       "       -2.13027418e-01, -2.12837309e-02, -4.40465689e-01,  1.44285530e-01,\n",
       "        1.51509523e-01, -3.15586329e-02,  7.54080191e-02,  5.86564764e-02,\n",
       "        4.39579263e-02,  1.08755276e-01, -3.89686525e-02,  5.20727001e-02],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(gathered_tensor, (-1, 40, 768))[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 40, 32000), dtype=float32, numpy=\n",
       "array([[[-0.96651876,  0.35889095, -0.72566915, ..., -0.6659758 ,\n",
       "         -0.8975039 ,  0.3739977 ],\n",
       "        [-1.169315  ,  0.04444015, -0.70775807, ..., -0.7374146 ,\n",
       "         -0.9624998 ,  0.32622477],\n",
       "        [-1.4330778 , -0.4447086 , -0.7175031 , ..., -0.8573061 ,\n",
       "         -0.96034586,  0.14076054],\n",
       "        ...,\n",
       "        [-0.20153634,  1.3292346 , -0.7900561 , ..., -0.13466144,\n",
       "         -0.00956674,  0.3052761 ],\n",
       "        [-0.20153634,  1.3292346 , -0.7900561 , ..., -0.13466144,\n",
       "         -0.00956674,  0.3052761 ],\n",
       "        [-0.20153634,  1.3292346 , -0.7900561 , ..., -0.13466144,\n",
       "         -0.00956674,  0.3052761 ]],\n",
       "\n",
       "       [[-0.20024347,  1.3415475 , -0.8412107 , ..., -0.16264868,\n",
       "          0.02211582,  0.27375466],\n",
       "        [-0.20024347,  1.3415475 , -0.84121084, ..., -0.16264874,\n",
       "          0.02211584,  0.27375466],\n",
       "        [-0.20024353,  1.3415475 , -0.84121084, ..., -0.16264856,\n",
       "          0.02211585,  0.27375472],\n",
       "        ...,\n",
       "        [-0.19807881,  1.3384638 , -0.8354469 , ..., -0.1575937 ,\n",
       "          0.02327738,  0.27649269],\n",
       "        [-0.19807881,  1.3384638 , -0.8354469 , ..., -0.1575937 ,\n",
       "          0.02327738,  0.27649269],\n",
       "        [-0.19807881,  1.3384638 , -0.8354469 , ..., -0.1575937 ,\n",
       "          0.02327738,  0.27649269]],\n",
       "\n",
       "       [[-0.966551  ,  0.3588614 , -0.7256575 , ..., -0.6659889 ,\n",
       "         -0.8974975 ,  0.3739881 ],\n",
       "        [-0.96654886,  0.3588633 , -0.7256575 , ..., -0.6659878 ,\n",
       "         -0.89749825,  0.3739881 ],\n",
       "        [-0.7874139 ,  0.63316286, -0.7051257 , ..., -0.57043695,\n",
       "         -0.7322821 ,  0.37394926],\n",
       "        ...,\n",
       "        [-0.20153621,  1.3292346 , -0.790056  , ..., -0.13466123,\n",
       "         -0.00956664,  0.30527624],\n",
       "        [-0.20153621,  1.3292346 , -0.790056  , ..., -0.13466123,\n",
       "         -0.00956664,  0.30527624],\n",
       "        [-0.20153621,  1.3292346 , -0.790056  , ..., -0.13466123,\n",
       "         -0.00956664,  0.30527624]],\n",
       "\n",
       "       [[-1.0424603 ,  0.63999426, -0.866225  , ..., -0.7137501 ,\n",
       "         -0.43765053,  0.19187668],\n",
       "        [-0.7620154 ,  0.952621  , -0.82321274, ..., -0.531653  ,\n",
       "         -0.3026958 ,  0.27675596],\n",
       "        [-0.76201534,  0.9526206 , -0.82321274, ..., -0.5316529 ,\n",
       "         -0.30269575,  0.2767557 ],\n",
       "        ...,\n",
       "        [-0.19807883,  1.338464  , -0.83544713, ..., -0.15759376,\n",
       "          0.0232776 ,  0.27649266],\n",
       "        [-0.19807883,  1.338464  , -0.83544713, ..., -0.15759376,\n",
       "          0.0232776 ,  0.27649266],\n",
       "        [-0.19807883,  1.338464  , -0.83544713, ..., -0.15759376,\n",
       "          0.0232776 ,  0.27649266]]], dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.matmul(gathered_tensor, layer_x.embedding_table, transpose_b = True), (4, 40, 32000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = layer_x.dense(gathered_tensor)\n",
    "l = layer_x.layer_norm(d)\n",
    "mm = tf.matmul(l, layer_x.embedding_table, transpose_b = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=-53.845802>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-95.79878>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(d[0] - d[1]), sum(d[0]-d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.18630481>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.4441756>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(l[0] - l[1]), sum(l[0]-l[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'transform/bias:0' shape=(32000,) dtype=float32, numpy=\n",
       "array([-0.3385531 , -0.27337646, -0.6101586 , ..., -0.06429594,\n",
       "       -0.06523742, -0.06201189], dtype=float32)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_x.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = mm + layer_x.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.reshape(k, shape = [ -1, 40, 32000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = np.zeros(32000)\n",
    "target_value[12382]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 32000), dtype=float32, numpy=array([[[0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(target_value, dtype = tf.float32)[tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_orig = layer_x([seg_output, masked_lm_positions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-12.617973>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.reduce_sum(tf.one_hot(inputs['masked_lm_ids'][0], depth = 32000) * logits_orig, axis = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       "array([ -7.9717956,  -9.810534 , -11.699598 , -11.021503 ,  -8.9260025,\n",
       "        -8.948836 ,  -8.158123 , -10.64055  ,  -9.918285 , -10.824669 ,\n",
       "        -7.0177984, -11.998983 ,  -3.5431592,  -5.5068183,  -2.327545 ,\n",
       "        -3.5452194,  -8.443156 , -10.519745 ,  -8.423456 ,  -9.255592 ,\n",
       "       -10.670185 , -12.049931 ,  -8.298307 , -17.980309 , -17.980309 ,\n",
       "       -17.980309 , -17.980309 , -17.980309 , -17.980309 , -17.980309 ,\n",
       "       -17.980309 , -17.980309 , -17.980309 , -17.980309 , -17.980309 ,\n",
       "       -17.980309 , -17.980309 , -17.980309 , -17.980309 , -17.980309 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.one_hot(inputs['masked_lm_ids'][0], depth = 32000) * logits_orig[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=17>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['masked_lm_ids'][0][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40, 32000), dtype=float32, numpy=\n",
       "array([[1.48039021e-08, 1.49417119e-08, 1.48754555e-08, ...,\n",
       "        6.09833160e-06, 8.42175996e-06, 3.68102610e-06],\n",
       "       [1.48186654e-08, 1.46814578e-08, 1.45875463e-08, ...,\n",
       "        5.94649600e-06, 8.69370433e-06, 4.28157273e-06],\n",
       "       [1.57232360e-08, 1.46630192e-08, 1.49479167e-08, ...,\n",
       "        5.22839855e-06, 8.96226993e-06, 5.39560051e-06],\n",
       "       ...,\n",
       "       [1.55328639e-08, 1.82006765e-08, 1.58578128e-08, ...,\n",
       "        6.68268922e-06, 8.18753688e-06, 5.43581791e-06],\n",
       "       [1.55328639e-08, 1.82006765e-08, 1.58578128e-08, ...,\n",
       "        6.68268922e-06, 8.18753688e-06, 5.43581791e-06],\n",
       "       [1.55328639e-08, 1.82006765e-08, 1.58578128e-08, ...,\n",
       "        6.68268922e-06, 8.18753688e-06, 5.43581791e-06]], dtype=float32)>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = tf.keras.losses.sparse_categorical_crossentropy(inputs['masked_lm_ids'][0], logits[0], from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       "array([ 7.971795 ,  9.810534 , 11.699598 , 11.021503 ,  8.9260025,\n",
       "        8.948836 ,  8.158123 , 10.64055  ,  9.918285 , 10.824669 ,\n",
       "        7.0177984, 11.998983 ,  3.5431592,  5.5068183,  2.327545 ,\n",
       "        3.5452194,  8.443156 , 10.519745 ,  8.423456 ,  9.255592 ,\n",
       "       10.670185 , 12.049931 ,  8.298307 , 17.980309 , 17.980309 ,\n",
       "       17.980309 , 17.980309 , 17.980309 , 17.980309 , 17.980309 ,\n",
       "       17.980309 , 17.980309 , 17.980309 , 17.980309 , 17.980309 ,\n",
       "       17.980309 , 17.980309 , 17.980309 , 17.980309 , 17.980309 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scc = tf.keras.losses.sparse_categorical_crossentropy(inputs['masked_lm_ids'][0], tf.nn.softmax(logits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       "array([ 7.9720297,  9.810738 , 11.699795 , 11.021713 ,  8.926208 ,\n",
       "        8.949032 ,  8.158358 , 10.640753 ,  9.91852  , 10.824872 ,\n",
       "        7.018003 , 11.999188 ,  3.543393 ,  5.507016 ,  2.3277202,\n",
       "        3.545422 ,  8.443391 , 10.519949 ,  8.42366  ,  9.255796 ,\n",
       "       10.67042  , 12.050126 ,  8.298504 , 16.118301 , 16.118301 ,\n",
       "       16.118301 , 16.118301 , 16.118301 , 16.118301 , 16.118301 ,\n",
       "       16.118301 , 16.118301 , 16.118301 , 16.118301 , 16.118301 ,\n",
       "       16.118301 , 16.118301 , 16.118301 , 16.118301 , 16.118301 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_numerator_loss = scc * inputs['masked_lm_weights'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_denomirator_loss = inputs['masked_lm_weights'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       "array([ 7.971795 ,  9.810534 , 11.699598 , 11.021503 ,  8.9260025,\n",
       "        8.948836 ,  8.158123 , 10.64055  ,  9.918285 , 10.824669 ,\n",
       "        7.0177984, 11.998983 ,  3.543159 ,  5.506818 ,  2.3275452,\n",
       "        3.5452194,  8.443156 , 10.519745 ,  8.423456 ,  9.255592 ,\n",
       "       10.670185 , 12.049931 ,  8.298307 ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_numerator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_denomirator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_loss = tf.math.divide_no_nan(tf.reduce_sum(lm_numerator_loss), tf.reduce_sum(lm_denomirator_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.674773>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
