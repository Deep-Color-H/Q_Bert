{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\LGCNS\\Documents\\GitHub\\Q_Bert\\QBert\\train_utils.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\LGCNS\\Documents\\GitHub\\Q_Bert\\QBert\\models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import datetime\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import train_utils, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해당 파일은 bert.run_pretraining.run_bert_pretrain을 구현하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Parameter는 FLAG 형식에서 직접 정의해주는 방식으로 변경하고, Main에서 직접 정의하도록 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Strategy 정의\n",
    "2. Input_Files 정의\n",
    "3. Bert Config 정의\n",
    "   - bert_config (1. Core_Model - Transformer Encoder - Sub Model)\n",
    "        - vocab_size\n",
    "        - type_vocab_size\n",
    "        - hidden_size\n",
    "        - max_seq_length\n",
    "        - initializer\n",
    "        - kernel_initializer\n",
    "        - initializer_range\n",
    "        - dropout_rate\n",
    "        - num_attention_heads\n",
    "        - intermediate_size\n",
    "        - intermediate_activation\n",
    "        - hidden_act\n",
    "        - attention_dropout_rate\n",
    "        - num_hidden_instances\n",
    "        - pooled_output_dim\n",
    "   - bert_config (2. Pretrained_Model - input to losses)\n",
    "        - (중복 생략)\n",
    "        - max_predictions_per_seq\n",
    "        \n",
    "3. Get Bert Model\n",
    "4. Training Config 정의\n",
    "5. Training\n",
    "6. Test\n",
    "\n",
    "\n",
    "   - init_checkpoint  # Used to initialize only the BERT submodel.\n",
    "   - max_seq_length\n",
    "   -  \n",
    "   - masked_lm\n",
    "   - model_dir\n",
    "   - num_steps_per_epoch\n",
    "   - steps_per_loop\n",
    "   - num_train_epochs\n",
    "   - learning_rate\n",
    "   - warmup_steps\n",
    "   - end_lr\n",
    "   - optimizer_type\n",
    "   - train_batch_size\n",
    "   - use_next_sentence_label\n",
    "   - train_summary_interval\n",
    "   - custom_callbacks\n",
    "   - explicit_allreduce\n",
    "   - pre_allreduce_callbacks\n",
    "   - allreduce_bytes_per_pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices() # device 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "\n",
      "장치의 수: 1\n"
     ]
    }
   ],
   "source": [
    "# Strategy 정의\n",
    "\n",
    "distribution_strategy = 'mirrored' # 'tpu'\n",
    "num_gpus = 0\n",
    "all_reduce_alg = None\n",
    "\n",
    "if distribution_strategy == 'tpu' :\n",
    "    tpu_address = \"\"\n",
    "else :\n",
    "    tpu_address = None\n",
    "\n",
    "\n",
    "\n",
    "strategy = train_utils.get_distribution_strategy(\n",
    "                  distribution_strategy=distribution_strategy,\n",
    "                  num_gpus=num_gpus,\n",
    "                  all_reduce_alg=all_reduce_alg,\n",
    "                  tpu_address=tpu_address)\n",
    "\n",
    "print ('\\n장치의 수: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 256 # 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['./Test_Examples.tfrecords']\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "# keys = feature_description.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "def _select_data_from_record(record):\n",
    "    \"\"\"Filter out features to use for pretraining.\"\"\"\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids'],\n",
    "        'masked_lm_positions': record['masked_lm_positions'],\n",
    "        'masked_lm_ids': record['masked_lm_ids'],\n",
    "        'masked_lm_weights': record['masked_lm_weights'],\n",
    "    }\n",
    "    if use_next_sentence_label:\n",
    "        x['next_sentence_labels'] = record['next_sentence_labels']\n",
    "    if use_position_id:\n",
    "        x['position_ids'] = record['position_ids']\n",
    "\n",
    "    # TODO(hongkuny): Remove the fake labels after migrating bert pretraining.\n",
    "    if output_fake_labels:\n",
    "        return (x, record['masked_lm_weights'])\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = 100\n",
    "\n",
    "GLOBAL_BATCH_SIZE = 2\n",
    "BATCH_SIZE_PER_REPLICA = np.ceil(GLOBAL_BATCH_SIZE // strategy.num_replicas_in_sync)\n",
    "\n",
    "use_next_sentence_label = True\n",
    "output_fake_labels = True\n",
    "use_position_id = False\n",
    "\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    train_dataset = train_dataset.interleave(tf.data.TFRecordDataset\n",
    "                                             , cycle_length = tf.data.experimental.AUTOTUNE\n",
    "                                             , num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    dataset_inputs = train_dataset.map(_parse_function,\n",
    "                                       num_parallel_calls=tf.data.experimental.AUTOTUNE) # String to Example\n",
    "    dataset_inputs_with_labels = dataset_inputs.map(_select_data_from_record,\n",
    "                                                    num_parallel_calls=tf.data.experimental.AUTOTUNE) # Example to InputData\n",
    "    ## 본래대로라면 그냥 써도 되지만, 현재 Label이 없는 데이터이기 때문에\n",
    "    ## max_predictions_per_seq 길이의 허위 정답 (Fake_y)를 삽입하는 mapping function이다.\n",
    "\n",
    "    dataset = dataset_inputs_with_labels\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(10000, reshuffle_each_iteration = True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "    \n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replicated_step(inputs):\n",
    "    \"\"\"Replicated training step.\"\"\"\n",
    "\n",
    "    inputs, labels = inputs\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        model_outputs = model(inputs, training=True)\n",
    "        loss = loss_fn(labels, model_outputs)\n",
    "        # Raw loss is used for reporting in metrics/logs.\n",
    "        raw_loss = loss\n",
    "    \n",
    "        if scale_loss:\n",
    "            # Scales down the loss for gradients to be invariant from replicas.\n",
    "            loss = loss / strategy.num_replicas_in_sync\n",
    "\n",
    "    if isinstance(optimizer, tf.keras.mixed_precision.experimental.LossScaleOptimizer):\n",
    "        \n",
    "        with tape:\n",
    "            scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "        scaled_grads = tape.gradient(scaled_loss, training_vars)\n",
    "        grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        grads = tape.gradient(loss, training_vars)\n",
    "        optimizer.apply_gradients(zip(grads, training_vars))\n",
    "    \n",
    "    # For reporting, the metric takes the mean of losses.\n",
    "    train_loss_metric.update_state(raw_loss)\n",
    "    \n",
    "    for metric in train_metrics:\n",
    "        metric.update_state(labels, model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(iterator, steps):\n",
    "    \n",
    "#     def step_fn(inputs):\n",
    "#         features, labels = inputs\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "            \n",
    "#             losses = model(features)    \n",
    "#             losses = losses * (1.0 / GLOBAL_BATCH_SIZE)\n",
    "\n",
    "#         grads = tape.gradient(losses, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        \n",
    "#         return losses\n",
    "\n",
    "    if not isinstance(steps, tf.Tensor):\n",
    "        raise ValueError('steps should be an Tensor. Python object may cause '\n",
    "                     'retracing.')\n",
    "\n",
    "    for _ in tf.range(steps):\n",
    "        strategy.run(_replicated_step, args=(next(iterator),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_step(iterator):\n",
    "    \"\"\"Performs a distributed training step.\n",
    "\n",
    "    Args:\n",
    "    iterator: the distributed iterator of training datasets.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
    "    \"\"\"\n",
    "    strategy.run(_replicated_step, args=(next(iterator),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps_to_run(current_step, steps_per_epoch, steps_per_loop):\n",
    "    \"\"\"Calculates steps to run on device.\"\"\"\n",
    "    if steps_per_loop <= 0:\n",
    "        raise ValueError('steps_per_loop should be positive integer.')\n",
    "    if steps_per_loop == 1:\n",
    "        return steps_per_loop\n",
    "    remainder_in_epoch = current_step % steps_per_epoch\n",
    "    if remainder_in_epoch != 0:\n",
    "        return min(steps_per_epoch - remainder_in_epoch, steps_per_loop)\n",
    "    else:\n",
    "        return steps_per_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def loss_fn(fake_y, losses, **unused_args) :\n",
    "    \n",
    "    return tf.reduce_mean(losses, axis = -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_step = tf.function(train_single_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 25 #25000\n",
    "steps_between_evals = 10 #\n",
    "steps_per_loop = 1\n",
    "epochs = 50\n",
    "\n",
    "total_training_steps = steps_per_epoch * epochs\n",
    "eval_loss_metric = tf.keras.metrics.Mean('traning loss', dtype = tf.float32)\n",
    "train_loss_metric = tf.keras.metrics.Mean('traning loss', dtype = tf.float32)\n",
    "\n",
    "scale_loss = True if strategy.num_replicas_in_sync > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "## Learning Rate Decay\n",
    "\n",
    "# lr = 1e-4 warmup stage (step <= 10000)\n",
    "# Decay linearly\n",
    "\n",
    "init_lr = 1e-4\n",
    "warmup_steps = int(total_training_steps // 10)\n",
    "end_lr = 0\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "      initial_learning_rate=init_lr,\n",
    "      decay_steps=total_training_steps,\n",
    "      end_learning_rate=end_lr)\n",
    "\n",
    "lr_schedule = train_utils.WarmUp(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_schedule_fn=lr_schedule,\n",
    "        warmup_steps=warmup_steps)\n",
    "\n",
    "optimizer = train_utils.AdamWeightDecay( \n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-6,\n",
    "    exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope() :\n",
    "\n",
    "    model, sub_model = models.get_bert_models_fn(vocab_size\n",
    "                                             , hidden_size\n",
    "                                             , type_vocab_size\n",
    "                                             , num_layers\n",
    "                                             , num_attention_heads\n",
    "                                             , max_seq_length\n",
    "                                             , max_predictions_per_seq\n",
    "                                             , dropout_rate\n",
    "                                             , inner_dim \n",
    "                                             , initializer)\n",
    "    model.compile(optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Set\n",
    "train_iterator = iter(dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_metrics = model.metrics\n",
    "train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "\n",
    "# callback\n",
    "\n",
    "## model checkpoint\n",
    "t = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "checkpoint_dir = './training_checkpoints_{}'.format(t)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "model_cp = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),\n",
    "\n",
    "## Learning Rate Print\n",
    "callbacks = [model_cp]\n",
    "\n",
    "# callbacks = tf.keras.callbacks.CallbackList(callbacks) # tf 2.5.0 제공\n",
    "\n",
    "\n",
    "optimizer = model.optimizer\n",
    "training_vars = model.trainable_variables\n",
    "\n",
    "# Callback\n",
    "for cb in callbacks :\n",
    "    cb[0].model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_step = optimizer.iterations.numpy()\n",
    "current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-3000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 11/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1364, MLM_ACC : 0.4976, MLM_LOSS : 0.5086, NSP_ACC : 0.4981, NSP_LOSS : 0.4991, \n",
      "\n",
      "Train Step: 12/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0693, MLM_ACC : 0.5036, MLM_LOSS : 0.5147, NSP_ACC : 0.5041, NSP_LOSS : 0.5050, \n",
      "\n",
      "Train Step: 13/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0233, MLM_ACC : 0.5060, MLM_LOSS : 0.5172, NSP_ACC : 0.5065, NSP_LOSS : 0.5074, \n",
      "\n",
      "Train Step: 14/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0664, MLM_ACC : 0.5205, MLM_LOSS : 0.5316, NSP_ACC : 0.5208, NSP_LOSS : 0.5219, \n",
      "\n",
      "Train Step: 15/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0124, MLM_ACC : 0.5223, MLM_LOSS : 0.5334, NSP_ACC : 0.5226, NSP_LOSS : 0.5237, \n",
      "\n",
      "Train Step: 16/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9828, MLM_ACC : 0.5299, MLM_LOSS : 0.5411, NSP_ACC : 0.5303, NSP_LOSS : 0.5313, \n",
      "\n",
      "Train Step: 17/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9287, MLM_ACC : 0.5283, MLM_LOSS : 0.5395, NSP_ACC : 0.5287, NSP_LOSS : 0.5296, \n",
      "\n",
      "Train Step: 18/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8867, MLM_ACC : 0.5502, MLM_LOSS : 0.5615, NSP_ACC : 0.5507, NSP_LOSS : 0.5515, \n",
      "\n",
      "Train Step: 19/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8401, MLM_ACC : 0.5447, MLM_LOSS : 0.5560, NSP_ACC : 0.5451, NSP_LOSS : 0.5459, \n",
      "\n",
      "Train Step: 20/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8286, MLM_ACC : 0.5364, MLM_LOSS : 0.5476, NSP_ACC : 0.5368, NSP_LOSS : 0.5376, \n",
      "======MLM TEST======\n",
      "['오로', '도치기', '탑승', '아침', '##에게', '두었다', '넣', '가격을', '개척', '인하여', '5일에', '환자가', '준우승', '장식되어', 'sil', '출시되었다', '빌보드', '그후', '##팹', '고아', '##담은', '사토', '발전', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가', '단어가']\n",
      "['##아', '##까지', '뭐', '##은', '김', '사용', '##타는', '곳이', '##하여', '한', '##산', '태백', '##까지', '오리지널', '히트를', '나란히', '것이다', '##되고', '타', '##섯', '기록한', '밤', '줄어들', '##국', ')', '각색', '은', '##래', '레코드', '##현', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 21/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7908, MLM_ACC : 0.5442, MLM_LOSS : 0.5555, NSP_ACC : 0.5447, NSP_LOSS : 0.5453, \n",
      "\n",
      "Train Step: 22/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8035, MLM_ACC : 0.5264, MLM_LOSS : 0.5377, NSP_ACC : 0.5269, NSP_LOSS : 0.5276, \n",
      "\n",
      "Train Step: 23/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7854, MLM_ACC : 0.5295, MLM_LOSS : 0.5408, NSP_ACC : 0.5299, NSP_LOSS : 0.5307, \n",
      "\n",
      "Train Step: 24/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7967, MLM_ACC : 0.5309, MLM_LOSS : 0.5422, NSP_ACC : 0.5313, NSP_LOSS : 0.5320, \n",
      "\n",
      "Train Step: 25/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7648, MLM_ACC : 0.5316, MLM_LOSS : 0.5429, NSP_ACC : 0.5321, NSP_LOSS : 0.5327, \n",
      "\n",
      "Train Step: 26/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7457, MLM_ACC : 0.5304, MLM_LOSS : 0.5417, NSP_ACC : 0.5309, NSP_LOSS : 0.5315, \n",
      "\n",
      "Train Step: 27/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8102, MLM_ACC : 0.5365, MLM_LOSS : 0.5478, NSP_ACC : 0.5370, NSP_LOSS : 0.5376, \n",
      "\n",
      "Train Step: 28/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7887, MLM_ACC : 0.5416, MLM_LOSS : 0.5529, NSP_ACC : 0.5421, NSP_LOSS : 0.5427, \n",
      "\n",
      "Train Step: 29/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8267, MLM_ACC : 0.5353, MLM_LOSS : 0.5466, NSP_ACC : 0.5357, NSP_LOSS : 0.5365, \n",
      "\n",
      "Train Step: 30/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9188, MLM_ACC : 0.5394, MLM_LOSS : 0.5506, NSP_ACC : 0.5399, NSP_LOSS : 0.5407, \n",
      "======MLM TEST======\n",
      "['극복', '들어서는', '##게는', '수컷이', '고아', '염', '쇼핑', '처칠', '연봉', '##덨', '기업', '밀리', 'formula12', '양식의', '##쌩', '주주', '165', '직책', '정류', '##문과', '우려가', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언', '이언']\n",
      "['중부', '##와', '##나', '##아프리카', '몸길이는', '몸길이', '귀가', '유라시아', ',', '.', '20', '##로', '텃', '있다', '##이다', '보통', '##성', '##적으로', '##아시아', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 31/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0764, MLM_ACC : 0.5410, MLM_LOSS : 0.5521, NSP_ACC : 0.5414, NSP_LOSS : 0.5424, \n",
      "\n",
      "Train Step: 32/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0358, MLM_ACC : 0.5426, MLM_LOSS : 0.5537, NSP_ACC : 0.5431, NSP_LOSS : 0.5440, \n",
      "\n",
      "Train Step: 33/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0046, MLM_ACC : 0.5522, MLM_LOSS : 0.5633, NSP_ACC : 0.5526, NSP_LOSS : 0.5536, \n",
      "\n",
      "Train Step: 34/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9889, MLM_ACC : 0.5584, MLM_LOSS : 0.5696, NSP_ACC : 0.5588, NSP_LOSS : 0.5598, \n",
      "\n",
      "Train Step: 35/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0153, MLM_ACC : 0.5536, MLM_LOSS : 0.5647, NSP_ACC : 0.5539, NSP_LOSS : 0.5549, \n",
      "\n",
      "Train Step: 36/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0663, MLM_ACC : 0.5542, MLM_LOSS : 0.5653, NSP_ACC : 0.5546, NSP_LOSS : 0.5557, \n",
      "\n",
      "Train Step: 37/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0569, MLM_ACC : 0.5535, MLM_LOSS : 0.5645, NSP_ACC : 0.5538, NSP_LOSS : 0.5549, \n",
      "\n",
      "Train Step: 38/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0217, MLM_ACC : 0.5569, MLM_LOSS : 0.5680, NSP_ACC : 0.5573, NSP_LOSS : 0.5583, \n",
      "\n",
      "Train Step: 39/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0907, MLM_ACC : 0.5531, MLM_LOSS : 0.5642, NSP_ACC : 0.5535, NSP_LOSS : 0.5546, \n",
      "\n",
      "Train Step: 40/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0615, MLM_ACC : 0.5514, MLM_LOSS : 0.5624, NSP_ACC : 0.5517, NSP_LOSS : 0.5528, \n",
      "======MLM TEST======\n",
      "['뉴캐슬', '##껙', 'ebs', '사단의', '지역구', '##수사', '비롯하여', '이르는', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를', '대우를']\n",
      "['초대', '총장', '##화', '.', '받았으며', '##학원', '제주', '##이', '박사', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 41/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0285, MLM_ACC : 0.5600, MLM_LOSS : 0.5711, NSP_ACC : 0.5604, NSP_LOSS : 0.5614, \n",
      "\n",
      "Train Step: 42/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9993, MLM_ACC : 0.5635, MLM_LOSS : 0.5746, NSP_ACC : 0.5639, NSP_LOSS : 0.5649, \n",
      "\n",
      "Train Step: 43/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0343, MLM_ACC : 0.5599, MLM_LOSS : 0.5710, NSP_ACC : 0.5603, NSP_LOSS : 0.5613, \n",
      "\n",
      "Train Step: 44/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0343, MLM_ACC : 0.5531, MLM_LOSS : 0.5642, NSP_ACC : 0.5536, NSP_LOSS : 0.5545, \n",
      "\n",
      "Train Step: 45/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0116, MLM_ACC : 0.5477, MLM_LOSS : 0.5588, NSP_ACC : 0.5482, NSP_LOSS : 0.5491, \n",
      "\n",
      "Train Step: 46/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0226, MLM_ACC : 0.5475, MLM_LOSS : 0.5586, NSP_ACC : 0.5479, NSP_LOSS : 0.5488, \n",
      "\n",
      "Train Step: 47/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9952, MLM_ACC : 0.5525, MLM_LOSS : 0.5636, NSP_ACC : 0.5530, NSP_LOSS : 0.5538, \n",
      "\n",
      "Train Step: 48/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0117, MLM_ACC : 0.5607, MLM_LOSS : 0.5718, NSP_ACC : 0.5612, NSP_LOSS : 0.5621, \n",
      "\n",
      "Train Step: 49/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0303, MLM_ACC : 0.5610, MLM_LOSS : 0.5721, NSP_ACC : 0.5615, NSP_LOSS : 0.5624, \n",
      "\n",
      "Train Step: 50/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0306, MLM_ACC : 0.5646, MLM_LOSS : 0.5756, NSP_ACC : 0.5650, NSP_LOSS : 0.5659, \n",
      "======MLM TEST======\n",
      "['창녕', '원로원', '깯', '6개', '확정된', '공화국을', '1994', '나누는', '방사선', '스베', '출발하여', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012']\n",
      "['##에', '17', '득점', '##와의', '골로', '번째', '62', 'ac', '국가대표팀의', '98', '경력', '57', '국가대표팀의', '.', '부상', ',', '리그', '##대항', '##하였지만', '2000', '월드컵', '힘들', '큰', '중심', '득점', ',', '63', '전망', '뮐', '첫', '25', '밀란', '-', '감독이', '브라질', '2000년', '##기는', '1998년', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 51/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0765, MLM_ACC : 0.5601, MLM_LOSS : 0.5712, NSP_ACC : 0.5606, NSP_LOSS : 0.5616, \n",
      "\n",
      "Train Step: 52/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1449, MLM_ACC : 0.5626, MLM_LOSS : 0.5736, NSP_ACC : 0.5631, NSP_LOSS : 0.5641, \n",
      "\n",
      "Train Step: 53/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1113, MLM_ACC : 0.5624, MLM_LOSS : 0.5734, NSP_ACC : 0.5629, NSP_LOSS : 0.5639, \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 54/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1494, MLM_ACC : 0.5589, MLM_LOSS : 0.5699, NSP_ACC : 0.5594, NSP_LOSS : 0.5604, \n",
      "\n",
      "Train Step: 55/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2060, MLM_ACC : 0.5615, MLM_LOSS : 0.5725, NSP_ACC : 0.5620, NSP_LOSS : 0.5631, \n",
      "\n",
      "Train Step: 56/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1740, MLM_ACC : 0.5572, MLM_LOSS : 0.5681, NSP_ACC : 0.5577, NSP_LOSS : 0.5587, \n",
      "\n",
      "Train Step: 57/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4065, MLM_ACC : 0.5650, MLM_LOSS : 0.5757, NSP_ACC : 0.5654, NSP_LOSS : 0.5667, \n",
      "\n",
      "Train Step: 58/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3780, MLM_ACC : 0.5612, MLM_LOSS : 0.5719, NSP_ACC : 0.5617, NSP_LOSS : 0.5629, \n",
      "\n",
      "Train Step: 59/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3454, MLM_ACC : 0.5592, MLM_LOSS : 0.5700, NSP_ACC : 0.5597, NSP_LOSS : 0.5609, \n",
      "\n",
      "Train Step: 60/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3140, MLM_ACC : 0.5589, MLM_LOSS : 0.5697, NSP_ACC : 0.5594, NSP_LOSS : 0.5606, \n",
      "======MLM TEST======\n",
      "['##븀', '많은데', '중앙에', '개최하는', '방영되었다', '켜', '호찌', '면적을', '유럽으로', '평안남도', '파기', '감소로', 'pd', '영국령', '1979년', '인근의', '큠', '대학생', '돍', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일', '28일']\n",
      "['##하여', '약', '그가', '##슨', ',', '1870년', '49', '미국', ',', '##러스', '세운', '해산', '##조', '##업에', '전념', '그', '오일', '달러', '해산', '그의', '소유', '회사를', '##드', ',', '##억', '법', '받지', '대학교를', '##교회', '사업에', '근무', '조직하여', '세계', '두고', '석유', '형편', '7월', '는', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 61/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2863, MLM_ACC : 0.5566, MLM_LOSS : 0.5675, NSP_ACC : 0.5572, NSP_LOSS : 0.5583, \n",
      "\n",
      "Train Step: 62/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4155, MLM_ACC : 0.5565, MLM_LOSS : 0.5672, NSP_ACC : 0.5570, NSP_LOSS : 0.5582, \n",
      "\n",
      "Train Step: 63/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3894, MLM_ACC : 0.5567, MLM_LOSS : 0.5674, NSP_ACC : 0.5573, NSP_LOSS : 0.5585, \n",
      "\n",
      "Train Step: 64/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4572, MLM_ACC : 0.5649, MLM_LOSS : 0.5756, NSP_ACC : 0.5655, NSP_LOSS : 0.5668, \n",
      "\n",
      "Train Step: 65/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4299, MLM_ACC : 0.5657, MLM_LOSS : 0.5764, NSP_ACC : 0.5663, NSP_LOSS : 0.5675, \n",
      "\n",
      "Train Step: 66/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4110, MLM_ACC : 0.5655, MLM_LOSS : 0.5762, NSP_ACC : 0.5661, NSP_LOSS : 0.5673, \n",
      "\n",
      "Train Step: 67/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3960, MLM_ACC : 0.5615, MLM_LOSS : 0.5722, NSP_ACC : 0.5621, NSP_LOSS : 0.5633, \n",
      "\n",
      "Train Step: 68/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4406, MLM_ACC : 0.5618, MLM_LOSS : 0.5724, NSP_ACC : 0.5623, NSP_LOSS : 0.5636, \n",
      "\n",
      "Train Step: 69/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4579, MLM_ACC : 0.5584, MLM_LOSS : 0.5691, NSP_ACC : 0.5589, NSP_LOSS : 0.5602, \n",
      "\n",
      "Train Step: 70/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.5603, MLM_ACC : 0.5671, MLM_LOSS : 0.5777, NSP_ACC : 0.5676, NSP_LOSS : 0.5690, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '가든', '.', '가든', '.', '.', '##힘', '##pm', '기여를', '.', '.', '.', '##pm', '.', '.', '.', '.', '가든', '.', '.', '##비에', '.', '조정의', '발휘', '대사관', '##pm', '죈', '중', '가든', '.', '##pm', '가든', '##비에', '##비에', '##비에', '전체의', '전체의']\n",
      "['기록', '이', '저서', '2002년', '##약에', '투표', '##직', '##하였다', '##하기도', '##하였지만', '##에서', '##렌', '##에', '후보가', '대한', '당일', '그의', '##에게', '나폴레옹의', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.026315782219171524\n",
      "\n",
      "Train Step: 71/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.5475, MLM_ACC : 0.5662, MLM_LOSS : 0.5768, NSP_ACC : 0.5667, NSP_LOSS : 0.5681, \n",
      "\n",
      "Train Step: 72/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.5282, MLM_ACC : 0.5629, MLM_LOSS : 0.5735, NSP_ACC : 0.5635, NSP_LOSS : 0.5648, \n",
      "\n",
      "Train Step: 73/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.5214, MLM_ACC : 0.5680, MLM_LOSS : 0.5786, NSP_ACC : 0.5685, NSP_LOSS : 0.5699, \n",
      "\n",
      "Train Step: 74/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.5013, MLM_ACC : 0.5726, MLM_LOSS : 0.5832, NSP_ACC : 0.5731, NSP_LOSS : 0.5744, \n",
      "\n",
      "Train Step: 75/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4898, MLM_ACC : 0.5765, MLM_LOSS : 0.5871, NSP_ACC : 0.5770, NSP_LOSS : 0.5783, \n",
      "\n",
      "Train Step: 76/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4632, MLM_ACC : 0.5780, MLM_LOSS : 0.5886, NSP_ACC : 0.5785, NSP_LOSS : 0.5798, \n",
      "\n",
      "Train Step: 77/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4411, MLM_ACC : 0.5771, MLM_LOSS : 0.5877, NSP_ACC : 0.5776, NSP_LOSS : 0.5789, \n",
      "\n",
      "Train Step: 78/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4179, MLM_ACC : 0.5765, MLM_LOSS : 0.5872, NSP_ACC : 0.5770, NSP_LOSS : 0.5783, \n",
      "\n",
      "Train Step: 79/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3909, MLM_ACC : 0.5765, MLM_LOSS : 0.5872, NSP_ACC : 0.5770, NSP_LOSS : 0.5783, \n",
      "\n",
      "Train Step: 80/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3915, MLM_ACC : 0.5780, MLM_LOSS : 0.5887, NSP_ACC : 0.5786, NSP_LOSS : 0.5798, \n",
      "======MLM TEST======\n",
      "['놓인', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '놓인', '.', '.', '.', '놓인', '.', '.', '.', '.', '보유하고', '.', '.', '.', '.', '##pm', '.', '보유하고', '.', '.', '.', '구분이', '구분이']\n",
      "['나', '있다', '##와', '1', '##사무소', '고속도로', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.07894734293222427\n",
      "\n",
      "Train Step: 81/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3992, MLM_ACC : 0.5759, MLM_LOSS : 0.5865, NSP_ACC : 0.5764, NSP_LOSS : 0.5777, \n",
      "\n",
      "Train Step: 82/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3767, MLM_ACC : 0.5752, MLM_LOSS : 0.5859, NSP_ACC : 0.5758, NSP_LOSS : 0.5770, \n",
      "\n",
      "Train Step: 83/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3890, MLM_ACC : 0.5760, MLM_LOSS : 0.5867, NSP_ACC : 0.5766, NSP_LOSS : 0.5778, \n",
      "\n",
      "Train Step: 84/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3648, MLM_ACC : 0.5751, MLM_LOSS : 0.5858, NSP_ACC : 0.5757, NSP_LOSS : 0.5769, \n",
      "\n",
      "Train Step: 85/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3404, MLM_ACC : 0.5722, MLM_LOSS : 0.5829, NSP_ACC : 0.5727, NSP_LOSS : 0.5739, \n",
      "\n",
      "Train Step: 86/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3380, MLM_ACC : 0.5750, MLM_LOSS : 0.5858, NSP_ACC : 0.5756, NSP_LOSS : 0.5768, \n",
      "\n",
      "Train Step: 87/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4081, MLM_ACC : 0.5744, MLM_LOSS : 0.5851, NSP_ACC : 0.5750, NSP_LOSS : 0.5762, \n",
      "\n",
      "Train Step: 88/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4501, MLM_ACC : 0.5766, MLM_LOSS : 0.5872, NSP_ACC : 0.5771, NSP_LOSS : 0.5785, \n",
      "\n",
      "Train Step: 89/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4455, MLM_ACC : 0.5774, MLM_LOSS : 0.5880, NSP_ACC : 0.5779, NSP_LOSS : 0.5793, \n",
      "\n",
      "Train Step: 90/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4250, MLM_ACC : 0.5796, MLM_LOSS : 0.5902, NSP_ACC : 0.5802, NSP_LOSS : 0.5815, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '차가', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['자기', '##에서', '원조를', '파일럿', '##화를', ',', '##에', '정해', '##지면서', '창설', '##놀', '셔', '바다', '-', '셔', '좀', '더', '흑', '##를', '아는', '대한', '##은', '##공', '.', '##키아', '##한', '범위', '넓', '##있다', '인도', '부족', '##리아', '현대', '플라', '##즈', '##욱', '##활동을', '남쪽으로는', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.19999960064888\n",
      "\n",
      "Train Step: 91/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4092, MLM_ACC : 0.5819, MLM_LOSS : 0.5925, NSP_ACC : 0.5824, NSP_LOSS : 0.5837, \n",
      "\n",
      "Train Step: 92/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3789, MLM_ACC : 0.5785, MLM_LOSS : 0.5892, NSP_ACC : 0.5791, NSP_LOSS : 0.5804, \n",
      "\n",
      "Train Step: 93/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3575, MLM_ACC : 0.5806, MLM_LOSS : 0.5913, NSP_ACC : 0.5812, NSP_LOSS : 0.5825, \n",
      "\n",
      "Train Step: 94/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4044, MLM_ACC : 0.5833, MLM_LOSS : 0.5939, NSP_ACC : 0.5838, NSP_LOSS : 0.5851, \n",
      "\n",
      "Train Step: 95/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4445, MLM_ACC : 0.5838, MLM_LOSS : 0.5944, NSP_ACC : 0.5844, NSP_LOSS : 0.5857, \n",
      "\n",
      "Train Step: 96/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4219, MLM_ACC : 0.5869, MLM_LOSS : 0.5975, NSP_ACC : 0.5874, NSP_LOSS : 0.5887, \n",
      "\n",
      "Train Step: 97/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3951, MLM_ACC : 0.5866, MLM_LOSS : 0.5972, NSP_ACC : 0.5871, NSP_LOSS : 0.5884, \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 98/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4248, MLM_ACC : 0.5862, MLM_LOSS : 0.5968, NSP_ACC : 0.5868, NSP_LOSS : 0.5881, \n",
      "\n",
      "Train Step: 99/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4222, MLM_ACC : 0.5882, MLM_LOSS : 0.5988, NSP_ACC : 0.5888, NSP_LOSS : 0.5901, \n",
      "\n",
      "Train Step: 100/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4114, MLM_ACC : 0.5896, MLM_LOSS : 0.6002, NSP_ACC : 0.5901, NSP_LOSS : 0.5915, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['썼으며', '##곳', '등이다', '##도', '종류', '.', ',', '##에는', '##지를', '##거나', '##는', '중기', '##톱', '##개', '##망', '돌', ',', '석', '##석기', '##나', '동물을', '##지', '1만', '##날', '##각', '뿔', '##한다', '돌', '쐐', ',', '나뉘', '##원', '##날', '구석기', '있는데', '##기', '##와', '썼다', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.12499985098838806\n",
      "\n",
      "Train Step: 101/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3896, MLM_ACC : 0.5875, MLM_LOSS : 0.5981, NSP_ACC : 0.5881, NSP_LOSS : 0.5894, \n",
      "\n",
      "Train Step: 102/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4449, MLM_ACC : 0.5889, MLM_LOSS : 0.5995, NSP_ACC : 0.5895, NSP_LOSS : 0.5908, \n",
      "\n",
      "Train Step: 103/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4580, MLM_ACC : 0.5864, MLM_LOSS : 0.5969, NSP_ACC : 0.5869, NSP_LOSS : 0.5883, \n",
      "\n",
      "Train Step: 104/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4344, MLM_ACC : 0.5892, MLM_LOSS : 0.5997, NSP_ACC : 0.5897, NSP_LOSS : 0.5911, \n",
      "\n",
      "Train Step: 105/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4097, MLM_ACC : 0.5919, MLM_LOSS : 0.6025, NSP_ACC : 0.5925, NSP_LOSS : 0.5938, \n",
      "\n",
      "Train Step: 106/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4240, MLM_ACC : 0.5893, MLM_LOSS : 0.5999, NSP_ACC : 0.5899, NSP_LOSS : 0.5912, \n",
      "\n",
      "Train Step: 107/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4124, MLM_ACC : 0.5909, MLM_LOSS : 0.6014, NSP_ACC : 0.5914, NSP_LOSS : 0.5928, \n",
      "\n",
      "Train Step: 108/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.4066, MLM_ACC : 0.5877, MLM_LOSS : 0.5982, NSP_ACC : 0.5882, NSP_LOSS : 0.5896, \n",
      "\n",
      "Train Step: 109/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3848, MLM_ACC : 0.5876, MLM_LOSS : 0.5981, NSP_ACC : 0.5881, NSP_LOSS : 0.5894, \n",
      "\n",
      "Train Step: 110/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3680, MLM_ACC : 0.5874, MLM_LOSS : 0.5979, NSP_ACC : 0.5879, NSP_LOSS : 0.5892, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['##에', '단', '##출', '##의', '민족', '##국', '성씨', '위', '.', '조사', '고대', '8', '한양', '끼', '.', '위', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.07894734293222427\n",
      "\n",
      "Train Step: 111/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3465, MLM_ACC : 0.5884, MLM_LOSS : 0.5990, NSP_ACC : 0.5889, NSP_LOSS : 0.5902, \n",
      "\n",
      "Train Step: 112/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3272, MLM_ACC : 0.5895, MLM_LOSS : 0.6002, NSP_ACC : 0.5901, NSP_LOSS : 0.5914, \n",
      "\n",
      "Train Step: 113/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3380, MLM_ACC : 0.5854, MLM_LOSS : 0.5960, NSP_ACC : 0.5860, NSP_LOSS : 0.5873, \n",
      "\n",
      "Train Step: 114/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3602, MLM_ACC : 0.5833, MLM_LOSS : 0.5939, NSP_ACC : 0.5839, NSP_LOSS : 0.5852, \n",
      "\n",
      "Train Step: 115/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3339, MLM_ACC : 0.5842, MLM_LOSS : 0.5948, NSP_ACC : 0.5848, NSP_LOSS : 0.5861, \n",
      "\n",
      "Train Step: 116/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3111, MLM_ACC : 0.5837, MLM_LOSS : 0.5943, NSP_ACC : 0.5843, NSP_LOSS : 0.5856, \n",
      "\n",
      "Train Step: 117/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3097, MLM_ACC : 0.5824, MLM_LOSS : 0.5930, NSP_ACC : 0.5830, NSP_LOSS : 0.5843, \n",
      "\n",
      "Train Step: 118/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3119, MLM_ACC : 0.5820, MLM_LOSS : 0.5926, NSP_ACC : 0.5826, NSP_LOSS : 0.5839, \n",
      "\n",
      "Train Step: 119/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2889, MLM_ACC : 0.5820, MLM_LOSS : 0.5925, NSP_ACC : 0.5825, NSP_LOSS : 0.5838, \n",
      "\n",
      "Train Step: 120/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2863, MLM_ACC : 0.5803, MLM_LOSS : 0.5909, NSP_ACC : 0.5808, NSP_LOSS : 0.5821, \n",
      "======MLM TEST======\n",
      "[',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "[',', '간을', '이타', '정비', \"'\", '당', '한신', '부흥', '후쿠', ',', '및', '.', '##성이', '##나', '그', '##국', '##산', '계획', '효고현', '##하기', '한신', '구성된', '계획', '##지', '있다', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 121/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2647, MLM_ACC : 0.5785, MLM_LOSS : 0.5891, NSP_ACC : 0.5791, NSP_LOSS : 0.5804, \n",
      "\n",
      "Train Step: 122/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2958, MLM_ACC : 0.5792, MLM_LOSS : 0.5897, NSP_ACC : 0.5797, NSP_LOSS : 0.5810, \n",
      "\n",
      "Train Step: 123/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2879, MLM_ACC : 0.5779, MLM_LOSS : 0.5885, NSP_ACC : 0.5785, NSP_LOSS : 0.5798, \n",
      "\n",
      "Train Step: 124/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2689, MLM_ACC : 0.5778, MLM_LOSS : 0.5884, NSP_ACC : 0.5784, NSP_LOSS : 0.5797, \n",
      "\n",
      "Train Step: 125/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2868, MLM_ACC : 0.5785, MLM_LOSS : 0.5890, NSP_ACC : 0.5790, NSP_LOSS : 0.5804, \n",
      "\n",
      "Train Step: 126/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2912, MLM_ACC : 0.5791, MLM_LOSS : 0.5896, NSP_ACC : 0.5796, NSP_LOSS : 0.5810, \n",
      "\n",
      "Train Step: 127/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3365, MLM_ACC : 0.5794, MLM_LOSS : 0.5899, NSP_ACC : 0.5800, NSP_LOSS : 0.5814, \n",
      "\n",
      "Train Step: 128/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3662, MLM_ACC : 0.5800, MLM_LOSS : 0.5905, NSP_ACC : 0.5806, NSP_LOSS : 0.5820, \n",
      "\n",
      "Train Step: 129/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3454, MLM_ACC : 0.5801, MLM_LOSS : 0.5906, NSP_ACC : 0.5807, NSP_LOSS : 0.5821, \n",
      "\n",
      "Train Step: 130/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3332, MLM_ACC : 0.5802, MLM_LOSS : 0.5907, NSP_ACC : 0.5807, NSP_LOSS : 0.5821, \n",
      "======MLM TEST======\n",
      "[',', '.', '.', ',', ',', ',', ',', ',', '.', '.', ',', '.', '.', ',', '.', ',', ',', ',', '.', ',', '.', '.', ',', '.', ',', ',', ',', ',', ',', '.', ',', '.', ',', ',', '.', '.', ',', ',', ',', ',']\n",
      "['5월', '입단', '수비수로', '##탄을', '##총', 'fc', '연', '늘어', '차이가', '부품', '후', '##총은', ',', '##로', '.', ',', '대한민국의', '포지션은', '2006년', '설계', '포지션은', '##더와', '서울', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.05263156443834305\n",
      "\n",
      "Train Step: 131/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3165, MLM_ACC : 0.5805, MLM_LOSS : 0.5910, NSP_ACC : 0.5811, NSP_LOSS : 0.5825, \n",
      "\n",
      "Train Step: 132/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3160, MLM_ACC : 0.5833, MLM_LOSS : 0.5938, NSP_ACC : 0.5839, NSP_LOSS : 0.5853, \n",
      "\n",
      "Train Step: 133/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3217, MLM_ACC : 0.5832, MLM_LOSS : 0.5936, NSP_ACC : 0.5837, NSP_LOSS : 0.5851, \n",
      "\n",
      "Train Step: 134/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3262, MLM_ACC : 0.5860, MLM_LOSS : 0.5965, NSP_ACC : 0.5866, NSP_LOSS : 0.5880, \n",
      "\n",
      "Train Step: 135/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3283, MLM_ACC : 0.5888, MLM_LOSS : 0.5992, NSP_ACC : 0.5893, NSP_LOSS : 0.5907, \n",
      "\n",
      "Train Step: 136/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3069, MLM_ACC : 0.5890, MLM_LOSS : 0.5995, NSP_ACC : 0.5895, NSP_LOSS : 0.5909, \n",
      "\n",
      "Train Step: 137/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.3041, MLM_ACC : 0.5879, MLM_LOSS : 0.5984, NSP_ACC : 0.5884, NSP_LOSS : 0.5899, \n",
      "\n",
      "Train Step: 138/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2899, MLM_ACC : 0.5876, MLM_LOSS : 0.5981, NSP_ACC : 0.5881, NSP_LOSS : 0.5895, \n",
      "\n",
      "Train Step: 139/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2753, MLM_ACC : 0.5898, MLM_LOSS : 0.6003, NSP_ACC : 0.5903, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 140/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2585, MLM_ACC : 0.5895, MLM_LOSS : 0.6000, NSP_ACC : 0.5901, NSP_LOSS : 0.5915, \n",
      "======MLM TEST======\n",
      "['.', '.', ',', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['병을', '가이', ',', '창단', '2월', '##야', '##가', '같은', '##사', '마쓰다이라', '.', '가의', '갑자기', '했다', '##이', '2006년', '##을', '고', '기요', '##을', ',', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 141/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2429, MLM_ACC : 0.5896, MLM_LOSS : 0.6002, NSP_ACC : 0.5902, NSP_LOSS : 0.5916, \n",
      "\n",
      "Train Step: 142/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2258, MLM_ACC : 0.5898, MLM_LOSS : 0.6003, NSP_ACC : 0.5903, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 143/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2247, MLM_ACC : 0.5904, MLM_LOSS : 0.6009, NSP_ACC : 0.5909, NSP_LOSS : 0.5923, \n",
      "\n",
      "Train Step: 144/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2299, MLM_ACC : 0.5910, MLM_LOSS : 0.6015, NSP_ACC : 0.5915, NSP_LOSS : 0.5929, \n",
      "\n",
      "Train Step: 145/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2339, MLM_ACC : 0.5900, MLM_LOSS : 0.6006, NSP_ACC : 0.5906, NSP_LOSS : 0.5920, \n",
      "\n",
      "Train Step: 146/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2167, MLM_ACC : 0.5900, MLM_LOSS : 0.6005, NSP_ACC : 0.5905, NSP_LOSS : 0.5919, \n",
      "\n",
      "Train Step: 147/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2153, MLM_ACC : 0.5904, MLM_LOSS : 0.6009, NSP_ACC : 0.5909, NSP_LOSS : 0.5923, \n",
      "\n",
      "Train Step: 148/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.2008, MLM_ACC : 0.5893, MLM_LOSS : 0.5998, NSP_ACC : 0.5899, NSP_LOSS : 0.5912, \n",
      "\n",
      "Train Step: 149/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1925, MLM_ACC : 0.5878, MLM_LOSS : 0.5984, NSP_ACC : 0.5884, NSP_LOSS : 0.5898, \n",
      "\n",
      "Train Step: 150/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1936, MLM_ACC : 0.5873, MLM_LOSS : 0.5978, NSP_ACC : 0.5879, NSP_LOSS : 0.5893, \n",
      "======MLM TEST======\n",
      "[',', '.', '.', '.', ',', '.', ',', '.', '.', ',', ',', '.', '.', '.', '.', '.', ',', '.', '.', ',', ',', '.', ',', '.', '.', '.', ',', '.', ',', '.', ',', '.', '.', '.', '.', '.', ',', ',', ',', ',']\n",
      "['것이다', '목욕', '##들로', '엄', '부진을', '것', '##집', '이에', '하기도', '##양', '##현', '##가', '##지원', '하', '##혜', '김해', '등이', ',', '##가', '##상', '김수', '항상', '##희', ',', '##들도', \"'\", '들어가', '나머지', '지', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.05263156443834305\n",
      "\n",
      "Train Step: 151/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1764, MLM_ACC : 0.5873, MLM_LOSS : 0.5978, NSP_ACC : 0.5878, NSP_LOSS : 0.5892, \n",
      "\n",
      "Train Step: 152/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1827, MLM_ACC : 0.5865, MLM_LOSS : 0.5970, NSP_ACC : 0.5870, NSP_LOSS : 0.5884, \n",
      "\n",
      "Train Step: 153/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1735, MLM_ACC : 0.5872, MLM_LOSS : 0.5977, NSP_ACC : 0.5877, NSP_LOSS : 0.5891, \n",
      "\n",
      "Train Step: 154/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1669, MLM_ACC : 0.5875, MLM_LOSS : 0.5980, NSP_ACC : 0.5880, NSP_LOSS : 0.5894, \n",
      "\n",
      "Train Step: 155/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1585, MLM_ACC : 0.5871, MLM_LOSS : 0.5976, NSP_ACC : 0.5877, NSP_LOSS : 0.5890, \n",
      "\n",
      "Train Step: 156/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1505, MLM_ACC : 0.5888, MLM_LOSS : 0.5993, NSP_ACC : 0.5893, NSP_LOSS : 0.5907, \n",
      "\n",
      "Train Step: 157/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1379, MLM_ACC : 0.5884, MLM_LOSS : 0.5990, NSP_ACC : 0.5890, NSP_LOSS : 0.5903, \n",
      "\n",
      "Train Step: 158/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1274, MLM_ACC : 0.5890, MLM_LOSS : 0.5996, NSP_ACC : 0.5896, NSP_LOSS : 0.5909, \n",
      "\n",
      "Train Step: 159/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1143, MLM_ACC : 0.5909, MLM_LOSS : 0.6014, NSP_ACC : 0.5914, NSP_LOSS : 0.5928, \n",
      "\n",
      "Train Step: 160/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1029, MLM_ACC : 0.5912, MLM_LOSS : 0.6017, NSP_ACC : 0.5917, NSP_LOSS : 0.5931, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', ',', '.', '.', ',', '.', '.', '.', ',', '.', '.', '.', '.', '.', '.', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['스카우트', '필', '활동하다가', '##리그', '대통령', '거론', '##되기', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 161/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0884, MLM_ACC : 0.5930, MLM_LOSS : 0.6035, NSP_ACC : 0.5935, NSP_LOSS : 0.5948, \n",
      "\n",
      "Train Step: 162/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0714, MLM_ACC : 0.5947, MLM_LOSS : 0.6052, NSP_ACC : 0.5952, NSP_LOSS : 0.5965, \n",
      "\n",
      "Train Step: 163/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0847, MLM_ACC : 0.5944, MLM_LOSS : 0.6050, NSP_ACC : 0.5949, NSP_LOSS : 0.5963, \n",
      "\n",
      "Train Step: 164/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.1049, MLM_ACC : 0.5944, MLM_LOSS : 0.6049, NSP_ACC : 0.5949, NSP_LOSS : 0.5963, \n",
      "\n",
      "Train Step: 165/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0912, MLM_ACC : 0.5925, MLM_LOSS : 0.6031, NSP_ACC : 0.5931, NSP_LOSS : 0.5944, \n",
      "\n",
      "Train Step: 166/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0913, MLM_ACC : 0.5896, MLM_LOSS : 0.6002, NSP_ACC : 0.5902, NSP_LOSS : 0.5916, \n",
      "\n",
      "Train Step: 167/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0719, MLM_ACC : 0.5894, MLM_LOSS : 0.5999, NSP_ACC : 0.5899, NSP_LOSS : 0.5913, \n",
      "\n",
      "Train Step: 168/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0584, MLM_ACC : 0.5885, MLM_LOSS : 0.5990, NSP_ACC : 0.5890, NSP_LOSS : 0.5904, \n",
      "\n",
      "Train Step: 169/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0698, MLM_ACC : 0.5883, MLM_LOSS : 0.5988, NSP_ACC : 0.5888, NSP_LOSS : 0.5902, \n",
      "\n",
      "Train Step: 170/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0536, MLM_ACC : 0.5880, MLM_LOSS : 0.5985, NSP_ACC : 0.5886, NSP_LOSS : 0.5899, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', ',', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['##지게', '바뀌었다', '트라', '.', '안쪽', '시대에', '특히', '##는', '.', '지역을', '1개의', '##하기도', '다수의', ',', '##만을', '##키아의', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.05263156443834305\n",
      "\n",
      "Train Step: 171/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0449, MLM_ACC : 0.5875, MLM_LOSS : 0.5980, NSP_ACC : 0.5880, NSP_LOSS : 0.5894, \n",
      "\n",
      "Train Step: 172/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0395, MLM_ACC : 0.5894, MLM_LOSS : 0.6000, NSP_ACC : 0.5900, NSP_LOSS : 0.5914, \n",
      "\n",
      "Train Step: 173/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0260, MLM_ACC : 0.5910, MLM_LOSS : 0.6015, NSP_ACC : 0.5915, NSP_LOSS : 0.5929, \n",
      "\n",
      "Train Step: 174/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0148, MLM_ACC : 0.5913, MLM_LOSS : 0.6019, NSP_ACC : 0.5919, NSP_LOSS : 0.5932, \n",
      "\n",
      "Train Step: 175/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0073, MLM_ACC : 0.5913, MLM_LOSS : 0.6018, NSP_ACC : 0.5918, NSP_LOSS : 0.5932, \n",
      "\n",
      "Train Step: 176/1250  \n",
      " - LOGS - \n",
      "training_loss : 12.0030, MLM_ACC : 0.5932, MLM_LOSS : 0.6037, NSP_ACC : 0.5937, NSP_LOSS : 0.5951, \n",
      "\n",
      "Train Step: 177/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9883, MLM_ACC : 0.5947, MLM_LOSS : 0.6053, NSP_ACC : 0.5953, NSP_LOSS : 0.5966, \n",
      "\n",
      "Train Step: 178/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9728, MLM_ACC : 0.5963, MLM_LOSS : 0.6068, NSP_ACC : 0.5968, NSP_LOSS : 0.5982, \n",
      "\n",
      "Train Step: 179/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9628, MLM_ACC : 0.5961, MLM_LOSS : 0.6067, NSP_ACC : 0.5966, NSP_LOSS : 0.5980, \n",
      "\n",
      "Train Step: 180/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9613, MLM_ACC : 0.5942, MLM_LOSS : 0.6048, NSP_ACC : 0.5947, NSP_LOSS : 0.5961, \n",
      "======MLM TEST======\n",
      "['.', ',', ',', ',', ',', '.', ',', ',', ',', '.', '.', ',', '.', ',', ',', '.', ',', '.', ',', '.', '.', ',', '.', ',', '.', ',', '.', ',', ',', ',', ',', ',', '.', ',', ',', ',', ',', '.', ',', ',']\n",
      "['알파', '13일부터', '2월', '참가했다', '참가하였다', '경기는', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.07894734293222427\n",
      "\n",
      "Train Step: 181/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9511, MLM_ACC : 0.5943, MLM_LOSS : 0.6049, NSP_ACC : 0.5948, NSP_LOSS : 0.5962, \n",
      "\n",
      "Train Step: 182/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9401, MLM_ACC : 0.5944, MLM_LOSS : 0.6049, NSP_ACC : 0.5949, NSP_LOSS : 0.5963, \n",
      "\n",
      "Train Step: 183/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9226, MLM_ACC : 0.5926, MLM_LOSS : 0.6032, NSP_ACC : 0.5931, NSP_LOSS : 0.5945, \n",
      "\n",
      "Train Step: 184/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9091, MLM_ACC : 0.5910, MLM_LOSS : 0.6015, NSP_ACC : 0.5915, NSP_LOSS : 0.5928, \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 185/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9043, MLM_ACC : 0.5909, MLM_LOSS : 0.6015, NSP_ACC : 0.5915, NSP_LOSS : 0.5928, \n",
      "\n",
      "Train Step: 186/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8879, MLM_ACC : 0.5908, MLM_LOSS : 0.6013, NSP_ACC : 0.5913, NSP_LOSS : 0.5927, \n",
      "\n",
      "Train Step: 187/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8931, MLM_ACC : 0.5905, MLM_LOSS : 0.6011, NSP_ACC : 0.5911, NSP_LOSS : 0.5924, \n",
      "\n",
      "Train Step: 188/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.9028, MLM_ACC : 0.5894, MLM_LOSS : 0.6000, NSP_ACC : 0.5900, NSP_LOSS : 0.5913, \n",
      "\n",
      "Train Step: 189/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8926, MLM_ACC : 0.5907, MLM_LOSS : 0.6012, NSP_ACC : 0.5912, NSP_LOSS : 0.5926, \n",
      "\n",
      "Train Step: 190/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8918, MLM_ACC : 0.5901, MLM_LOSS : 0.6007, NSP_ACC : 0.5906, NSP_LOSS : 0.5920, \n",
      "======MLM TEST======\n",
      "[',', '.', ',', ',', ',', '.', '.', '.', ',', ',', ',', '.', '.', '.', ',', '.', '.', '.', ',', ',', ',', ',', ',', '.', '.', ',', ',', ',', ',', ',', ',', '.', '.', ',', ',', '.', '.', ',', '.', '.']\n",
      "['오바마', '.', ',', '첫', '##lo', '##도', '##대회에서', '##의', '높은', '##하기', '##패', '오바마', '롬', '알려', '##할', '지명되었다', '##니는', '일본에서', '투표', '정치', '##다', '##고', '열린', '접', '건', '아니고', '##지기도', '이는', '낮은', '##로라', '동부', '거기에', '자전거', '##패', '##을', '.', '한', '후보', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.026315782219171524\n",
      "\n",
      "Train Step: 191/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8860, MLM_ACC : 0.5896, MLM_LOSS : 0.6001, NSP_ACC : 0.5901, NSP_LOSS : 0.5915, \n",
      "\n",
      "Train Step: 192/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8771, MLM_ACC : 0.5889, MLM_LOSS : 0.5994, NSP_ACC : 0.5894, NSP_LOSS : 0.5908, \n",
      "\n",
      "Train Step: 193/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8627, MLM_ACC : 0.5898, MLM_LOSS : 0.6003, NSP_ACC : 0.5903, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 194/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8666, MLM_ACC : 0.5905, MLM_LOSS : 0.6011, NSP_ACC : 0.5910, NSP_LOSS : 0.5924, \n",
      "\n",
      "Train Step: 195/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8705, MLM_ACC : 0.5915, MLM_LOSS : 0.6020, NSP_ACC : 0.5920, NSP_LOSS : 0.5934, \n",
      "\n",
      "Train Step: 196/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8725, MLM_ACC : 0.5912, MLM_LOSS : 0.6017, NSP_ACC : 0.5916, NSP_LOSS : 0.5931, \n",
      "\n",
      "Train Step: 197/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8626, MLM_ACC : 0.5908, MLM_LOSS : 0.6013, NSP_ACC : 0.5913, NSP_LOSS : 0.5927, \n",
      "\n",
      "Train Step: 198/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8557, MLM_ACC : 0.5900, MLM_LOSS : 0.6006, NSP_ACC : 0.5905, NSP_LOSS : 0.5919, \n",
      "\n",
      "Train Step: 199/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8494, MLM_ACC : 0.5885, MLM_LOSS : 0.5991, NSP_ACC : 0.5890, NSP_LOSS : 0.5905, \n",
      "\n",
      "Train Step: 200/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8362, MLM_ACC : 0.5881, MLM_LOSS : 0.5986, NSP_ACC : 0.5886, NSP_LOSS : 0.5900, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['##양', '##었', '훗날', '공항이', '##진의', '위치', '향을', '.', '(', '중요한', '##하를', '것이다', '이곳에서', '잘못', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 201/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8262, MLM_ACC : 0.5878, MLM_LOSS : 0.5984, NSP_ACC : 0.5883, NSP_LOSS : 0.5897, \n",
      "\n",
      "Train Step: 202/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8168, MLM_ACC : 0.5893, MLM_LOSS : 0.5999, NSP_ACC : 0.5898, NSP_LOSS : 0.5912, \n",
      "\n",
      "Train Step: 203/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.8053, MLM_ACC : 0.5908, MLM_LOSS : 0.6013, NSP_ACC : 0.5913, NSP_LOSS : 0.5927, \n",
      "\n",
      "Train Step: 204/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7972, MLM_ACC : 0.5921, MLM_LOSS : 0.6026, NSP_ACC : 0.5926, NSP_LOSS : 0.5940, \n",
      "\n",
      "Train Step: 205/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7908, MLM_ACC : 0.5909, MLM_LOSS : 0.6015, NSP_ACC : 0.5914, NSP_LOSS : 0.5928, \n",
      "\n",
      "Train Step: 206/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7795, MLM_ACC : 0.5915, MLM_LOSS : 0.6020, NSP_ACC : 0.5920, NSP_LOSS : 0.5934, \n",
      "\n",
      "Train Step: 207/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7749, MLM_ACC : 0.5911, MLM_LOSS : 0.6017, NSP_ACC : 0.5916, NSP_LOSS : 0.5930, \n",
      "\n",
      "Train Step: 208/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7577, MLM_ACC : 0.5904, MLM_LOSS : 0.6009, NSP_ACC : 0.5909, NSP_LOSS : 0.5922, \n",
      "\n",
      "Train Step: 209/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7437, MLM_ACC : 0.5898, MLM_LOSS : 0.6004, NSP_ACC : 0.5903, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 210/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7335, MLM_ACC : 0.5900, MLM_LOSS : 0.6006, NSP_ACC : 0.5905, NSP_LOSS : 0.5919, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', ',', ',', ',', ',', '.', '.', '.', ',', ',', '.', ',', ',', ',', '.', ',', ',', ',', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['##었고', '##군이', '##트는', '##을', '##과', '오만', '타이거즈', '총사령관', '##이', '##놀', '앙', '##언론', '셔', '되었다', '##공', '멸', '지원', '##것', '미국', '인도', '##갚', '##쟁', '물러나', '##다', '##군을', '##이', '자주', '권', '상대로', '소환', '양산', '##기자', '##웰', '##좌에', '##정부를', '군벌', '##했었다', '장제', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 211/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7164, MLM_ACC : 0.5899, MLM_LOSS : 0.6005, NSP_ACC : 0.5904, NSP_LOSS : 0.5918, \n",
      "\n",
      "Train Step: 212/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7104, MLM_ACC : 0.5901, MLM_LOSS : 0.6007, NSP_ACC : 0.5907, NSP_LOSS : 0.5920, \n",
      "\n",
      "Train Step: 213/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7054, MLM_ACC : 0.5908, MLM_LOSS : 0.6014, NSP_ACC : 0.5913, NSP_LOSS : 0.5927, \n",
      "\n",
      "Train Step: 214/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.7000, MLM_ACC : 0.5909, MLM_LOSS : 0.6014, NSP_ACC : 0.5914, NSP_LOSS : 0.5927, \n",
      "\n",
      "Train Step: 215/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6874, MLM_ACC : 0.5897, MLM_LOSS : 0.6003, NSP_ACC : 0.5902, NSP_LOSS : 0.5916, \n",
      "\n",
      "Train Step: 216/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6745, MLM_ACC : 0.5908, MLM_LOSS : 0.6014, NSP_ACC : 0.5913, NSP_LOSS : 0.5926, \n",
      "\n",
      "Train Step: 217/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6660, MLM_ACC : 0.5898, MLM_LOSS : 0.6004, NSP_ACC : 0.5903, NSP_LOSS : 0.5916, \n",
      "\n",
      "Train Step: 218/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6562, MLM_ACC : 0.5899, MLM_LOSS : 0.6005, NSP_ACC : 0.5904, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 219/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6474, MLM_ACC : 0.5891, MLM_LOSS : 0.5997, NSP_ACC : 0.5896, NSP_LOSS : 0.5909, \n",
      "\n",
      "Train Step: 220/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6382, MLM_ACC : 0.5898, MLM_LOSS : 0.6004, NSP_ACC : 0.5903, NSP_LOSS : 0.5916, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['같이', '행성', '##과', '태양계', '모든', '손자', '면적', '케플러', '제3', '##하다고', '##속도', '##의', '##위성', '위성', '상수', '그', '만', '##이라고도', '있다', '위성', '##위성', '케플러', '.', '법칙', '넓', '##에', '중심으로', '운동', '비해', '제3', '##의', '있다', '##하는', '법칙', '제2', '(', '##을', '##선', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.16666638851165771\n",
      "\n",
      "Train Step: 221/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6296, MLM_ACC : 0.5912, MLM_LOSS : 0.6018, NSP_ACC : 0.5917, NSP_LOSS : 0.5930, \n",
      "\n",
      "Train Step: 222/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6171, MLM_ACC : 0.5916, MLM_LOSS : 0.6022, NSP_ACC : 0.5921, NSP_LOSS : 0.5934, \n",
      "\n",
      "Train Step: 223/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.6056, MLM_ACC : 0.5915, MLM_LOSS : 0.6021, NSP_ACC : 0.5920, NSP_LOSS : 0.5933, \n",
      "\n",
      "Train Step: 224/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5990, MLM_ACC : 0.5908, MLM_LOSS : 0.6014, NSP_ACC : 0.5913, NSP_LOSS : 0.5927, \n",
      "\n",
      "Train Step: 225/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5909, MLM_ACC : 0.5908, MLM_LOSS : 0.6014, NSP_ACC : 0.5913, NSP_LOSS : 0.5926, \n",
      "\n",
      "Train Step: 226/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5797, MLM_ACC : 0.5907, MLM_LOSS : 0.6013, NSP_ACC : 0.5912, NSP_LOSS : 0.5925, \n",
      "\n",
      "Train Step: 227/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5711, MLM_ACC : 0.5900, MLM_LOSS : 0.6006, NSP_ACC : 0.5905, NSP_LOSS : 0.5919, \n",
      "\n",
      "Train Step: 228/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5672, MLM_ACC : 0.5899, MLM_LOSS : 0.6005, NSP_ACC : 0.5904, NSP_LOSS : 0.5918, \n",
      "\n",
      "Train Step: 229/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5595, MLM_ACC : 0.5904, MLM_LOSS : 0.6010, NSP_ACC : 0.5909, NSP_LOSS : 0.5923, \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 230/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5449, MLM_ACC : 0.5892, MLM_LOSS : 0.5998, NSP_ACC : 0.5897, NSP_LOSS : 0.5910, \n",
      "======MLM TEST======\n",
      "[',', ',', '.', '.', ',', ',', ',', ',', ',', '.', ',', ',', '.', '.', ',', ',', ',', ',', ',', ',', ',', ',', '.', ',', '.', '.', ',', ',', '.', ',', '.', '.', ',', ',', '.', ',', ',', '.', '.', '.']\n",
      "['##있는', '들어', '라는', '보면', '유명해졌다', '##원장', '##고', '퍼져', '##만에', '##에', '알려지지', '2', '.', '##이', '9월', ')', '##로', '##들이', '##스', '이틀', '(', '개념이', '##와', '않을', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.026315782219171524\n",
      "\n",
      "Train Step: 231/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5387, MLM_ACC : 0.5876, MLM_LOSS : 0.5982, NSP_ACC : 0.5881, NSP_LOSS : 0.5894, \n",
      "\n",
      "Train Step: 232/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5289, MLM_ACC : 0.5889, MLM_LOSS : 0.5995, NSP_ACC : 0.5894, NSP_LOSS : 0.5907, \n",
      "\n",
      "Train Step: 233/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5187, MLM_ACC : 0.5888, MLM_LOSS : 0.5994, NSP_ACC : 0.5893, NSP_LOSS : 0.5906, \n",
      "\n",
      "Train Step: 234/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5115, MLM_ACC : 0.5901, MLM_LOSS : 0.6007, NSP_ACC : 0.5906, NSP_LOSS : 0.5919, \n",
      "\n",
      "Train Step: 235/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.5050, MLM_ACC : 0.5894, MLM_LOSS : 0.6000, NSP_ACC : 0.5899, NSP_LOSS : 0.5912, \n",
      "\n",
      "Train Step: 236/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4939, MLM_ACC : 0.5906, MLM_LOSS : 0.6012, NSP_ACC : 0.5911, NSP_LOSS : 0.5924, \n",
      "\n",
      "Train Step: 237/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4862, MLM_ACC : 0.5915, MLM_LOSS : 0.6021, NSP_ACC : 0.5920, NSP_LOSS : 0.5933, \n",
      "\n",
      "Train Step: 238/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4755, MLM_ACC : 0.5901, MLM_LOSS : 0.6008, NSP_ACC : 0.5906, NSP_LOSS : 0.5919, \n",
      "\n",
      "Train Step: 239/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4675, MLM_ACC : 0.5914, MLM_LOSS : 0.6020, NSP_ACC : 0.5919, NSP_LOSS : 0.5932, \n",
      "\n",
      "Train Step: 240/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4579, MLM_ACC : 0.5904, MLM_LOSS : 0.6010, NSP_ACC : 0.5909, NSP_LOSS : 0.5922, \n",
      "======MLM TEST======\n",
      "[',', ',', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['충족', '독일과', '인한', '분명히', '그를', '우크라이나', '품질', '##당했다', '사이에', '경영', '아버지가', '수준의', '##도', '경우에', '있거나', '서비스의', '##을', '##중', '##하지', '##이란', ',', '부모의', '창', '##과', '오늘날의', '어떤', '때', '그의', '소비에트', '인식', '53', '.', '거의', '##을', '##뒀', '서비스의', ',', '것을', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 241/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4505, MLM_ACC : 0.5894, MLM_LOSS : 0.6001, NSP_ACC : 0.5899, NSP_LOSS : 0.5912, \n",
      "\n",
      "Train Step: 242/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4435, MLM_ACC : 0.5890, MLM_LOSS : 0.5997, NSP_ACC : 0.5895, NSP_LOSS : 0.5908, \n",
      "\n",
      "Train Step: 243/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4342, MLM_ACC : 0.5879, MLM_LOSS : 0.5985, NSP_ACC : 0.5884, NSP_LOSS : 0.5897, \n",
      "\n",
      "Train Step: 244/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4221, MLM_ACC : 0.5878, MLM_LOSS : 0.5984, NSP_ACC : 0.5883, NSP_LOSS : 0.5895, \n",
      "\n",
      "Train Step: 245/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.4113, MLM_ACC : 0.5880, MLM_LOSS : 0.5987, NSP_ACC : 0.5885, NSP_LOSS : 0.5898, \n",
      "\n",
      "Train Step: 246/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3994, MLM_ACC : 0.5888, MLM_LOSS : 0.5995, NSP_ACC : 0.5893, NSP_LOSS : 0.5906, \n",
      "\n",
      "Train Step: 247/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3887, MLM_ACC : 0.5886, MLM_LOSS : 0.5993, NSP_ACC : 0.5891, NSP_LOSS : 0.5904, \n",
      "\n",
      "Train Step: 248/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3784, MLM_ACC : 0.5886, MLM_LOSS : 0.5992, NSP_ACC : 0.5891, NSP_LOSS : 0.5904, \n",
      "\n",
      "Train Step: 249/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3665, MLM_ACC : 0.5875, MLM_LOSS : 0.5982, NSP_ACC : 0.5880, NSP_LOSS : 0.5893, \n",
      "\n",
      "Train Step: 250/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3610, MLM_ACC : 0.5878, MLM_LOSS : 0.5985, NSP_ACC : 0.5883, NSP_LOSS : 0.5896, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['폐', '##되면서', '2에', '못하고', '타이', '단편', '알', '작품을', '.', '리그', '기준으로', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 251/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3511, MLM_ACC : 0.5885, MLM_LOSS : 0.5992, NSP_ACC : 0.5890, NSP_LOSS : 0.5903, \n",
      "\n",
      "Train Step: 252/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3444, MLM_ACC : 0.5890, MLM_LOSS : 0.5997, NSP_ACC : 0.5895, NSP_LOSS : 0.5908, \n",
      "\n",
      "Train Step: 253/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3299, MLM_ACC : 0.5900, MLM_LOSS : 0.6006, NSP_ACC : 0.5905, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 254/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3351, MLM_ACC : 0.5882, MLM_LOSS : 0.5989, NSP_ACC : 0.5887, NSP_LOSS : 0.5900, \n",
      "\n",
      "Train Step: 255/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3267, MLM_ACC : 0.5885, MLM_LOSS : 0.5992, NSP_ACC : 0.5891, NSP_LOSS : 0.5903, \n",
      "\n",
      "Train Step: 256/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3146, MLM_ACC : 0.5876, MLM_LOSS : 0.5983, NSP_ACC : 0.5881, NSP_LOSS : 0.5893, \n",
      "\n",
      "Train Step: 257/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.3013, MLM_ACC : 0.5886, MLM_LOSS : 0.5992, NSP_ACC : 0.5891, NSP_LOSS : 0.5903, \n",
      "\n",
      "Train Step: 258/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2962, MLM_ACC : 0.5884, MLM_LOSS : 0.5991, NSP_ACC : 0.5889, NSP_LOSS : 0.5902, \n",
      "\n",
      "Train Step: 259/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2888, MLM_ACC : 0.5892, MLM_LOSS : 0.5999, NSP_ACC : 0.5897, NSP_LOSS : 0.5910, \n",
      "\n",
      "Train Step: 260/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2822, MLM_ACC : 0.5885, MLM_LOSS : 0.5992, NSP_ACC : 0.5890, NSP_LOSS : 0.5902, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['북부', '대한민국', '해상', '햄', '해상', '섬에', '##륙', '7일', '기준', '##하여', '이상', '제18', 'c', '재상', '.', '제출한', '##로를', '##하면서', '레오', '10월', '약화', '필리핀', '##로를', '##부로', '##을', '##해', '##되어', '나올', '.', ',', '##에', '남동', '일본', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.19999980926513672\n",
      "\n",
      "Train Step: 261/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2799, MLM_ACC : 0.5895, MLM_LOSS : 0.6001, NSP_ACC : 0.5900, NSP_LOSS : 0.5912, \n",
      "\n",
      "Train Step: 262/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2750, MLM_ACC : 0.5898, MLM_LOSS : 0.6005, NSP_ACC : 0.5903, NSP_LOSS : 0.5916, \n",
      "\n",
      "Train Step: 263/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2689, MLM_ACC : 0.5900, MLM_LOSS : 0.6006, NSP_ACC : 0.5905, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 264/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2643, MLM_ACC : 0.5904, MLM_LOSS : 0.6010, NSP_ACC : 0.5909, NSP_LOSS : 0.5921, \n",
      "\n",
      "Train Step: 265/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2557, MLM_ACC : 0.5902, MLM_LOSS : 0.6009, NSP_ACC : 0.5908, NSP_LOSS : 0.5920, \n",
      "\n",
      "Train Step: 266/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2496, MLM_ACC : 0.5902, MLM_LOSS : 0.6009, NSP_ACC : 0.5907, NSP_LOSS : 0.5920, \n",
      "\n",
      "Train Step: 267/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2498, MLM_ACC : 0.5903, MLM_LOSS : 0.6010, NSP_ACC : 0.5908, NSP_LOSS : 0.5921, \n",
      "\n",
      "Train Step: 268/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2443, MLM_ACC : 0.5906, MLM_LOSS : 0.6012, NSP_ACC : 0.5911, NSP_LOSS : 0.5924, \n",
      "\n",
      "Train Step: 269/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2346, MLM_ACC : 0.5900, MLM_LOSS : 0.6006, NSP_ACC : 0.5905, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 270/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2302, MLM_ACC : 0.5901, MLM_LOSS : 0.6008, NSP_ACC : 0.5906, NSP_LOSS : 0.5919, \n",
      "======MLM TEST======\n",
      "[',', '.', '.', '.', '.', '.', ',', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['소속되어', '세르비아', '##j', '\"', '##에', '##27', '명명되었다', '국방부', '\"', '해상', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 271/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2238, MLM_ACC : 0.5905, MLM_LOSS : 0.6012, NSP_ACC : 0.5910, NSP_LOSS : 0.5923, \n",
      "\n",
      "Train Step: 272/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2194, MLM_ACC : 0.5904, MLM_LOSS : 0.6010, NSP_ACC : 0.5909, NSP_LOSS : 0.5921, \n",
      "\n",
      "Train Step: 273/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2107, MLM_ACC : 0.5894, MLM_LOSS : 0.6001, NSP_ACC : 0.5899, NSP_LOSS : 0.5912, \n",
      "\n",
      "Train Step: 274/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2022, MLM_ACC : 0.5905, MLM_LOSS : 0.6011, NSP_ACC : 0.5910, NSP_LOSS : 0.5922, \n",
      "\n",
      "Train Step: 275/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2008, MLM_ACC : 0.5901, MLM_LOSS : 0.6008, NSP_ACC : 0.5907, NSP_LOSS : 0.5919, \n",
      "\n",
      "Train Step: 276/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2065, MLM_ACC : 0.5906, MLM_LOSS : 0.6013, NSP_ACC : 0.5912, NSP_LOSS : 0.5924, \n",
      "\n",
      "Train Step: 277/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2141, MLM_ACC : 0.5904, MLM_LOSS : 0.6011, NSP_ACC : 0.5909, NSP_LOSS : 0.5922, \n",
      "\n",
      "Train Step: 278/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2137, MLM_ACC : 0.5901, MLM_LOSS : 0.6007, NSP_ACC : 0.5906, NSP_LOSS : 0.5919, \n",
      "\n",
      "Train Step: 279/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2113, MLM_ACC : 0.5900, MLM_LOSS : 0.6006, NSP_ACC : 0.5905, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 280/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2113, MLM_ACC : 0.5898, MLM_LOSS : 0.6004, NSP_ACC : 0.5903, NSP_LOSS : 0.5915, \n",
      "======MLM TEST======\n",
      "[',', '.', ',', ',', '.', ',', ',', ',', ',', ',', ',', '.', ',', ',', ',', '.', ',', '.', ',', ',', ',', ',', ',', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', '.', ',', ',', ',', ',', ',', ',']\n",
      "['위', '##의', '##을', '거둔', '##과', '마지막', '경험이', '출발', '.', '처음으로', '전혀', '##부터', '있도록', '측면', 'cal', '세', '했다', ',', '##했었다', ')', '##자', '부분에서', '양자', '##스', '용인', '도덕적', '보', '##기', '어떤', '##on', '##활', '##보다는', '권력의', '##는', '##에', '##도', '요구하고', '모두를', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.026315782219171524\n",
      "\n",
      "Train Step: 281/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2107, MLM_ACC : 0.5906, MLM_LOSS : 0.6012, NSP_ACC : 0.5911, NSP_LOSS : 0.5924, \n",
      "\n",
      "Train Step: 282/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2035, MLM_ACC : 0.5905, MLM_LOSS : 0.6012, NSP_ACC : 0.5910, NSP_LOSS : 0.5923, \n",
      "\n",
      "Train Step: 283/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.2017, MLM_ACC : 0.5903, MLM_LOSS : 0.6009, NSP_ACC : 0.5908, NSP_LOSS : 0.5921, \n",
      "\n",
      "Train Step: 284/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1908, MLM_ACC : 0.5895, MLM_LOSS : 0.6001, NSP_ACC : 0.5900, NSP_LOSS : 0.5912, \n",
      "\n",
      "Train Step: 285/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1836, MLM_ACC : 0.5899, MLM_LOSS : 0.6006, NSP_ACC : 0.5904, NSP_LOSS : 0.5917, \n",
      "\n",
      "Train Step: 286/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1897, MLM_ACC : 0.5903, MLM_LOSS : 0.6009, NSP_ACC : 0.5908, NSP_LOSS : 0.5921, \n",
      "\n",
      "Train Step: 287/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1782, MLM_ACC : 0.5899, MLM_LOSS : 0.6005, NSP_ACC : 0.5904, NSP_LOSS : 0.5916, \n",
      "\n",
      "Train Step: 288/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1864, MLM_ACC : 0.5914, MLM_LOSS : 0.6020, NSP_ACC : 0.5919, NSP_LOSS : 0.5932, \n",
      "\n",
      "Train Step: 289/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1780, MLM_ACC : 0.5911, MLM_LOSS : 0.6017, NSP_ACC : 0.5916, NSP_LOSS : 0.5929, \n",
      "\n",
      "Train Step: 290/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1772, MLM_ACC : 0.5915, MLM_LOSS : 0.6021, NSP_ACC : 0.5920, NSP_LOSS : 0.5933, \n",
      "======MLM TEST======\n",
      "['.', ',', '.', '.', '.', '.', '.', '.', '.', '.', ',', '.', ',', '.', '.', ',', ',', '.', '.', '.', ',', '.', ',', '.', '.', '.', ',', ',', ',', ',', ',', '.', ',', ',', ',', '.', '.', ',', ',', ',']\n",
      "['하계', '1개', '페루', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.026315782219171524\n",
      "\n",
      "Train Step: 291/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1782, MLM_ACC : 0.5914, MLM_LOSS : 0.6021, NSP_ACC : 0.5919, NSP_LOSS : 0.5932, \n",
      "\n",
      "Train Step: 292/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1801, MLM_ACC : 0.5919, MLM_LOSS : 0.6025, NSP_ACC : 0.5924, NSP_LOSS : 0.5937, \n",
      "\n",
      "Train Step: 293/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1682, MLM_ACC : 0.5923, MLM_LOSS : 0.6029, NSP_ACC : 0.5928, NSP_LOSS : 0.5941, \n",
      "\n",
      "Train Step: 294/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1677, MLM_ACC : 0.5930, MLM_LOSS : 0.6036, NSP_ACC : 0.5935, NSP_LOSS : 0.5948, \n",
      "\n",
      "Train Step: 295/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1669, MLM_ACC : 0.5942, MLM_LOSS : 0.6048, NSP_ACC : 0.5947, NSP_LOSS : 0.5960, \n",
      "\n",
      "Train Step: 296/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1609, MLM_ACC : 0.5942, MLM_LOSS : 0.6049, NSP_ACC : 0.5947, NSP_LOSS : 0.5961, \n",
      "\n",
      "Train Step: 297/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1500, MLM_ACC : 0.5944, MLM_LOSS : 0.6051, NSP_ACC : 0.5949, NSP_LOSS : 0.5962, \n",
      "\n",
      "Train Step: 298/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1416, MLM_ACC : 0.5954, MLM_LOSS : 0.6060, NSP_ACC : 0.5959, NSP_LOSS : 0.5972, \n",
      "\n",
      "Train Step: 299/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1387, MLM_ACC : 0.5965, MLM_LOSS : 0.6071, NSP_ACC : 0.5969, NSP_LOSS : 0.5983, \n",
      "\n",
      "Train Step: 300/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1304, MLM_ACC : 0.5956, MLM_LOSS : 0.6062, NSP_ACC : 0.5961, NSP_LOSS : 0.5974, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['전국', '목에', '##이', '상위', '##콩', '분포하며', '있고', '##에게', '##를', '허', '코리아', '##은', ',', '같은', '1', '널리', '##제비', '42', '##노', '분류', '세계', '거두어', '##대칭', ',', '했고', '##회', '##랑', ',', '##퀴', '월드', '##다리', '##속의', '한편', '비', '##스', '##전국', '양성', '25', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.05405403673648834\n",
      "\n",
      "Train Step: 301/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1206, MLM_ACC : 0.5958, MLM_LOSS : 0.6065, NSP_ACC : 0.5963, NSP_LOSS : 0.5976, \n",
      "\n",
      "Train Step: 302/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1168, MLM_ACC : 0.5958, MLM_LOSS : 0.6064, NSP_ACC : 0.5963, NSP_LOSS : 0.5976, \n",
      "\n",
      "Train Step: 303/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1086, MLM_ACC : 0.5949, MLM_LOSS : 0.6055, NSP_ACC : 0.5954, NSP_LOSS : 0.5967, \n",
      "\n",
      "Train Step: 304/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.1036, MLM_ACC : 0.5937, MLM_LOSS : 0.6043, NSP_ACC : 0.5942, NSP_LOSS : 0.5955, \n",
      "\n",
      "Train Step: 305/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0962, MLM_ACC : 0.5935, MLM_LOSS : 0.6041, NSP_ACC : 0.5940, NSP_LOSS : 0.5953, \n",
      "\n",
      "Train Step: 306/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0940, MLM_ACC : 0.5946, MLM_LOSS : 0.6052, NSP_ACC : 0.5951, NSP_LOSS : 0.5964, \n",
      "\n",
      "Train Step: 307/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0930, MLM_ACC : 0.5934, MLM_LOSS : 0.6040, NSP_ACC : 0.5939, NSP_LOSS : 0.5952, \n",
      "\n",
      "Train Step: 308/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0859, MLM_ACC : 0.5938, MLM_LOSS : 0.6045, NSP_ACC : 0.5943, NSP_LOSS : 0.5956, \n",
      "\n",
      "Train Step: 309/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0839, MLM_ACC : 0.5944, MLM_LOSS : 0.6050, NSP_ACC : 0.5949, NSP_LOSS : 0.5962, \n",
      "\n",
      "Train Step: 310/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0796, MLM_ACC : 0.5946, MLM_LOSS : 0.6052, NSP_ACC : 0.5951, NSP_LOSS : 0.5964, \n",
      "======MLM TEST======\n",
      "[',', '.', ',', '.', ',', ',', ',', '.', '.', ',', ',', ',', ',', ',', ',', ',', '.', '.', ',', '.', '.', ',', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',']\n",
      "['모두', '아이', '##를', ',', '(', '##게', '##의', '사건이', '가동', '여섯', '##자와', '사관', '이소', '##의', '게임에서', '아시안', '땄다', '수영', '고소', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.13157890737056732\n",
      "\n",
      "Train Step: 311/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0698, MLM_ACC : 0.5936, MLM_LOSS : 0.6042, NSP_ACC : 0.5941, NSP_LOSS : 0.5954, \n",
      "\n",
      "Train Step: 312/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0600, MLM_ACC : 0.5934, MLM_LOSS : 0.6040, NSP_ACC : 0.5939, NSP_LOSS : 0.5952, \n",
      "\n",
      "Train Step: 313/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0582, MLM_ACC : 0.5923, MLM_LOSS : 0.6029, NSP_ACC : 0.5927, NSP_LOSS : 0.5940, \n",
      "\n",
      "Train Step: 314/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0577, MLM_ACC : 0.5926, MLM_LOSS : 0.6033, NSP_ACC : 0.5931, NSP_LOSS : 0.5944, \n",
      "\n",
      "Train Step: 315/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0577, MLM_ACC : 0.5927, MLM_LOSS : 0.6033, NSP_ACC : 0.5932, NSP_LOSS : 0.5945, \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 316/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0497, MLM_ACC : 0.5926, MLM_LOSS : 0.6032, NSP_ACC : 0.5931, NSP_LOSS : 0.5944, \n",
      "\n",
      "Train Step: 317/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0409, MLM_ACC : 0.5930, MLM_LOSS : 0.6036, NSP_ACC : 0.5935, NSP_LOSS : 0.5948, \n",
      "\n",
      "Train Step: 318/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0333, MLM_ACC : 0.5920, MLM_LOSS : 0.6027, NSP_ACC : 0.5925, NSP_LOSS : 0.5938, \n",
      "\n",
      "Train Step: 319/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0308, MLM_ACC : 0.5925, MLM_LOSS : 0.6031, NSP_ACC : 0.5930, NSP_LOSS : 0.5943, \n",
      "\n",
      "Train Step: 320/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0266, MLM_ACC : 0.5925, MLM_LOSS : 0.6031, NSP_ACC : 0.5930, NSP_LOSS : 0.5943, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', ',', ',', '.', ',', '.', ',', ',', ',', ',', ',', ',', '.', ',', ',', ',', '.', '.', ',', '.', ',', ',', ',', '.', ',', ',', ',', ',', '.', '.', '.', ',', ',', ',', ',', ',', ',']\n",
      "['\"', '##호인', '\"', '전통적인', '진', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.026315782219171524\n",
      "\n",
      "Train Step: 321/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0237, MLM_ACC : 0.5935, MLM_LOSS : 0.6041, NSP_ACC : 0.5940, NSP_LOSS : 0.5953, \n",
      "\n",
      "Train Step: 322/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0158, MLM_ACC : 0.5937, MLM_LOSS : 0.6044, NSP_ACC : 0.5942, NSP_LOSS : 0.5955, \n",
      "\n",
      "Train Step: 323/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0102, MLM_ACC : 0.5932, MLM_LOSS : 0.6038, NSP_ACC : 0.5937, NSP_LOSS : 0.5950, \n",
      "\n",
      "Train Step: 324/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0090, MLM_ACC : 0.5933, MLM_LOSS : 0.6040, NSP_ACC : 0.5938, NSP_LOSS : 0.5951, \n",
      "\n",
      "Train Step: 325/1250  \n",
      " - LOGS - \n",
      "training_loss : 11.0030, MLM_ACC : 0.5942, MLM_LOSS : 0.6049, NSP_ACC : 0.5947, NSP_LOSS : 0.5960, \n",
      "\n",
      "Train Step: 326/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9982, MLM_ACC : 0.5940, MLM_LOSS : 0.6046, NSP_ACC : 0.5945, NSP_LOSS : 0.5958, \n",
      "\n",
      "Train Step: 327/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9917, MLM_ACC : 0.5939, MLM_LOSS : 0.6045, NSP_ACC : 0.5944, NSP_LOSS : 0.5957, \n",
      "\n",
      "Train Step: 328/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9845, MLM_ACC : 0.5938, MLM_LOSS : 0.6044, NSP_ACC : 0.5943, NSP_LOSS : 0.5956, \n",
      "\n",
      "Train Step: 329/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9790, MLM_ACC : 0.5930, MLM_LOSS : 0.6037, NSP_ACC : 0.5935, NSP_LOSS : 0.5948, \n",
      "\n",
      "Train Step: 330/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9756, MLM_ACC : 0.5936, MLM_LOSS : 0.6042, NSP_ACC : 0.5940, NSP_LOSS : 0.5954, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['##해', '##에는', '정', '##와', '맡', '스', '7월', '되었다고', '과장', '##으로', '##참모', '8월', ',', '##쿠라', '제20', '육군', '아사', '무라', '주', '##했고', '한', '나가', '마사', '카타', '##드라', '##사를', '교도', '방위', ',', '##기', '##하지만', ',', '주', '.', '명은', '기도', '##도', '##등이', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.05263156443834305\n",
      "\n",
      "Train Step: 331/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9698, MLM_ACC : 0.5930, MLM_LOSS : 0.6037, NSP_ACC : 0.5935, NSP_LOSS : 0.5948, \n",
      "\n",
      "Train Step: 332/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9630, MLM_ACC : 0.5939, MLM_LOSS : 0.6045, NSP_ACC : 0.5944, NSP_LOSS : 0.5956, \n",
      "\n",
      "Train Step: 333/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9554, MLM_ACC : 0.5933, MLM_LOSS : 0.6039, NSP_ACC : 0.5938, NSP_LOSS : 0.5951, \n",
      "\n",
      "Train Step: 334/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9487, MLM_ACC : 0.5934, MLM_LOSS : 0.6040, NSP_ACC : 0.5939, NSP_LOSS : 0.5952, \n",
      "\n",
      "Train Step: 335/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9475, MLM_ACC : 0.5925, MLM_LOSS : 0.6032, NSP_ACC : 0.5930, NSP_LOSS : 0.5943, \n",
      "\n",
      "Train Step: 336/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9445, MLM_ACC : 0.5915, MLM_LOSS : 0.6021, NSP_ACC : 0.5920, NSP_LOSS : 0.5932, \n",
      "\n",
      "Train Step: 337/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9441, MLM_ACC : 0.5922, MLM_LOSS : 0.6029, NSP_ACC : 0.5927, NSP_LOSS : 0.5940, \n",
      "\n",
      "Train Step: 338/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9413, MLM_ACC : 0.5926, MLM_LOSS : 0.6032, NSP_ACC : 0.5931, NSP_LOSS : 0.5944, \n",
      "\n",
      "Train Step: 339/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9461, MLM_ACC : 0.5938, MLM_LOSS : 0.6044, NSP_ACC : 0.5943, NSP_LOSS : 0.5956, \n",
      "\n",
      "Train Step: 340/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9409, MLM_ACC : 0.5938, MLM_LOSS : 0.6045, NSP_ACC : 0.5943, NSP_LOSS : 0.5956, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['남', '연극', '되는', '위에', '개인', '행동을', '##는', '##으로', '그것은', '때문에', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.0\n",
      "\n",
      "Train Step: 341/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9334, MLM_ACC : 0.5937, MLM_LOSS : 0.6043, NSP_ACC : 0.5941, NSP_LOSS : 0.5954, \n",
      "\n",
      "Train Step: 342/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9264, MLM_ACC : 0.5935, MLM_LOSS : 0.6042, NSP_ACC : 0.5940, NSP_LOSS : 0.5953, \n",
      "\n",
      "Train Step: 343/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9235, MLM_ACC : 0.5929, MLM_LOSS : 0.6035, NSP_ACC : 0.5934, NSP_LOSS : 0.5947, \n",
      "\n",
      "Train Step: 344/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9228, MLM_ACC : 0.5934, MLM_LOSS : 0.6040, NSP_ACC : 0.5939, NSP_LOSS : 0.5952, \n",
      "\n",
      "Train Step: 345/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9150, MLM_ACC : 0.5938, MLM_LOSS : 0.6045, NSP_ACC : 0.5943, NSP_LOSS : 0.5956, \n",
      "\n",
      "Train Step: 346/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9092, MLM_ACC : 0.5939, MLM_LOSS : 0.6046, NSP_ACC : 0.5944, NSP_LOSS : 0.5957, \n",
      "\n",
      "Train Step: 347/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.9027, MLM_ACC : 0.5947, MLM_LOSS : 0.6054, NSP_ACC : 0.5952, NSP_LOSS : 0.5965, \n",
      "\n",
      "Train Step: 348/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.8998, MLM_ACC : 0.5949, MLM_LOSS : 0.6056, NSP_ACC : 0.5954, NSP_LOSS : 0.5967, \n",
      "\n",
      "Train Step: 349/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.8960, MLM_ACC : 0.5950, MLM_LOSS : 0.6056, NSP_ACC : 0.5955, NSP_LOSS : 0.5967, \n",
      "\n",
      "Train Step: 350/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.8977, MLM_ACC : 0.5948, MLM_LOSS : 0.6054, NSP_ACC : 0.5953, NSP_LOSS : 0.5966, \n",
      "======MLM TEST======\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', ',', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['모두', '아이', '##를', ',', '(', '##게', '##의', '사건이', '가동', '여섯', '##자와', '사관', '이소', '##의', '게임에서', '아시안', '땄다', '수영', '고소', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.03846152499318123\n",
      "\n",
      "Train Step: 351/1250  \n",
      " - LOGS - \n",
      "training_loss : 10.8980, MLM_ACC : 0.5951, MLM_LOSS : 0.6057, NSP_ACC : 0.5956, NSP_LOSS : 0.5969, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = {}\n",
    "\n",
    "for cb in callbacks :\n",
    "    cb[0].on_train_begin()\n",
    "\n",
    "while current_step < total_training_steps and not model.stop_training :\n",
    "    if (current_step % steps_per_epoch == 0) :\n",
    "#         print(\"{} EPOCH BEGIN\".format(int(current_step // steps_per_epoch) + 1))\n",
    "        for cb in callbacks :\n",
    "            cb[0].on_epoch_begin(int(current_step // steps_per_epoch) + 1)\n",
    "\n",
    "    for cb in callbacks :\n",
    "#         print(\"BATCH_BEGIN - {}\".format(current_step))\n",
    "        cb[0].on_batch_begin(current_step)\n",
    "        \n",
    "    steps = steps_to_run(current_step, steps_between_evals, steps_per_loop)\n",
    "    \n",
    "    if tf.config.list_physical_devices('GPU') :\n",
    "        for _ in steps :\n",
    "            train_single_step(train_iterator)\n",
    "    else :\n",
    "        train_step(train_iterator, tf.convert_to_tensor(steps, dtype = tf.int32))\n",
    "        \n",
    "        # GPU, TPU 아키텍쳐, Iteration HOST ,\n",
    "        # TPU N장 data 각각 들어감.\n",
    "        # GPU는 나눠준다.\n",
    "\n",
    "    train_loss = train_loss_metric.result().numpy().astype(float)\n",
    "    current_step += steps\n",
    "\n",
    "    logs = {'training_loss' : train_loss}\n",
    "\n",
    "    for t_metric in train_metrics :\n",
    "        logs[t_metric.name] = t_metric.result().numpy()\n",
    "\n",
    "    for key in logs.keys() :\n",
    "        if key in history.keys() :\n",
    "            history[key].append(logs[key])\n",
    "        else :\n",
    "            history[key] = [logs[key]]\n",
    "\n",
    "    training_status = 'Train Step: %d/%d  \\n - LOGS - \\n'%(current_step, total_training_steps)\n",
    "    \n",
    "    for key in logs.keys() :\n",
    "        training_status += \"{} : {:.4f}, \".format(key, logs[key])\n",
    "\n",
    "    print(training_status)\n",
    "\n",
    "    if current_step % steps_between_evals:\n",
    "        for cb in callbacks :\n",
    "#             print(\"BATCH_END - {}\".format(current_step - 1))\n",
    "            cb[0].on_batch_end(current_step - 1, {'loss': train_loss})\n",
    "            \n",
    "    else :\n",
    "        print(\"======MLM TEST======\")\n",
    "        inputs, label = next(train_iterator)\n",
    "        sub_res = sub_model([inputs['input_ids'], inputs['input_mask'][:, tf.newaxis, tf.newaxis, :], inputs['segment_ids']])\n",
    "        \n",
    "        layer_x = model.layers[6]\n",
    "        layer_x._output_type = 'predicitions'\n",
    "        output_logits = layer_x([sub_res['sequence_output'], tf.cast(inputs['masked_lm_positions'], dtype = tf.int32)])\n",
    "        \n",
    "        prediction = tf.math.exp(output_logits)\n",
    "        print(tokenizer_for_load.convert_ids_to_tokens(tf.argmax(output_logits[0], axis = 1)))\n",
    "        print(tokenizer_for_load.convert_ids_to_tokens(inputs['masked_lm_ids'][1]))\n",
    "        \n",
    "        lm_labels = inputs['masked_lm_ids'][0]\n",
    "        lm_output = output_logits[0]\n",
    "        lm_label_weights = inputs['masked_lm_weights'][0]\n",
    "        \n",
    "        masked_lm_accuracy = tf.keras.metrics.sparse_categorical_accuracy(lm_labels, lm_output)\n",
    "        numerator = tf.reduce_sum(masked_lm_accuracy * lm_label_weights)\n",
    "        denominator = tf.reduce_sum(lm_label_weights) + 1e-5\n",
    "        masked_lm_accuracy = numerator / denominator\n",
    "        \n",
    "        print(\"REAL_ACC : {}\".format(masked_lm_accuracy))\n",
    "        \n",
    "    \n",
    "    if current_step % steps_per_epoch == 0:\n",
    "        for cb in callbacks :\n",
    "#             print(\"{} EPOCH_END\".format(int(current_step / steps_per_epoch)))\n",
    "            cb[0].on_epoch_end(int(current_step / steps_per_epoch), logs)\n",
    "            \n",
    "    print()\n",
    "for cb in callbacks :\n",
    "    cb[0].on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['training_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['MLM_LOSS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history['NSP_ACC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['NSP_LOSS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_res = sub_model([inputs['input_ids'], inputs['input_mask'][:, tf.newaxis, tf.newaxis, :], inputs['segment_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_x = model.layers[6]\n",
    "layer_x._output_type = 'predicitions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = layer_x([sub_res['sequence_output'], tf.cast(inputs['masked_lm_positions'], dtype = tf.int32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.math.exp(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argmax(prediction[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['masked_lm_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-3000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(tf.argmax(prediction[1], axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(inputs['masked_lm_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_labels = inputs['masked_lm_ids'][0]\n",
    "lm_output = output_logits[0]\n",
    "lm_label_weights = inputs['masked_lm_weights'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_accuracy = tf.keras.metrics.sparse_categorical_accuracy(lm_labels, lm_output)\n",
    "numerator = tf.reduce_sum(masked_lm_accuracy * lm_label_weights)\n",
    "denominator = tf.reduce_sum(lm_label_weights) + 1e-5\n",
    "masked_lm_accuracy = numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
