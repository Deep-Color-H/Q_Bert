{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\LGCNS\\Documents\\GitHub\\Q_Bert\\QBert\\train_utils.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\LGCNS\\Documents\\GitHub\\Q_Bert\\QBert\\models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import datetime\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import train_utils, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해당 파일은 bert.run_pretraining.run_bert_pretrain을 구현하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Parameter는 FLAG 형식에서 직접 정의해주는 방식으로 변경하고, Main에서 직접 정의하도록 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Strategy 정의\n",
    "2. Input_Files 정의\n",
    "3. Bert Config 정의\n",
    "   - bert_config (1. Core_Model - Transformer Encoder - Sub Model)\n",
    "        - vocab_size\n",
    "        - type_vocab_size\n",
    "        - hidden_size\n",
    "        - max_seq_length\n",
    "        - initializer\n",
    "        - kernel_initializer\n",
    "        - initializer_range\n",
    "        - dropout_rate\n",
    "        - num_attention_heads\n",
    "        - intermediate_size\n",
    "        - intermediate_activation\n",
    "        - hidden_act\n",
    "        - attention_dropout_rate\n",
    "        - num_hidden_instances\n",
    "        - pooled_output_dim\n",
    "   - bert_config (2. Pretrained_Model - input to losses)\n",
    "        - (중복 생략)\n",
    "        - max_predictions_per_seq\n",
    "        \n",
    "3. Get Bert Model\n",
    "4. Training Config 정의\n",
    "5. Training\n",
    "6. Test\n",
    "\n",
    "\n",
    "   - init_checkpoint  # Used to initialize only the BERT submodel.\n",
    "   - max_seq_length\n",
    "   -  \n",
    "   - masked_lm\n",
    "   - model_dir\n",
    "   - num_steps_per_epoch\n",
    "   - steps_per_loop\n",
    "   - num_train_epochs\n",
    "   - learning_rate\n",
    "   - warmup_steps\n",
    "   - end_lr\n",
    "   - optimizer_type\n",
    "   - train_batch_size\n",
    "   - use_next_sentence_label\n",
    "   - train_summary_interval\n",
    "   - custom_callbacks\n",
    "   - explicit_allreduce\n",
    "   - pre_allreduce_callbacks\n",
    "   - allreduce_bytes_per_pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices() # device 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "\n",
      "장치의 수: 1\n"
     ]
    }
   ],
   "source": [
    "# Strategy 정의\n",
    "\n",
    "distribution_strategy = 'mirrored' # 'tpu'\n",
    "num_gpus = 0\n",
    "all_reduce_alg = None\n",
    "\n",
    "if distribution_strategy == 'tpu' :\n",
    "    tpu_address = \"\"\n",
    "else :\n",
    "    tpu_address = None\n",
    "\n",
    "\n",
    "\n",
    "strategy = train_utils.get_distribution_strategy(\n",
    "                  distribution_strategy=distribution_strategy,\n",
    "                  num_gpus=num_gpus,\n",
    "                  all_reduce_alg=all_reduce_alg,\n",
    "                  tpu_address=tpu_address)\n",
    "\n",
    "print ('\\n장치의 수: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 256 # 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['./Test_Examples.tfrecords']\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "# keys = feature_description.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "def _select_data_from_record(record):\n",
    "    \"\"\"Filter out features to use for pretraining.\"\"\"\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids'],\n",
    "        'masked_lm_positions': record['masked_lm_positions'],\n",
    "        'masked_lm_ids': record['masked_lm_ids'],\n",
    "        'masked_lm_weights': record['masked_lm_weights'],\n",
    "    }\n",
    "    if use_next_sentence_label:\n",
    "        x['next_sentence_labels'] = record['next_sentence_labels']\n",
    "    if use_position_id:\n",
    "        x['position_ids'] = record['position_ids']\n",
    "\n",
    "    # TODO(hongkuny): Remove the fake labels after migrating bert pretraining.\n",
    "    if output_fake_labels:\n",
    "        return (x, record['masked_lm_weights'])\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = 100\n",
    "\n",
    "GLOBAL_BATCH_SIZE = 2\n",
    "BATCH_SIZE_PER_REPLICA = np.ceil(GLOBAL_BATCH_SIZE // strategy.num_replicas_in_sync)\n",
    "\n",
    "use_next_sentence_label = True\n",
    "output_fake_labels = True\n",
    "use_position_id = False\n",
    "\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    train_dataset = train_dataset.interleave(tf.data.TFRecordDataset\n",
    "                                             , cycle_length = tf.data.experimental.AUTOTUNE\n",
    "                                             , num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    dataset_inputs = train_dataset.map(_parse_function,\n",
    "                                       num_parallel_calls=tf.data.experimental.AUTOTUNE) # String to Example\n",
    "    dataset_inputs_with_labels = dataset_inputs.map(_select_data_from_record,\n",
    "                                                    num_parallel_calls=tf.data.experimental.AUTOTUNE) # Example to InputData\n",
    "    ## 본래대로라면 그냥 써도 되지만, 현재 Label이 없는 데이터이기 때문에\n",
    "    ## max_predictions_per_seq 길이의 허위 정답 (Fake_y)를 삽입하는 mapping function이다.\n",
    "\n",
    "    dataset = dataset_inputs_with_labels\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(10000, reshuffle_each_iteration = True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "    \n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback\n",
    "\n",
    "## model checkpoint\n",
    "t = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "checkpoint_dir = './training_checkpoints_{}'.format(t)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "model_cp = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),\n",
    "\n",
    "## Learning Rate Print\n",
    "\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\n에포크 {}의 학습률은 {}입니다.'.format(epoch + 1,\n",
    "                                                      model.optimizer.lr.numpy()))\n",
    "\n",
    "callbacks = [model_cp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "## Learning Rate Decay\n",
    "\n",
    "# lr = 1e-4 warmup stage (step <= 10000)\n",
    "# Decay linearly\n",
    "\n",
    "init_lr = 1e-4\n",
    "warmup_steps = 10000\n",
    "num_train_steps = 1000000\n",
    "end_lr = 0\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "      initial_learning_rate=init_lr,\n",
    "      decay_steps=num_train_steps,\n",
    "      end_learning_rate=end_lr)\n",
    "\n",
    "lr_schedule = train_utils.WarmUp(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_schedule_fn=lr_schedule,\n",
    "        warmup_steps=warmup_steps)\n",
    "\n",
    "optimizer = train_utils.AdamWeightDecay( \n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-6,\n",
    "    exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def loss_fn(fake_y, losses, **unused_args) :\n",
    "    \n",
    "    return tf.reduce_mean(losses, axis = -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope() :\n",
    "\n",
    "    model, sub_model = models.get_bert_models_fn(vocab_size\n",
    "                                             , hidden_size\n",
    "                                             , type_vocab_size\n",
    "                                             , num_layers\n",
    "                                             , num_attention_heads\n",
    "                                             , max_seq_length\n",
    "                                             , max_predictions_per_seq\n",
    "                                             , dropout_rate\n",
    "                                             , inner_dim \n",
    "                                             , initializer)\n",
    "    model.compile(optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replicated_step(inputs):\n",
    "    \"\"\"Replicated training step.\"\"\"\n",
    "\n",
    "    inputs, labels = inputs\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        model_outputs = model(inputs, training=True)\n",
    "        loss = loss_fn(labels, model_outputs)\n",
    "        # Raw loss is used for reporting in metrics/logs.\n",
    "        raw_loss = loss\n",
    "    \n",
    "        if scale_loss:\n",
    "            # Scales down the loss for gradients to be invariant from replicas.\n",
    "            loss = loss / strategy.num_replicas_in_sync\n",
    "\n",
    "    if isinstance(optimizer, tf.keras.mixed_precision.experimental.LossScaleOptimizer):\n",
    "        \n",
    "        with tape:\n",
    "            scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "        scaled_grads = tape.gradient(scaled_loss, training_vars)\n",
    "        grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        grads = tape.gradient(loss, training_vars)\n",
    "        optimizer.apply_gradients(zip(grads, training_vars))\n",
    "    \n",
    "    # For reporting, the metric takes the mean of losses.\n",
    "    train_loss_metric.update_state(raw_loss)\n",
    "    \n",
    "    for metric in train_metrics:\n",
    "        metric.update_state(labels, model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(iterator, steps):\n",
    "    \n",
    "#     def step_fn(inputs):\n",
    "#         features, labels = inputs\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "            \n",
    "#             losses = model(features)    \n",
    "#             losses = losses * (1.0 / GLOBAL_BATCH_SIZE)\n",
    "\n",
    "#         grads = tape.gradient(losses, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        \n",
    "#         return losses\n",
    "\n",
    "    if not isinstance(steps, tf.Tensor):\n",
    "        raise ValueError('steps should be an Tensor. Python object may cause '\n",
    "                     'retracing.')\n",
    "\n",
    "    for _ in tf.range(steps):\n",
    "        strategy.run(_replicated_step, args=(next(iterator),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_step(iterator):\n",
    "    \"\"\"Performs a distributed training step.\n",
    "\n",
    "    Args:\n",
    "    iterator: the distributed iterator of training datasets.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
    "    \"\"\"\n",
    "    strategy.run(_replicated_step, args=(next(iterator),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps_to_run(current_step, steps_per_epoch, steps_per_loop):\n",
    "    \"\"\"Calculates steps to run on device.\"\"\"\n",
    "    if steps_per_loop <= 0:\n",
    "        raise ValueError('steps_per_loop should be positive integer.')\n",
    "    if steps_per_loop == 1:\n",
    "        return steps_per_loop\n",
    "    remainder_in_epoch = current_step % steps_per_epoch\n",
    "    if remainder_in_epoch != 0:\n",
    "        return min(steps_per_epoch - remainder_in_epoch, steps_per_loop)\n",
    "    else:\n",
    "        return steps_per_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_step = tf.function(train_single_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 25 #25000\n",
    "steps_between_evals = 25 #\n",
    "steps_per_loop = 1\n",
    "epochs = 50\n",
    "\n",
    "total_training_steps = steps_per_epoch * epochs\n",
    "eval_loss_metric = tf.keras.metrics.Mean('traning loss', dtype = tf.float32)\n",
    "train_loss_metric = tf.keras.metrics.Mean('traning loss', dtype = tf.float32)\n",
    "\n",
    "scale_loss = True if strategy.num_replicas_in_sync > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Set\n",
    "train_iterator = iter(dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_metrics = model.metrics\n",
    "train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "\n",
    "# callbacks = tf.keras.callbacks.CallbackList(callbacks) # tf 2.5.0 제공\n",
    "optimizer = model.optimizer\n",
    "training_vars = model.trainable_variables\n",
    "\n",
    "# Callback\n",
    "for cb in callbacks :\n",
    "    cb[0].model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_step = optimizer.iterations.numpy()\n",
    "current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 EPOCH BEGIN\n",
      "BATCH_BEGIN - 250\n",
      "Train Step: 251/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6028, MLM_LOSS : 0.6141, NSP_ACC : 0.6032, NSP_LOSS : 0.6039, \n",
      "BATCH_END - 250\n",
      "\n",
      "BATCH_BEGIN - 251\n",
      "Train Step: 252/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6041, MLM_LOSS : 0.6155, NSP_ACC : 0.6046, NSP_LOSS : 0.6053, \n",
      "BATCH_END - 251\n",
      "\n",
      "BATCH_BEGIN - 252\n",
      "Train Step: 253/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6048, MLM_LOSS : 0.6162, NSP_ACC : 0.6053, NSP_LOSS : 0.6059, \n",
      "BATCH_END - 252\n",
      "\n",
      "BATCH_BEGIN - 253\n",
      "Train Step: 254/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6058, MLM_LOSS : 0.6172, NSP_ACC : 0.6063, NSP_LOSS : 0.6069, \n",
      "BATCH_END - 253\n",
      "\n",
      "BATCH_BEGIN - 254\n",
      "Train Step: 255/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6056, MLM_LOSS : 0.6170, NSP_ACC : 0.6061, NSP_LOSS : 0.6067, \n",
      "BATCH_END - 254\n",
      "\n",
      "BATCH_BEGIN - 255\n",
      "Train Step: 256/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6069, MLM_LOSS : 0.6183, NSP_ACC : 0.6073, NSP_LOSS : 0.6080, \n",
      "BATCH_END - 255\n",
      "\n",
      "BATCH_BEGIN - 256\n",
      "Train Step: 257/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6061, MLM_LOSS : 0.6175, NSP_ACC : 0.6066, NSP_LOSS : 0.6072, \n",
      "BATCH_END - 256\n",
      "\n",
      "BATCH_BEGIN - 257\n",
      "Train Step: 258/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6070, MLM_LOSS : 0.6183, NSP_ACC : 0.6074, NSP_LOSS : 0.6081, \n",
      "BATCH_END - 257\n",
      "\n",
      "BATCH_BEGIN - 258\n",
      "Train Step: 259/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6072, MLM_LOSS : 0.6185, NSP_ACC : 0.6076, NSP_LOSS : 0.6083, \n",
      "BATCH_END - 258\n",
      "\n",
      "BATCH_BEGIN - 259\n",
      "Train Step: 260/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6079, MLM_LOSS : 0.6192, NSP_ACC : 0.6083, NSP_LOSS : 0.6090, \n",
      "BATCH_END - 259\n",
      "\n",
      "BATCH_BEGIN - 260\n",
      "Train Step: 261/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6084, MLM_LOSS : 0.6198, NSP_ACC : 0.6089, NSP_LOSS : 0.6095, \n",
      "BATCH_END - 260\n",
      "\n",
      "BATCH_BEGIN - 261\n",
      "Train Step: 262/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6096, MLM_LOSS : 0.6210, NSP_ACC : 0.6100, NSP_LOSS : 0.6107, \n",
      "BATCH_END - 261\n",
      "\n",
      "BATCH_BEGIN - 262\n",
      "Train Step: 263/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6089, MLM_LOSS : 0.6203, NSP_ACC : 0.6094, NSP_LOSS : 0.6100, \n",
      "BATCH_END - 262\n",
      "\n",
      "BATCH_BEGIN - 263\n",
      "Train Step: 264/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6086, MLM_LOSS : 0.6200, NSP_ACC : 0.6091, NSP_LOSS : 0.6097, \n",
      "BATCH_END - 263\n",
      "\n",
      "BATCH_BEGIN - 264\n",
      "Train Step: 265/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6078, MLM_LOSS : 0.6192, NSP_ACC : 0.6083, NSP_LOSS : 0.6089, \n",
      "BATCH_END - 264\n",
      "\n",
      "BATCH_BEGIN - 265\n",
      "Train Step: 266/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6080, MLM_LOSS : 0.6194, NSP_ACC : 0.6085, NSP_LOSS : 0.6091, \n",
      "BATCH_END - 265\n",
      "\n",
      "BATCH_BEGIN - 266\n",
      "Train Step: 267/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6092, MLM_LOSS : 0.6206, NSP_ACC : 0.6097, NSP_LOSS : 0.6103, \n",
      "BATCH_END - 266\n",
      "\n",
      "BATCH_BEGIN - 267\n",
      "Train Step: 268/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6082, MLM_LOSS : 0.6196, NSP_ACC : 0.6086, NSP_LOSS : 0.6093, \n",
      "BATCH_END - 267\n",
      "\n",
      "BATCH_BEGIN - 268\n",
      "Train Step: 269/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6082, MLM_LOSS : 0.6196, NSP_ACC : 0.6087, NSP_LOSS : 0.6093, \n",
      "BATCH_END - 268\n",
      "\n",
      "BATCH_BEGIN - 269\n",
      "Train Step: 270/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6205, NSP_ACC : 0.6096, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 269\n",
      "\n",
      "BATCH_BEGIN - 270\n",
      "Train Step: 271/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6097, MLM_LOSS : 0.6211, NSP_ACC : 0.6102, NSP_LOSS : 0.6108, \n",
      "BATCH_END - 270\n",
      "\n",
      "BATCH_BEGIN - 271\n",
      "Train Step: 272/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6098, MLM_LOSS : 0.6212, NSP_ACC : 0.6102, NSP_LOSS : 0.6108, \n",
      "BATCH_END - 271\n",
      "\n",
      "BATCH_BEGIN - 272\n",
      "Train Step: 273/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6109, MLM_LOSS : 0.6223, NSP_ACC : 0.6114, NSP_LOSS : 0.6120, \n",
      "BATCH_END - 272\n",
      "\n",
      "BATCH_BEGIN - 273\n",
      "Train Step: 274/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6097, MLM_LOSS : 0.6211, NSP_ACC : 0.6102, NSP_LOSS : 0.6108, \n",
      "BATCH_END - 273\n",
      "\n",
      "BATCH_BEGIN - 274\n",
      "Train Step: 275/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6102, MLM_LOSS : 0.6216, NSP_ACC : 0.6106, NSP_LOSS : 0.6112, \n",
      "11 EPOCH_END\n",
      "\n",
      "12 EPOCH BEGIN\n",
      "BATCH_BEGIN - 275\n",
      "Train Step: 276/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6099, MLM_LOSS : 0.6213, NSP_ACC : 0.6104, NSP_LOSS : 0.6110, \n",
      "BATCH_END - 275\n",
      "\n",
      "BATCH_BEGIN - 276\n",
      "Train Step: 277/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6111, MLM_LOSS : 0.6225, NSP_ACC : 0.6116, NSP_LOSS : 0.6122, \n",
      "BATCH_END - 276\n",
      "\n",
      "BATCH_BEGIN - 277\n",
      "Train Step: 278/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6102, MLM_LOSS : 0.6216, NSP_ACC : 0.6107, NSP_LOSS : 0.6113, \n",
      "BATCH_END - 277\n",
      "\n",
      "BATCH_BEGIN - 278\n",
      "Train Step: 279/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6116, MLM_LOSS : 0.6230, NSP_ACC : 0.6121, NSP_LOSS : 0.6127, \n",
      "BATCH_END - 278\n",
      "\n",
      "BATCH_BEGIN - 279\n",
      "Train Step: 280/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6115, MLM_LOSS : 0.6229, NSP_ACC : 0.6119, NSP_LOSS : 0.6125, \n",
      "BATCH_END - 279\n",
      "\n",
      "BATCH_BEGIN - 280\n",
      "Train Step: 281/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6113, MLM_LOSS : 0.6227, NSP_ACC : 0.6118, NSP_LOSS : 0.6124, \n",
      "BATCH_END - 280\n",
      "\n",
      "BATCH_BEGIN - 281\n",
      "Train Step: 282/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6107, MLM_LOSS : 0.6221, NSP_ACC : 0.6111, NSP_LOSS : 0.6117, \n",
      "BATCH_END - 281\n",
      "\n",
      "BATCH_BEGIN - 282\n",
      "Train Step: 283/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6096, MLM_LOSS : 0.6210, NSP_ACC : 0.6100, NSP_LOSS : 0.6106, \n",
      "BATCH_END - 282\n",
      "\n",
      "BATCH_BEGIN - 283\n",
      "Train Step: 284/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6103, MLM_LOSS : 0.6217, NSP_ACC : 0.6108, NSP_LOSS : 0.6113, \n",
      "BATCH_END - 283\n",
      "\n",
      "BATCH_BEGIN - 284\n",
      "Train Step: 286/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 285\n",
      "\n",
      "BATCH_BEGIN - 286\n",
      "Train Step: 287/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6108, MLM_LOSS : 0.6222, NSP_ACC : 0.6112, NSP_LOSS : 0.6118, \n",
      "BATCH_END - 286\n",
      "\n",
      "BATCH_BEGIN - 287\n",
      "Train Step: 288/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6107, MLM_LOSS : 0.6221, NSP_ACC : 0.6112, NSP_LOSS : 0.6118, \n",
      "BATCH_END - 287\n",
      "\n",
      "BATCH_BEGIN - 288\n",
      "Train Step: 289/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6104, MLM_LOSS : 0.6218, NSP_ACC : 0.6109, NSP_LOSS : 0.6115, \n",
      "BATCH_END - 288\n",
      "\n",
      "BATCH_BEGIN - 289\n",
      "Train Step: 290/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6105, MLM_LOSS : 0.6219, NSP_ACC : 0.6109, NSP_LOSS : 0.6115, \n",
      "BATCH_END - 289\n",
      "\n",
      "BATCH_BEGIN - 290\n",
      "Train Step: 291/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6092, MLM_LOSS : 0.6206, NSP_ACC : 0.6097, NSP_LOSS : 0.6103, \n",
      "BATCH_END - 290\n",
      "\n",
      "BATCH_BEGIN - 291\n",
      "Train Step: 292/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6102, MLM_LOSS : 0.6216, NSP_ACC : 0.6107, NSP_LOSS : 0.6113, \n",
      "BATCH_END - 291\n",
      "\n",
      "BATCH_BEGIN - 292\n",
      "Train Step: 293/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6111, MLM_LOSS : 0.6225, NSP_ACC : 0.6116, NSP_LOSS : 0.6122, \n",
      "BATCH_END - 292\n",
      "\n",
      "BATCH_BEGIN - 293\n",
      "Train Step: 294/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6112, MLM_LOSS : 0.6226, NSP_ACC : 0.6116, NSP_LOSS : 0.6123, \n",
      "BATCH_END - 293\n",
      "\n",
      "BATCH_BEGIN - 294\n",
      "Train Step: 295/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6099, MLM_LOSS : 0.6213, NSP_ACC : 0.6104, NSP_LOSS : 0.6110, \n",
      "BATCH_END - 294\n",
      "\n",
      "BATCH_BEGIN - 295\n",
      "Train Step: 296/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 295\n",
      "\n",
      "BATCH_BEGIN - 296\n",
      "Train Step: 297/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6102, MLM_LOSS : 0.6217, NSP_ACC : 0.6107, NSP_LOSS : 0.6113, \n",
      "BATCH_END - 296\n",
      "\n",
      "BATCH_BEGIN - 297\n",
      "Train Step: 298/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6107, MLM_LOSS : 0.6221, NSP_ACC : 0.6112, NSP_LOSS : 0.6118, \n",
      "BATCH_END - 297\n",
      "\n",
      "BATCH_BEGIN - 298\n",
      "Train Step: 299/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6099, MLM_LOSS : 0.6213, NSP_ACC : 0.6103, NSP_LOSS : 0.6109, \n",
      "BATCH_END - 298\n",
      "\n",
      "BATCH_BEGIN - 299\n",
      "Train Step: 300/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6084, MLM_LOSS : 0.6198, NSP_ACC : 0.6089, NSP_LOSS : 0.6095, \n",
      "12 EPOCH_END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13 EPOCH BEGIN\n",
      "BATCH_BEGIN - 300\n",
      "Train Step: 301/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6087, MLM_LOSS : 0.6201, NSP_ACC : 0.6092, NSP_LOSS : 0.6098, \n",
      "BATCH_END - 300\n",
      "\n",
      "BATCH_BEGIN - 301\n",
      "Train Step: 302/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6097, MLM_LOSS : 0.6211, NSP_ACC : 0.6102, NSP_LOSS : 0.6108, \n",
      "BATCH_END - 301\n",
      "\n",
      "BATCH_BEGIN - 302\n",
      "Train Step: 303/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6092, MLM_LOSS : 0.6206, NSP_ACC : 0.6097, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 302\n",
      "\n",
      "BATCH_BEGIN - 303\n",
      "Train Step: 304/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6081, MLM_LOSS : 0.6195, NSP_ACC : 0.6086, NSP_LOSS : 0.6092, \n",
      "BATCH_END - 303\n",
      "\n",
      "BATCH_BEGIN - 304\n",
      "Train Step: 305/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6081, MLM_LOSS : 0.6195, NSP_ACC : 0.6086, NSP_LOSS : 0.6091, \n",
      "BATCH_END - 304\n",
      "\n",
      "BATCH_BEGIN - 305\n",
      "Train Step: 306/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6206, NSP_ACC : 0.6096, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 305\n",
      "\n",
      "BATCH_BEGIN - 306\n",
      "Train Step: 307/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6101, MLM_LOSS : 0.6215, NSP_ACC : 0.6105, NSP_LOSS : 0.6111, \n",
      "BATCH_END - 306\n",
      "\n",
      "BATCH_BEGIN - 307\n",
      "Train Step: 308/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6089, MLM_LOSS : 0.6203, NSP_ACC : 0.6094, NSP_LOSS : 0.6100, \n",
      "BATCH_END - 307\n",
      "\n",
      "BATCH_BEGIN - 308\n",
      "Train Step: 309/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6083, MLM_LOSS : 0.6197, NSP_ACC : 0.6088, NSP_LOSS : 0.6094, \n",
      "BATCH_END - 308\n",
      "\n",
      "BATCH_BEGIN - 309\n",
      "Train Step: 310/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6089, MLM_LOSS : 0.6203, NSP_ACC : 0.6094, NSP_LOSS : 0.6100, \n",
      "BATCH_END - 309\n",
      "\n",
      "BATCH_BEGIN - 310\n",
      "Train Step: 311/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6206, NSP_ACC : 0.6096, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 310\n",
      "\n",
      "BATCH_BEGIN - 311\n",
      "Train Step: 312/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6092, MLM_LOSS : 0.6206, NSP_ACC : 0.6097, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 311\n",
      "\n",
      "BATCH_BEGIN - 312\n",
      "Train Step: 313/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6205, NSP_ACC : 0.6096, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 312\n",
      "\n",
      "BATCH_BEGIN - 313\n",
      "Train Step: 314/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6081, MLM_LOSS : 0.6195, NSP_ACC : 0.6085, NSP_LOSS : 0.6091, \n",
      "BATCH_END - 313\n",
      "\n",
      "BATCH_BEGIN - 314\n",
      "Train Step: 315/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6078, MLM_LOSS : 0.6192, NSP_ACC : 0.6083, NSP_LOSS : 0.6089, \n",
      "BATCH_END - 314\n",
      "\n",
      "BATCH_BEGIN - 315\n",
      "Train Step: 316/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6089, MLM_LOSS : 0.6204, NSP_ACC : 0.6094, NSP_LOSS : 0.6100, \n",
      "BATCH_END - 315\n",
      "\n",
      "BATCH_BEGIN - 316\n",
      "Train Step: 317/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6077, MLM_LOSS : 0.6191, NSP_ACC : 0.6082, NSP_LOSS : 0.6088, \n",
      "BATCH_END - 316\n",
      "\n",
      "BATCH_BEGIN - 317\n",
      "Train Step: 318/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6070, MLM_LOSS : 0.6184, NSP_ACC : 0.6075, NSP_LOSS : 0.6081, \n",
      "BATCH_END - 317\n",
      "\n",
      "BATCH_BEGIN - 318\n",
      "Train Step: 319/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6081, MLM_LOSS : 0.6195, NSP_ACC : 0.6086, NSP_LOSS : 0.6092, \n",
      "BATCH_END - 318\n",
      "\n",
      "BATCH_BEGIN - 319\n",
      "Train Step: 320/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6084, MLM_LOSS : 0.6198, NSP_ACC : 0.6089, NSP_LOSS : 0.6095, \n",
      "BATCH_END - 319\n",
      "\n",
      "BATCH_BEGIN - 320\n",
      "Train Step: 321/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6086, MLM_LOSS : 0.6200, NSP_ACC : 0.6090, NSP_LOSS : 0.6096, \n",
      "BATCH_END - 320\n",
      "\n",
      "BATCH_BEGIN - 321\n",
      "Train Step: 322/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6088, MLM_LOSS : 0.6203, NSP_ACC : 0.6093, NSP_LOSS : 0.6099, \n",
      "BATCH_END - 321\n",
      "\n",
      "BATCH_BEGIN - 322\n",
      "Train Step: 323/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6087, MLM_LOSS : 0.6201, NSP_ACC : 0.6092, NSP_LOSS : 0.6098, \n",
      "BATCH_END - 322\n",
      "\n",
      "BATCH_BEGIN - 323\n",
      "Train Step: 324/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6088, MLM_LOSS : 0.6202, NSP_ACC : 0.6093, NSP_LOSS : 0.6099, \n",
      "BATCH_END - 323\n",
      "\n",
      "BATCH_BEGIN - 324\n",
      "Train Step: 325/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6078, MLM_LOSS : 0.6192, NSP_ACC : 0.6083, NSP_LOSS : 0.6089, \n",
      "13 EPOCH_END\n",
      "\n",
      "14 EPOCH BEGIN\n",
      "BATCH_BEGIN - 325\n",
      "Train Step: 326/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6079, MLM_LOSS : 0.6193, NSP_ACC : 0.6084, NSP_LOSS : 0.6090, \n",
      "BATCH_END - 325\n",
      "\n",
      "BATCH_BEGIN - 326\n",
      "Train Step: 327/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6080, MLM_LOSS : 0.6194, NSP_ACC : 0.6085, NSP_LOSS : 0.6091, \n",
      "BATCH_END - 326\n",
      "\n",
      "BATCH_BEGIN - 327\n",
      "Train Step: 328/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6079, MLM_LOSS : 0.6193, NSP_ACC : 0.6084, NSP_LOSS : 0.6090, \n",
      "BATCH_END - 327\n",
      "\n",
      "BATCH_BEGIN - 328\n",
      "Train Step: 329/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6086, MLM_LOSS : 0.6200, NSP_ACC : 0.6091, NSP_LOSS : 0.6097, \n",
      "BATCH_END - 328\n",
      "\n",
      "BATCH_BEGIN - 329\n",
      "Train Step: 330/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6080, MLM_LOSS : 0.6194, NSP_ACC : 0.6085, NSP_LOSS : 0.6091, \n",
      "BATCH_END - 329\n",
      "\n",
      "BATCH_BEGIN - 330\n",
      "Train Step: 331/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6087, MLM_LOSS : 0.6201, NSP_ACC : 0.6091, NSP_LOSS : 0.6098, \n",
      "BATCH_END - 330\n",
      "\n",
      "BATCH_BEGIN - 331\n",
      "Train Step: 332/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6092, MLM_LOSS : 0.6206, NSP_ACC : 0.6097, NSP_LOSS : 0.6103, \n",
      "BATCH_END - 331\n",
      "\n",
      "BATCH_BEGIN - 332\n",
      "Train Step: 333/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6081, MLM_LOSS : 0.6195, NSP_ACC : 0.6086, NSP_LOSS : 0.6092, \n",
      "BATCH_END - 332\n",
      "\n",
      "BATCH_BEGIN - 333\n",
      "Train Step: 334/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6067, MLM_LOSS : 0.6181, NSP_ACC : 0.6072, NSP_LOSS : 0.6078, \n",
      "BATCH_END - 333\n",
      "\n",
      "BATCH_BEGIN - 334\n",
      "Train Step: 335/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6069, MLM_LOSS : 0.6183, NSP_ACC : 0.6073, NSP_LOSS : 0.6079, \n",
      "BATCH_END - 334\n",
      "\n",
      "BATCH_BEGIN - 335\n",
      "Train Step: 336/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6063, MLM_LOSS : 0.6177, NSP_ACC : 0.6068, NSP_LOSS : 0.6074, \n",
      "BATCH_END - 335\n",
      "\n",
      "BATCH_BEGIN - 336\n",
      "Train Step: 337/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6063, MLM_LOSS : 0.6177, NSP_ACC : 0.6068, NSP_LOSS : 0.6074, \n",
      "BATCH_END - 336\n",
      "\n",
      "BATCH_BEGIN - 337\n",
      "Train Step: 338/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6056, MLM_LOSS : 0.6170, NSP_ACC : 0.6060, NSP_LOSS : 0.6067, \n",
      "BATCH_END - 337\n",
      "\n",
      "BATCH_BEGIN - 338\n",
      "Train Step: 339/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6066, MLM_LOSS : 0.6180, NSP_ACC : 0.6071, NSP_LOSS : 0.6077, \n",
      "BATCH_END - 338\n",
      "\n",
      "BATCH_BEGIN - 339\n",
      "Train Step: 340/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6063, MLM_LOSS : 0.6177, NSP_ACC : 0.6068, NSP_LOSS : 0.6074, \n",
      "BATCH_END - 339\n",
      "\n",
      "BATCH_BEGIN - 340\n",
      "Train Step: 341/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6065, MLM_LOSS : 0.6179, NSP_ACC : 0.6069, NSP_LOSS : 0.6076, \n",
      "BATCH_END - 340\n",
      "\n",
      "BATCH_BEGIN - 341\n",
      "Train Step: 342/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6067, MLM_LOSS : 0.6181, NSP_ACC : 0.6072, NSP_LOSS : 0.6078, \n",
      "BATCH_END - 341\n",
      "\n",
      "BATCH_BEGIN - 342\n",
      "Train Step: 343/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6077, MLM_LOSS : 0.6191, NSP_ACC : 0.6082, NSP_LOSS : 0.6088, \n",
      "BATCH_END - 342\n",
      "\n",
      "BATCH_BEGIN - 343\n",
      "Train Step: 344/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6069, MLM_LOSS : 0.6183, NSP_ACC : 0.6073, NSP_LOSS : 0.6079, \n",
      "BATCH_END - 343\n",
      "\n",
      "BATCH_BEGIN - 344\n",
      "Train Step: 345/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6068, MLM_LOSS : 0.6182, NSP_ACC : 0.6073, NSP_LOSS : 0.6079, \n",
      "BATCH_END - 344\n",
      "\n",
      "BATCH_BEGIN - 345\n",
      "Train Step: 346/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6062, MLM_LOSS : 0.6176, NSP_ACC : 0.6067, NSP_LOSS : 0.6073, \n",
      "BATCH_END - 345\n",
      "\n",
      "BATCH_BEGIN - 346\n",
      "Train Step: 347/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6061, MLM_LOSS : 0.6175, NSP_ACC : 0.6065, NSP_LOSS : 0.6072, \n",
      "BATCH_END - 346\n",
      "\n",
      "BATCH_BEGIN - 347\n",
      "Train Step: 348/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6061, MLM_LOSS : 0.6175, NSP_ACC : 0.6065, NSP_LOSS : 0.6071, \n",
      "BATCH_END - 347\n",
      "\n",
      "BATCH_BEGIN - 348\n",
      "Train Step: 349/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6070, MLM_LOSS : 0.6184, NSP_ACC : 0.6075, NSP_LOSS : 0.6081, \n",
      "BATCH_END - 348\n",
      "\n",
      "BATCH_BEGIN - 349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 350/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6073, MLM_LOSS : 0.6187, NSP_ACC : 0.6078, NSP_LOSS : 0.6084, \n",
      "14 EPOCH_END\n",
      "\n",
      "15 EPOCH BEGIN\n",
      "BATCH_BEGIN - 350\n",
      "Train Step: 351/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6075, MLM_LOSS : 0.6189, NSP_ACC : 0.6080, NSP_LOSS : 0.6086, \n",
      "BATCH_END - 350\n",
      "\n",
      "BATCH_BEGIN - 351\n",
      "Train Step: 352/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6077, MLM_LOSS : 0.6191, NSP_ACC : 0.6082, NSP_LOSS : 0.6088, \n",
      "BATCH_END - 351\n",
      "\n",
      "BATCH_BEGIN - 352\n",
      "Train Step: 353/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6077, MLM_LOSS : 0.6191, NSP_ACC : 0.6081, NSP_LOSS : 0.6088, \n",
      "BATCH_END - 352\n",
      "\n",
      "BATCH_BEGIN - 353\n",
      "Train Step: 354/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6077, MLM_LOSS : 0.6191, NSP_ACC : 0.6081, NSP_LOSS : 0.6087, \n",
      "BATCH_END - 353\n",
      "\n",
      "BATCH_BEGIN - 354\n",
      "Train Step: 355/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6074, MLM_LOSS : 0.6188, NSP_ACC : 0.6079, NSP_LOSS : 0.6085, \n",
      "BATCH_END - 354\n",
      "\n",
      "BATCH_BEGIN - 355\n",
      "Train Step: 356/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6075, MLM_LOSS : 0.6189, NSP_ACC : 0.6080, NSP_LOSS : 0.6086, \n",
      "BATCH_END - 355\n",
      "\n",
      "BATCH_BEGIN - 356\n",
      "Train Step: 357/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6085, MLM_LOSS : 0.6199, NSP_ACC : 0.6089, NSP_LOSS : 0.6096, \n",
      "BATCH_END - 356\n",
      "\n",
      "BATCH_BEGIN - 357\n",
      "Train Step: 358/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6084, MLM_LOSS : 0.6198, NSP_ACC : 0.6089, NSP_LOSS : 0.6095, \n",
      "BATCH_END - 357\n",
      "\n",
      "BATCH_BEGIN - 358\n",
      "Train Step: 359/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6084, MLM_LOSS : 0.6198, NSP_ACC : 0.6089, NSP_LOSS : 0.6095, \n",
      "BATCH_END - 358\n",
      "\n",
      "BATCH_BEGIN - 359\n",
      "Train Step: 360/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6094, MLM_LOSS : 0.6208, NSP_ACC : 0.6098, NSP_LOSS : 0.6104, \n",
      "BATCH_END - 359\n",
      "\n",
      "BATCH_BEGIN - 360\n",
      "Train Step: 361/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6092, MLM_LOSS : 0.6206, NSP_ACC : 0.6096, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 360\n",
      "\n",
      "BATCH_BEGIN - 361\n",
      "Train Step: 362/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6088, MLM_LOSS : 0.6202, NSP_ACC : 0.6092, NSP_LOSS : 0.6099, \n",
      "BATCH_END - 361\n",
      "\n",
      "BATCH_BEGIN - 362\n",
      "Train Step: 363/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6088, MLM_LOSS : 0.6202, NSP_ACC : 0.6092, NSP_LOSS : 0.6098, \n",
      "BATCH_END - 362\n",
      "\n",
      "BATCH_BEGIN - 363\n",
      "Train Step: 364/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6087, MLM_LOSS : 0.6201, NSP_ACC : 0.6092, NSP_LOSS : 0.6098, \n",
      "BATCH_END - 363\n",
      "\n",
      "BATCH_BEGIN - 364\n",
      "Train Step: 365/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6205, NSP_ACC : 0.6095, NSP_LOSS : 0.6101, \n",
      "BATCH_END - 364\n",
      "\n",
      "BATCH_BEGIN - 365\n",
      "Train Step: 366/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6090, MLM_LOSS : 0.6205, NSP_ACC : 0.6095, NSP_LOSS : 0.6101, \n",
      "BATCH_END - 365\n",
      "\n",
      "BATCH_BEGIN - 366\n",
      "Train Step: 367/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6205, NSP_ACC : 0.6095, NSP_LOSS : 0.6101, \n",
      "BATCH_END - 366\n",
      "\n",
      "BATCH_BEGIN - 367\n",
      "Train Step: 368/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6094, MLM_LOSS : 0.6208, NSP_ACC : 0.6099, NSP_LOSS : 0.6105, \n",
      "BATCH_END - 367\n",
      "\n",
      "BATCH_BEGIN - 368\n",
      "Train Step: 369/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6205, NSP_ACC : 0.6096, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 368\n",
      "\n",
      "BATCH_BEGIN - 369\n",
      "Train Step: 370/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6080, MLM_LOSS : 0.6194, NSP_ACC : 0.6085, NSP_LOSS : 0.6091, \n",
      "BATCH_END - 369\n",
      "\n",
      "BATCH_BEGIN - 370\n",
      "Train Step: 371/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6069, MLM_LOSS : 0.6184, NSP_ACC : 0.6074, NSP_LOSS : 0.6080, \n",
      "BATCH_END - 370\n",
      "\n",
      "BATCH_BEGIN - 371\n",
      "Train Step: 372/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6075, MLM_LOSS : 0.6189, NSP_ACC : 0.6080, NSP_LOSS : 0.6086, \n",
      "BATCH_END - 371\n",
      "\n",
      "BATCH_BEGIN - 372\n",
      "Train Step: 373/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6074, MLM_LOSS : 0.6188, NSP_ACC : 0.6079, NSP_LOSS : 0.6085, \n",
      "BATCH_END - 372\n",
      "\n",
      "BATCH_BEGIN - 373\n",
      "Train Step: 374/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6078, MLM_LOSS : 0.6192, NSP_ACC : 0.6083, NSP_LOSS : 0.6089, \n",
      "BATCH_END - 373\n",
      "\n",
      "BATCH_BEGIN - 374\n",
      "Train Step: 375/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6078, MLM_LOSS : 0.6192, NSP_ACC : 0.6083, NSP_LOSS : 0.6089, \n",
      "15 EPOCH_END\n",
      "\n",
      "16 EPOCH BEGIN\n",
      "BATCH_BEGIN - 375\n",
      "Train Step: 376/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6085, MLM_LOSS : 0.6200, NSP_ACC : 0.6090, NSP_LOSS : 0.6096, \n",
      "BATCH_END - 375\n",
      "\n",
      "BATCH_BEGIN - 376\n",
      "Train Step: 377/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6089, MLM_LOSS : 0.6203, NSP_ACC : 0.6094, NSP_LOSS : 0.6100, \n",
      "BATCH_END - 376\n",
      "\n",
      "BATCH_BEGIN - 377\n",
      "Train Step: 378/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6096, MLM_LOSS : 0.6210, NSP_ACC : 0.6101, NSP_LOSS : 0.6107, \n",
      "BATCH_END - 377\n",
      "\n",
      "BATCH_BEGIN - 378\n",
      "Train Step: 379/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6099, MLM_LOSS : 0.6213, NSP_ACC : 0.6104, NSP_LOSS : 0.6109, \n",
      "BATCH_END - 378\n",
      "\n",
      "BATCH_BEGIN - 379\n",
      "Train Step: 380/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6098, MLM_LOSS : 0.6212, NSP_ACC : 0.6103, NSP_LOSS : 0.6109, \n",
      "BATCH_END - 379\n",
      "\n",
      "BATCH_BEGIN - 380\n",
      "Train Step: 381/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6096, MLM_LOSS : 0.6211, NSP_ACC : 0.6101, NSP_LOSS : 0.6107, \n",
      "BATCH_END - 380\n",
      "\n",
      "BATCH_BEGIN - 381\n",
      "Train Step: 382/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6098, MLM_LOSS : 0.6212, NSP_ACC : 0.6103, NSP_LOSS : 0.6109, \n",
      "BATCH_END - 381\n",
      "\n",
      "BATCH_BEGIN - 382\n",
      "Train Step: 383/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6101, MLM_LOSS : 0.6215, NSP_ACC : 0.6106, NSP_LOSS : 0.6112, \n",
      "BATCH_END - 382\n",
      "\n",
      "BATCH_BEGIN - 383\n",
      "Train Step: 384/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6102, MLM_LOSS : 0.6216, NSP_ACC : 0.6107, NSP_LOSS : 0.6113, \n",
      "BATCH_END - 383\n",
      "\n",
      "BATCH_BEGIN - 384\n",
      "Train Step: 385/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6205, NSP_ACC : 0.6096, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 384\n",
      "\n",
      "BATCH_BEGIN - 385\n",
      "Train Step: 386/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6091, MLM_LOSS : 0.6206, NSP_ACC : 0.6096, NSP_LOSS : 0.6102, \n",
      "BATCH_END - 385\n",
      "\n",
      "BATCH_BEGIN - 386\n",
      "Train Step: 387/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6100, MLM_LOSS : 0.6214, NSP_ACC : 0.6105, NSP_LOSS : 0.6111, \n",
      "BATCH_END - 386\n",
      "\n",
      "BATCH_BEGIN - 387\n",
      "Train Step: 388/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6108, MLM_LOSS : 0.6222, NSP_ACC : 0.6113, NSP_LOSS : 0.6119, \n",
      "BATCH_END - 387\n",
      "\n",
      "BATCH_BEGIN - 388\n",
      "Train Step: 389/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6120, \n",
      "BATCH_END - 388\n",
      "\n",
      "BATCH_BEGIN - 389\n",
      "Train Step: 390/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6225, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 389\n",
      "\n",
      "BATCH_BEGIN - 390\n",
      "Train Step: 391/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6109, MLM_LOSS : 0.6224, NSP_ACC : 0.6114, NSP_LOSS : 0.6120, \n",
      "BATCH_END - 390\n",
      "\n",
      "BATCH_BEGIN - 391\n",
      "Train Step: 392/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6115, MLM_LOSS : 0.6230, NSP_ACC : 0.6120, NSP_LOSS : 0.6126, \n",
      "BATCH_END - 391\n",
      "\n",
      "BATCH_BEGIN - 392\n",
      "Train Step: 393/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6123, MLM_LOSS : 0.6238, NSP_ACC : 0.6128, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 392\n",
      "\n",
      "BATCH_BEGIN - 393\n",
      "Train Step: 394/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6236, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 393\n",
      "\n",
      "BATCH_BEGIN - 394\n",
      "Train Step: 395/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6122, MLM_LOSS : 0.6236, NSP_ACC : 0.6127, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 394\n",
      "\n",
      "BATCH_BEGIN - 395\n",
      "Train Step: 396/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6125, MLM_LOSS : 0.6239, NSP_ACC : 0.6130, NSP_LOSS : 0.6135, \n",
      "BATCH_END - 395\n",
      "\n",
      "BATCH_BEGIN - 396\n",
      "Train Step: 397/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6130, MLM_LOSS : 0.6244, NSP_ACC : 0.6135, NSP_LOSS : 0.6141, \n",
      "BATCH_END - 396\n",
      "\n",
      "BATCH_BEGIN - 397\n",
      "Train Step: 398/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6126, MLM_LOSS : 0.6240, NSP_ACC : 0.6131, NSP_LOSS : 0.6137, \n",
      "BATCH_END - 397\n",
      "\n",
      "BATCH_BEGIN - 398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 399/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6125, MLM_LOSS : 0.6239, NSP_ACC : 0.6129, NSP_LOSS : 0.6135, \n",
      "BATCH_END - 398\n",
      "\n",
      "BATCH_BEGIN - 399\n",
      "Train Step: 400/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6120, MLM_LOSS : 0.6234, NSP_ACC : 0.6125, NSP_LOSS : 0.6131, \n",
      "16 EPOCH_END\n",
      "\n",
      "17 EPOCH BEGIN\n",
      "BATCH_BEGIN - 400\n",
      "Train Step: 401/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6112, MLM_LOSS : 0.6226, NSP_ACC : 0.6117, NSP_LOSS : 0.6123, \n",
      "BATCH_END - 400\n",
      "\n",
      "BATCH_BEGIN - 401\n",
      "Train Step: 402/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 401\n",
      "\n",
      "BATCH_BEGIN - 402\n",
      "Train Step: 403/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6108, MLM_LOSS : 0.6222, NSP_ACC : 0.6113, NSP_LOSS : 0.6118, \n",
      "BATCH_END - 402\n",
      "\n",
      "BATCH_BEGIN - 403\n",
      "Train Step: 404/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6101, MLM_LOSS : 0.6216, NSP_ACC : 0.6106, NSP_LOSS : 0.6112, \n",
      "BATCH_END - 403\n",
      "\n",
      "BATCH_BEGIN - 404\n",
      "Train Step: 405/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6102, MLM_LOSS : 0.6216, NSP_ACC : 0.6107, NSP_LOSS : 0.6112, \n",
      "BATCH_END - 404\n",
      "\n",
      "BATCH_BEGIN - 405\n",
      "Train Step: 406/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6100, MLM_LOSS : 0.6215, NSP_ACC : 0.6105, NSP_LOSS : 0.6111, \n",
      "BATCH_END - 405\n",
      "\n",
      "BATCH_BEGIN - 406\n",
      "Train Step: 407/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6105, MLM_LOSS : 0.6219, NSP_ACC : 0.6110, NSP_LOSS : 0.6116, \n",
      "BATCH_END - 406\n",
      "\n",
      "BATCH_BEGIN - 407\n",
      "Train Step: 408/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6108, MLM_LOSS : 0.6222, NSP_ACC : 0.6113, NSP_LOSS : 0.6119, \n",
      "BATCH_END - 407\n",
      "\n",
      "BATCH_BEGIN - 408\n",
      "Train Step: 409/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6111, MLM_LOSS : 0.6226, NSP_ACC : 0.6116, NSP_LOSS : 0.6122, \n",
      "BATCH_END - 408\n",
      "\n",
      "BATCH_BEGIN - 409\n",
      "Train Step: 410/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6107, MLM_LOSS : 0.6221, NSP_ACC : 0.6112, NSP_LOSS : 0.6117, \n",
      "BATCH_END - 409\n",
      "\n",
      "BATCH_BEGIN - 410\n",
      "Train Step: 411/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6105, MLM_LOSS : 0.6219, NSP_ACC : 0.6110, NSP_LOSS : 0.6115, \n",
      "BATCH_END - 410\n",
      "\n",
      "BATCH_BEGIN - 411\n",
      "Train Step: 412/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6112, MLM_LOSS : 0.6227, NSP_ACC : 0.6117, NSP_LOSS : 0.6123, \n",
      "BATCH_END - 411\n",
      "\n",
      "BATCH_BEGIN - 412\n",
      "Train Step: 413/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6105, MLM_LOSS : 0.6219, NSP_ACC : 0.6110, NSP_LOSS : 0.6116, \n",
      "BATCH_END - 412\n",
      "\n",
      "BATCH_BEGIN - 413\n",
      "Train Step: 414/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6106, MLM_LOSS : 0.6220, NSP_ACC : 0.6111, NSP_LOSS : 0.6117, \n",
      "BATCH_END - 413\n",
      "\n",
      "BATCH_BEGIN - 414\n",
      "Train Step: 415/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 414\n",
      "\n",
      "BATCH_BEGIN - 415\n",
      "Train Step: 416/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6122, MLM_LOSS : 0.6236, NSP_ACC : 0.6127, NSP_LOSS : 0.6133, \n",
      "BATCH_END - 415\n",
      "\n",
      "BATCH_BEGIN - 416\n",
      "Train Step: 417/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6126, MLM_LOSS : 0.6240, NSP_ACC : 0.6130, NSP_LOSS : 0.6136, \n",
      "BATCH_END - 416\n",
      "\n",
      "BATCH_BEGIN - 417\n",
      "Train Step: 418/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6129, MLM_LOSS : 0.6243, NSP_ACC : 0.6134, NSP_LOSS : 0.6140, \n",
      "BATCH_END - 417\n",
      "\n",
      "BATCH_BEGIN - 418\n",
      "Train Step: 419/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6136, MLM_LOSS : 0.6250, NSP_ACC : 0.6141, NSP_LOSS : 0.6147, \n",
      "BATCH_END - 418\n",
      "\n",
      "BATCH_BEGIN - 419\n",
      "Train Step: 420/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6135, MLM_LOSS : 0.6249, NSP_ACC : 0.6140, NSP_LOSS : 0.6146, \n",
      "BATCH_END - 419\n",
      "\n",
      "BATCH_BEGIN - 420\n",
      "Train Step: 421/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6132, MLM_LOSS : 0.6246, NSP_ACC : 0.6137, NSP_LOSS : 0.6143, \n",
      "BATCH_END - 420\n",
      "\n",
      "BATCH_BEGIN - 421\n",
      "Train Step: 422/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6127, MLM_LOSS : 0.6241, NSP_ACC : 0.6132, NSP_LOSS : 0.6138, \n",
      "BATCH_END - 421\n",
      "\n",
      "BATCH_BEGIN - 422\n",
      "Train Step: 423/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6123, MLM_LOSS : 0.6237, NSP_ACC : 0.6128, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 422\n",
      "\n",
      "BATCH_BEGIN - 423\n",
      "Train Step: 424/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6113, MLM_LOSS : 0.6227, NSP_ACC : 0.6118, NSP_LOSS : 0.6124, \n",
      "BATCH_END - 423\n",
      "\n",
      "BATCH_BEGIN - 424\n",
      "Train Step: 425/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6112, MLM_LOSS : 0.6226, NSP_ACC : 0.6117, NSP_LOSS : 0.6123, \n",
      "17 EPOCH_END\n",
      "\n",
      "18 EPOCH BEGIN\n",
      "BATCH_BEGIN - 425\n",
      "Train Step: 426/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6235, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 425\n",
      "\n",
      "BATCH_BEGIN - 426\n",
      "Train Step: 427/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6123, MLM_LOSS : 0.6237, NSP_ACC : 0.6128, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 426\n",
      "\n",
      "BATCH_BEGIN - 427\n",
      "Train Step: 428/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6235, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 427\n",
      "\n",
      "BATCH_BEGIN - 428\n",
      "Train Step: 429/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6126, MLM_LOSS : 0.6240, NSP_ACC : 0.6131, NSP_LOSS : 0.6137, \n",
      "BATCH_END - 428\n",
      "\n",
      "BATCH_BEGIN - 429\n",
      "Train Step: 430/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6124, MLM_LOSS : 0.6238, NSP_ACC : 0.6129, NSP_LOSS : 0.6135, \n",
      "BATCH_END - 429\n",
      "\n",
      "BATCH_BEGIN - 430\n",
      "Train Step: 431/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6130, MLM_LOSS : 0.6244, NSP_ACC : 0.6135, NSP_LOSS : 0.6141, \n",
      "BATCH_END - 430\n",
      "\n",
      "BATCH_BEGIN - 431\n",
      "Train Step: 432/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6120, MLM_LOSS : 0.6234, NSP_ACC : 0.6125, NSP_LOSS : 0.6131, \n",
      "BATCH_END - 431\n",
      "\n",
      "BATCH_BEGIN - 432\n",
      "Train Step: 433/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6114, MLM_LOSS : 0.6228, NSP_ACC : 0.6119, NSP_LOSS : 0.6125, \n",
      "BATCH_END - 432\n",
      "\n",
      "BATCH_BEGIN - 433\n",
      "Train Step: 434/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6116, MLM_LOSS : 0.6230, NSP_ACC : 0.6121, NSP_LOSS : 0.6127, \n",
      "BATCH_END - 433\n",
      "\n",
      "BATCH_BEGIN - 434\n",
      "Train Step: 435/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6124, MLM_LOSS : 0.6238, NSP_ACC : 0.6129, NSP_LOSS : 0.6135, \n",
      "BATCH_END - 434\n",
      "\n",
      "BATCH_BEGIN - 435\n",
      "Train Step: 436/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6132, MLM_LOSS : 0.6246, NSP_ACC : 0.6137, NSP_LOSS : 0.6143, \n",
      "BATCH_END - 435\n",
      "\n",
      "BATCH_BEGIN - 436\n",
      "Train Step: 437/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6131, MLM_LOSS : 0.6245, NSP_ACC : 0.6136, NSP_LOSS : 0.6142, \n",
      "BATCH_END - 436\n",
      "\n",
      "BATCH_BEGIN - 437\n",
      "Train Step: 438/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6130, MLM_LOSS : 0.6244, NSP_ACC : 0.6135, NSP_LOSS : 0.6141, \n",
      "BATCH_END - 437\n",
      "\n",
      "BATCH_BEGIN - 438\n",
      "Train Step: 439/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6130, MLM_LOSS : 0.6244, NSP_ACC : 0.6135, NSP_LOSS : 0.6141, \n",
      "BATCH_END - 438\n",
      "\n",
      "BATCH_BEGIN - 439\n",
      "Train Step: 440/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6129, MLM_LOSS : 0.6243, NSP_ACC : 0.6134, NSP_LOSS : 0.6140, \n",
      "BATCH_END - 439\n",
      "\n",
      "BATCH_BEGIN - 440\n",
      "Train Step: 441/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6135, MLM_LOSS : 0.6249, NSP_ACC : 0.6140, NSP_LOSS : 0.6146, \n",
      "BATCH_END - 440\n",
      "\n",
      "BATCH_BEGIN - 441\n",
      "Train Step: 442/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6143, MLM_LOSS : 0.6257, NSP_ACC : 0.6148, NSP_LOSS : 0.6154, \n",
      "BATCH_END - 441\n",
      "\n",
      "BATCH_BEGIN - 442\n",
      "Train Step: 443/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6150, MLM_LOSS : 0.6264, NSP_ACC : 0.6155, NSP_LOSS : 0.6161, \n",
      "BATCH_END - 442\n",
      "\n",
      "BATCH_BEGIN - 443\n",
      "Train Step: 444/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6157, MLM_LOSS : 0.6271, NSP_ACC : 0.6162, NSP_LOSS : 0.6168, \n",
      "BATCH_END - 443\n",
      "\n",
      "BATCH_BEGIN - 444\n",
      "Train Step: 445/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6162, MLM_LOSS : 0.6276, NSP_ACC : 0.6167, NSP_LOSS : 0.6173, \n",
      "BATCH_END - 444\n",
      "\n",
      "BATCH_BEGIN - 445\n",
      "Train Step: 446/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6165, MLM_LOSS : 0.6279, NSP_ACC : 0.6170, NSP_LOSS : 0.6176, \n",
      "BATCH_END - 445\n",
      "\n",
      "BATCH_BEGIN - 446\n",
      "Train Step: 447/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6160, MLM_LOSS : 0.6274, NSP_ACC : 0.6165, NSP_LOSS : 0.6171, \n",
      "BATCH_END - 446\n",
      "\n",
      "BATCH_BEGIN - 447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 448/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6159, MLM_LOSS : 0.6273, NSP_ACC : 0.6164, NSP_LOSS : 0.6170, \n",
      "BATCH_END - 447\n",
      "\n",
      "BATCH_BEGIN - 448\n",
      "Train Step: 449/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6158, MLM_LOSS : 0.6272, NSP_ACC : 0.6163, NSP_LOSS : 0.6169, \n",
      "BATCH_END - 448\n",
      "\n",
      "BATCH_BEGIN - 449\n",
      "Train Step: 450/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6162, MLM_LOSS : 0.6276, NSP_ACC : 0.6167, NSP_LOSS : 0.6173, \n",
      "18 EPOCH_END\n",
      "\n",
      "19 EPOCH BEGIN\n",
      "BATCH_BEGIN - 450\n",
      "Train Step: 451/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6169, MLM_LOSS : 0.6283, NSP_ACC : 0.6174, NSP_LOSS : 0.6180, \n",
      "BATCH_END - 450\n",
      "\n",
      "BATCH_BEGIN - 451\n",
      "Train Step: 452/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6172, MLM_LOSS : 0.6286, NSP_ACC : 0.6177, NSP_LOSS : 0.6183, \n",
      "BATCH_END - 451\n",
      "\n",
      "BATCH_BEGIN - 452\n",
      "Train Step: 453/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6170, MLM_LOSS : 0.6285, NSP_ACC : 0.6175, NSP_LOSS : 0.6181, \n",
      "BATCH_END - 452\n",
      "\n",
      "BATCH_BEGIN - 453\n",
      "Train Step: 454/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6162, MLM_LOSS : 0.6276, NSP_ACC : 0.6167, NSP_LOSS : 0.6173, \n",
      "BATCH_END - 453\n",
      "\n",
      "BATCH_BEGIN - 454\n",
      "Train Step: 455/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6155, MLM_LOSS : 0.6269, NSP_ACC : 0.6159, NSP_LOSS : 0.6165, \n",
      "BATCH_END - 454\n",
      "\n",
      "BATCH_BEGIN - 455\n",
      "Train Step: 456/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6149, MLM_LOSS : 0.6263, NSP_ACC : 0.6154, NSP_LOSS : 0.6160, \n",
      "BATCH_END - 455\n",
      "\n",
      "BATCH_BEGIN - 456\n",
      "Train Step: 457/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6141, MLM_LOSS : 0.6255, NSP_ACC : 0.6146, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 456\n",
      "\n",
      "BATCH_BEGIN - 457\n",
      "Train Step: 458/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6142, MLM_LOSS : 0.6256, NSP_ACC : 0.6146, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 457\n",
      "\n",
      "BATCH_BEGIN - 458\n",
      "Train Step: 459/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6142, MLM_LOSS : 0.6256, NSP_ACC : 0.6146, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 458\n",
      "\n",
      "BATCH_BEGIN - 459\n",
      "Train Step: 460/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6258, NSP_ACC : 0.6149, NSP_LOSS : 0.6155, \n",
      "BATCH_END - 459\n",
      "\n",
      "BATCH_BEGIN - 460\n",
      "Train Step: 461/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6138, MLM_LOSS : 0.6252, NSP_ACC : 0.6142, NSP_LOSS : 0.6148, \n",
      "BATCH_END - 460\n",
      "\n",
      "BATCH_BEGIN - 461\n",
      "Train Step: 462/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6137, MLM_LOSS : 0.6251, NSP_ACC : 0.6142, NSP_LOSS : 0.6147, \n",
      "BATCH_END - 461\n",
      "\n",
      "BATCH_BEGIN - 462\n",
      "Train Step: 463/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6133, MLM_LOSS : 0.6247, NSP_ACC : 0.6137, NSP_LOSS : 0.6143, \n",
      "BATCH_END - 462\n",
      "\n",
      "BATCH_BEGIN - 463\n",
      "Train Step: 464/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6131, MLM_LOSS : 0.6245, NSP_ACC : 0.6136, NSP_LOSS : 0.6142, \n",
      "BATCH_END - 463\n",
      "\n",
      "BATCH_BEGIN - 464\n",
      "Train Step: 465/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6134, MLM_LOSS : 0.6248, NSP_ACC : 0.6139, NSP_LOSS : 0.6145, \n",
      "BATCH_END - 464\n",
      "\n",
      "BATCH_BEGIN - 465\n",
      "Train Step: 466/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6136, MLM_LOSS : 0.6250, NSP_ACC : 0.6141, NSP_LOSS : 0.6147, \n",
      "BATCH_END - 465\n",
      "\n",
      "BATCH_BEGIN - 466\n",
      "Train Step: 467/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6136, MLM_LOSS : 0.6250, NSP_ACC : 0.6141, NSP_LOSS : 0.6147, \n",
      "BATCH_END - 466\n",
      "\n",
      "BATCH_BEGIN - 467\n",
      "Train Step: 468/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6130, MLM_LOSS : 0.6244, NSP_ACC : 0.6135, NSP_LOSS : 0.6141, \n",
      "BATCH_END - 467\n",
      "\n",
      "BATCH_BEGIN - 468\n",
      "Train Step: 469/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6124, MLM_LOSS : 0.6238, NSP_ACC : 0.6129, NSP_LOSS : 0.6135, \n",
      "BATCH_END - 468\n",
      "\n",
      "BATCH_BEGIN - 469\n",
      "Train Step: 470/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6125, MLM_LOSS : 0.6239, NSP_ACC : 0.6130, NSP_LOSS : 0.6136, \n",
      "BATCH_END - 469\n",
      "\n",
      "BATCH_BEGIN - 470\n",
      "Train Step: 471/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6117, MLM_LOSS : 0.6231, NSP_ACC : 0.6122, NSP_LOSS : 0.6128, \n",
      "BATCH_END - 470\n",
      "\n",
      "BATCH_BEGIN - 471\n",
      "Train Step: 472/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6117, MLM_LOSS : 0.6231, NSP_ACC : 0.6122, NSP_LOSS : 0.6128, \n",
      "BATCH_END - 471\n",
      "\n",
      "BATCH_BEGIN - 472\n",
      "Train Step: 473/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 472\n",
      "\n",
      "BATCH_BEGIN - 473\n",
      "Train Step: 474/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6114, MLM_LOSS : 0.6228, NSP_ACC : 0.6119, NSP_LOSS : 0.6125, \n",
      "BATCH_END - 473\n",
      "\n",
      "BATCH_BEGIN - 474\n",
      "Train Step: 475/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6104, MLM_LOSS : 0.6218, NSP_ACC : 0.6109, NSP_LOSS : 0.6115, \n",
      "19 EPOCH_END\n",
      "\n",
      "20 EPOCH BEGIN\n",
      "BATCH_BEGIN - 475\n",
      "Train Step: 476/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 475\n",
      "\n",
      "BATCH_BEGIN - 476\n",
      "Train Step: 477/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6117, MLM_LOSS : 0.6230, NSP_ACC : 0.6121, NSP_LOSS : 0.6128, \n",
      "BATCH_END - 476\n",
      "\n",
      "BATCH_BEGIN - 477\n",
      "Train Step: 478/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6235, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 477\n",
      "\n",
      "BATCH_BEGIN - 478\n",
      "Train Step: 479/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6124, MLM_LOSS : 0.6237, NSP_ACC : 0.6128, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 478\n",
      "\n",
      "BATCH_BEGIN - 479\n",
      "Train Step: 480/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6120, MLM_LOSS : 0.6233, NSP_ACC : 0.6124, NSP_LOSS : 0.6131, \n",
      "BATCH_END - 479\n",
      "\n",
      "BATCH_BEGIN - 480\n",
      "Train Step: 481/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6123, MLM_LOSS : 0.6237, NSP_ACC : 0.6127, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 480\n",
      "\n",
      "BATCH_BEGIN - 481\n",
      "Train Step: 482/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6112, MLM_LOSS : 0.6226, NSP_ACC : 0.6117, NSP_LOSS : 0.6123, \n",
      "BATCH_END - 481\n",
      "\n",
      "BATCH_BEGIN - 482\n",
      "Train Step: 483/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6115, MLM_LOSS : 0.6229, NSP_ACC : 0.6120, NSP_LOSS : 0.6126, \n",
      "BATCH_END - 482\n",
      "\n",
      "BATCH_BEGIN - 483\n",
      "Train Step: 484/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6112, MLM_LOSS : 0.6226, NSP_ACC : 0.6117, NSP_LOSS : 0.6123, \n",
      "BATCH_END - 483\n",
      "\n",
      "BATCH_BEGIN - 484\n",
      "Train Step: 485/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6107, MLM_LOSS : 0.6221, NSP_ACC : 0.6112, NSP_LOSS : 0.6118, \n",
      "BATCH_END - 484\n",
      "\n",
      "BATCH_BEGIN - 485\n",
      "Train Step: 486/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6100, MLM_LOSS : 0.6214, NSP_ACC : 0.6105, NSP_LOSS : 0.6111, \n",
      "BATCH_END - 485\n",
      "\n",
      "BATCH_BEGIN - 486\n",
      "Train Step: 487/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6102, MLM_LOSS : 0.6216, NSP_ACC : 0.6107, NSP_LOSS : 0.6113, \n",
      "BATCH_END - 486\n",
      "\n",
      "BATCH_BEGIN - 487\n",
      "Train Step: 488/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6105, MLM_LOSS : 0.6218, NSP_ACC : 0.6109, NSP_LOSS : 0.6116, \n",
      "BATCH_END - 487\n",
      "\n",
      "BATCH_BEGIN - 488\n",
      "Train Step: 489/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6105, MLM_LOSS : 0.6219, NSP_ACC : 0.6110, NSP_LOSS : 0.6116, \n",
      "BATCH_END - 488\n",
      "\n",
      "BATCH_BEGIN - 489\n",
      "Train Step: 490/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6112, MLM_LOSS : 0.6225, NSP_ACC : 0.6116, NSP_LOSS : 0.6123, \n",
      "BATCH_END - 489\n",
      "\n",
      "BATCH_BEGIN - 490\n",
      "Train Step: 491/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6115, MLM_LOSS : 0.6229, NSP_ACC : 0.6120, NSP_LOSS : 0.6126, \n",
      "BATCH_END - 490\n",
      "\n",
      "BATCH_BEGIN - 491\n",
      "Train Step: 492/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6118, MLM_LOSS : 0.6232, NSP_ACC : 0.6123, NSP_LOSS : 0.6129, \n",
      "BATCH_END - 491\n",
      "\n",
      "BATCH_BEGIN - 492\n",
      "Train Step: 493/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 492\n",
      "\n",
      "BATCH_BEGIN - 493\n",
      "Train Step: 494/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6111, MLM_LOSS : 0.6225, NSP_ACC : 0.6116, NSP_LOSS : 0.6122, \n",
      "BATCH_END - 493\n",
      "\n",
      "BATCH_BEGIN - 494\n",
      "Train Step: 495/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6110, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6121, \n",
      "BATCH_END - 494\n",
      "\n",
      "BATCH_BEGIN - 495\n",
      "Train Step: 496/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6107, MLM_LOSS : 0.6220, NSP_ACC : 0.6111, NSP_LOSS : 0.6118, \n",
      "BATCH_END - 495\n",
      "\n",
      "BATCH_BEGIN - 496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 497/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6100, MLM_LOSS : 0.6214, NSP_ACC : 0.6105, NSP_LOSS : 0.6111, \n",
      "BATCH_END - 496\n",
      "\n",
      "BATCH_BEGIN - 497\n",
      "Train Step: 498/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6095, MLM_LOSS : 0.6209, NSP_ACC : 0.6099, NSP_LOSS : 0.6106, \n",
      "BATCH_END - 497\n",
      "\n",
      "BATCH_BEGIN - 498\n",
      "Train Step: 499/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6097, MLM_LOSS : 0.6211, NSP_ACC : 0.6102, NSP_LOSS : 0.6109, \n",
      "BATCH_END - 498\n",
      "\n",
      "BATCH_BEGIN - 499\n",
      "Train Step: 500/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6105, MLM_LOSS : 0.6218, NSP_ACC : 0.6109, NSP_LOSS : 0.6116, \n",
      "20 EPOCH_END\n",
      "\n",
      "21 EPOCH BEGIN\n",
      "BATCH_BEGIN - 500\n",
      "Train Step: 501/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6111, MLM_LOSS : 0.6224, NSP_ACC : 0.6115, NSP_LOSS : 0.6122, \n",
      "BATCH_END - 500\n",
      "\n",
      "BATCH_BEGIN - 501\n",
      "Train Step: 502/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6112, MLM_LOSS : 0.6226, NSP_ACC : 0.6117, NSP_LOSS : 0.6123, \n",
      "BATCH_END - 501\n",
      "\n",
      "BATCH_BEGIN - 502\n",
      "Train Step: 503/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6106, MLM_LOSS : 0.6219, NSP_ACC : 0.6110, NSP_LOSS : 0.6117, \n",
      "BATCH_END - 502\n",
      "\n",
      "BATCH_BEGIN - 503\n",
      "Train Step: 504/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6107, MLM_LOSS : 0.6221, NSP_ACC : 0.6112, NSP_LOSS : 0.6118, \n",
      "BATCH_END - 503\n",
      "\n",
      "BATCH_BEGIN - 504\n",
      "Train Step: 505/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6114, MLM_LOSS : 0.6228, NSP_ACC : 0.6119, NSP_LOSS : 0.6125, \n",
      "BATCH_END - 504\n",
      "\n",
      "BATCH_BEGIN - 505\n",
      "Train Step: 506/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6113, MLM_LOSS : 0.6227, NSP_ACC : 0.6118, NSP_LOSS : 0.6124, \n",
      "BATCH_END - 505\n",
      "\n",
      "BATCH_BEGIN - 506\n",
      "Train Step: 507/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6120, MLM_LOSS : 0.6234, NSP_ACC : 0.6125, NSP_LOSS : 0.6131, \n",
      "BATCH_END - 506\n",
      "\n",
      "BATCH_BEGIN - 507\n",
      "Train Step: 508/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6118, MLM_LOSS : 0.6232, NSP_ACC : 0.6123, NSP_LOSS : 0.6129, \n",
      "BATCH_END - 507\n",
      "\n",
      "BATCH_BEGIN - 508\n",
      "Train Step: 509/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6123, MLM_LOSS : 0.6237, NSP_ACC : 0.6128, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 508\n",
      "\n",
      "BATCH_BEGIN - 509\n",
      "Train Step: 510/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6234, NSP_ACC : 0.6125, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 509\n",
      "\n",
      "BATCH_BEGIN - 510\n",
      "Train Step: 511/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6127, MLM_LOSS : 0.6241, NSP_ACC : 0.6132, NSP_LOSS : 0.6138, \n",
      "BATCH_END - 510\n",
      "\n",
      "BATCH_BEGIN - 511\n",
      "Train Step: 512/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6126, MLM_LOSS : 0.6240, NSP_ACC : 0.6131, NSP_LOSS : 0.6137, \n",
      "BATCH_END - 511\n",
      "\n",
      "BATCH_BEGIN - 512\n",
      "Train Step: 513/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6126, MLM_LOSS : 0.6240, NSP_ACC : 0.6131, NSP_LOSS : 0.6137, \n",
      "BATCH_END - 512\n",
      "\n",
      "BATCH_BEGIN - 513\n",
      "Train Step: 514/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6132, MLM_LOSS : 0.6246, NSP_ACC : 0.6137, NSP_LOSS : 0.6143, \n",
      "BATCH_END - 513\n",
      "\n",
      "BATCH_BEGIN - 514\n",
      "Train Step: 515/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6135, MLM_LOSS : 0.6249, NSP_ACC : 0.6140, NSP_LOSS : 0.6146, \n",
      "BATCH_END - 514\n",
      "\n",
      "BATCH_BEGIN - 515\n",
      "Train Step: 516/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6134, MLM_LOSS : 0.6248, NSP_ACC : 0.6139, NSP_LOSS : 0.6145, \n",
      "BATCH_END - 515\n",
      "\n",
      "BATCH_BEGIN - 516\n",
      "Train Step: 517/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6140, MLM_LOSS : 0.6254, NSP_ACC : 0.6145, NSP_LOSS : 0.6151, \n",
      "BATCH_END - 516\n",
      "\n",
      "BATCH_BEGIN - 517\n",
      "Train Step: 518/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6142, MLM_LOSS : 0.6256, NSP_ACC : 0.6147, NSP_LOSS : 0.6153, \n",
      "BATCH_END - 517\n",
      "\n",
      "BATCH_BEGIN - 518\n",
      "Train Step: 519/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6258, NSP_ACC : 0.6149, NSP_LOSS : 0.6155, \n",
      "BATCH_END - 518\n",
      "\n",
      "BATCH_BEGIN - 519\n",
      "Train Step: 520/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6137, MLM_LOSS : 0.6251, NSP_ACC : 0.6142, NSP_LOSS : 0.6148, \n",
      "BATCH_END - 519\n",
      "\n",
      "BATCH_BEGIN - 520\n",
      "Train Step: 521/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6258, NSP_ACC : 0.6149, NSP_LOSS : 0.6155, \n",
      "BATCH_END - 520\n",
      "\n",
      "BATCH_BEGIN - 521\n",
      "Train Step: 522/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6143, MLM_LOSS : 0.6257, NSP_ACC : 0.6148, NSP_LOSS : 0.6154, \n",
      "BATCH_END - 521\n",
      "\n",
      "BATCH_BEGIN - 522\n",
      "Train Step: 523/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6135, MLM_LOSS : 0.6249, NSP_ACC : 0.6140, NSP_LOSS : 0.6146, \n",
      "BATCH_END - 522\n",
      "\n",
      "BATCH_BEGIN - 523\n",
      "Train Step: 524/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6141, MLM_LOSS : 0.6255, NSP_ACC : 0.6146, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 523\n",
      "\n",
      "BATCH_BEGIN - 524\n",
      "Train Step: 525/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6133, MLM_LOSS : 0.6247, NSP_ACC : 0.6138, NSP_LOSS : 0.6144, \n",
      "21 EPOCH_END\n",
      "\n",
      "22 EPOCH BEGIN\n",
      "BATCH_BEGIN - 525\n",
      "Train Step: 526/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6129, MLM_LOSS : 0.6243, NSP_ACC : 0.6134, NSP_LOSS : 0.6140, \n",
      "BATCH_END - 525\n",
      "\n",
      "BATCH_BEGIN - 526\n",
      "Train Step: 527/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6124, MLM_LOSS : 0.6238, NSP_ACC : 0.6129, NSP_LOSS : 0.6135, \n",
      "BATCH_END - 526\n",
      "\n",
      "BATCH_BEGIN - 527\n",
      "Train Step: 528/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6128, MLM_LOSS : 0.6241, NSP_ACC : 0.6132, NSP_LOSS : 0.6139, \n",
      "BATCH_END - 527\n",
      "\n",
      "BATCH_BEGIN - 528\n",
      "Train Step: 529/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6123, MLM_LOSS : 0.6237, NSP_ACC : 0.6128, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 528\n",
      "\n",
      "BATCH_BEGIN - 529\n",
      "Train Step: 530/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6129, MLM_LOSS : 0.6243, NSP_ACC : 0.6134, NSP_LOSS : 0.6140, \n",
      "BATCH_END - 529\n",
      "\n",
      "BATCH_BEGIN - 530\n",
      "Train Step: 531/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6128, MLM_LOSS : 0.6242, NSP_ACC : 0.6133, NSP_LOSS : 0.6139, \n",
      "BATCH_END - 530\n",
      "\n",
      "BATCH_BEGIN - 531\n",
      "Train Step: 532/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6126, MLM_LOSS : 0.6240, NSP_ACC : 0.6131, NSP_LOSS : 0.6137, \n",
      "BATCH_END - 531\n",
      "\n",
      "BATCH_BEGIN - 532\n",
      "Train Step: 533/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6127, MLM_LOSS : 0.6241, NSP_ACC : 0.6132, NSP_LOSS : 0.6138, \n",
      "BATCH_END - 532\n",
      "\n",
      "BATCH_BEGIN - 533\n",
      "Train Step: 534/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6123, MLM_LOSS : 0.6237, NSP_ACC : 0.6128, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 533\n",
      "\n",
      "BATCH_BEGIN - 534\n",
      "Train Step: 535/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6119, MLM_LOSS : 0.6233, NSP_ACC : 0.6123, NSP_LOSS : 0.6130, \n",
      "BATCH_END - 534\n",
      "\n",
      "BATCH_BEGIN - 535\n",
      "Train Step: 536/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6123, MLM_LOSS : 0.6237, NSP_ACC : 0.6128, NSP_LOSS : 0.6134, \n",
      "BATCH_END - 535\n",
      "\n",
      "BATCH_BEGIN - 536\n",
      "Train Step: 537/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6124, MLM_LOSS : 0.6237, NSP_ACC : 0.6128, NSP_LOSS : 0.6135, \n",
      "BATCH_END - 536\n",
      "\n",
      "BATCH_BEGIN - 537\n",
      "Train Step: 538/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6116, MLM_LOSS : 0.6230, NSP_ACC : 0.6121, NSP_LOSS : 0.6127, \n",
      "BATCH_END - 537\n",
      "\n",
      "BATCH_BEGIN - 538\n",
      "Train Step: 539/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6116, MLM_LOSS : 0.6230, NSP_ACC : 0.6121, NSP_LOSS : 0.6127, \n",
      "BATCH_END - 538\n",
      "\n",
      "BATCH_BEGIN - 539\n",
      "Train Step: 540/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6235, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 539\n",
      "\n",
      "BATCH_BEGIN - 540\n",
      "Train Step: 541/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6122, MLM_LOSS : 0.6235, NSP_ACC : 0.6126, NSP_LOSS : 0.6133, \n",
      "BATCH_END - 540\n",
      "\n",
      "BATCH_BEGIN - 541\n",
      "Train Step: 542/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6235, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 541\n",
      "\n",
      "BATCH_BEGIN - 542\n",
      "Train Step: 543/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6235, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 542\n",
      "\n",
      "BATCH_BEGIN - 543\n",
      "Train Step: 544/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6117, MLM_LOSS : 0.6231, NSP_ACC : 0.6122, NSP_LOSS : 0.6128, \n",
      "BATCH_END - 543\n",
      "\n",
      "BATCH_BEGIN - 544\n",
      "Train Step: 545/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6117, MLM_LOSS : 0.6231, NSP_ACC : 0.6122, NSP_LOSS : 0.6128, \n",
      "BATCH_END - 544\n",
      "\n",
      "BATCH_BEGIN - 545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 546/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6119, MLM_LOSS : 0.6233, NSP_ACC : 0.6124, NSP_LOSS : 0.6130, \n",
      "BATCH_END - 545\n",
      "\n",
      "BATCH_BEGIN - 546\n",
      "Train Step: 547/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6235, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 546\n",
      "\n",
      "BATCH_BEGIN - 547\n",
      "Train Step: 548/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6120, MLM_LOSS : 0.6234, NSP_ACC : 0.6125, NSP_LOSS : 0.6131, \n",
      "BATCH_END - 547\n",
      "\n",
      "BATCH_BEGIN - 548\n",
      "Train Step: 549/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6125, MLM_LOSS : 0.6239, NSP_ACC : 0.6130, NSP_LOSS : 0.6136, \n",
      "BATCH_END - 548\n",
      "\n",
      "BATCH_BEGIN - 549\n",
      "Train Step: 550/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6127, MLM_LOSS : 0.6241, NSP_ACC : 0.6132, NSP_LOSS : 0.6138, \n",
      "22 EPOCH_END\n",
      "\n",
      "23 EPOCH BEGIN\n",
      "BATCH_BEGIN - 550\n",
      "Train Step: 551/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6127, MLM_LOSS : 0.6241, NSP_ACC : 0.6132, NSP_LOSS : 0.6138, \n",
      "BATCH_END - 550\n",
      "\n",
      "BATCH_BEGIN - 551\n",
      "Train Step: 552/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6125, MLM_LOSS : 0.6239, NSP_ACC : 0.6130, NSP_LOSS : 0.6136, \n",
      "BATCH_END - 551\n",
      "\n",
      "BATCH_BEGIN - 552\n",
      "Train Step: 553/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6131, MLM_LOSS : 0.6245, NSP_ACC : 0.6136, NSP_LOSS : 0.6142, \n",
      "BATCH_END - 552\n",
      "\n",
      "BATCH_BEGIN - 553\n",
      "Train Step: 554/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6126, MLM_LOSS : 0.6240, NSP_ACC : 0.6131, NSP_LOSS : 0.6137, \n",
      "BATCH_END - 553\n",
      "\n",
      "BATCH_BEGIN - 554\n",
      "Train Step: 555/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6129, MLM_LOSS : 0.6243, NSP_ACC : 0.6134, NSP_LOSS : 0.6140, \n",
      "BATCH_END - 554\n",
      "\n",
      "BATCH_BEGIN - 555\n",
      "Train Step: 556/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6130, MLM_LOSS : 0.6244, NSP_ACC : 0.6135, NSP_LOSS : 0.6141, \n",
      "BATCH_END - 555\n",
      "\n",
      "BATCH_BEGIN - 556\n",
      "Train Step: 557/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6130, MLM_LOSS : 0.6244, NSP_ACC : 0.6135, NSP_LOSS : 0.6141, \n",
      "BATCH_END - 556\n",
      "\n",
      "BATCH_BEGIN - 557\n",
      "Train Step: 558/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6125, MLM_LOSS : 0.6239, NSP_ACC : 0.6130, NSP_LOSS : 0.6136, \n",
      "BATCH_END - 557\n",
      "\n",
      "BATCH_BEGIN - 558\n",
      "Train Step: 559/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6120, MLM_LOSS : 0.6234, NSP_ACC : 0.6125, NSP_LOSS : 0.6131, \n",
      "BATCH_END - 558\n",
      "\n",
      "BATCH_BEGIN - 559\n",
      "Train Step: 560/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6114, MLM_LOSS : 0.6228, NSP_ACC : 0.6119, NSP_LOSS : 0.6125, \n",
      "BATCH_END - 559\n",
      "\n",
      "BATCH_BEGIN - 560\n",
      "Train Step: 561/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6117, MLM_LOSS : 0.6231, NSP_ACC : 0.6122, NSP_LOSS : 0.6128, \n",
      "BATCH_END - 560\n",
      "\n",
      "BATCH_BEGIN - 561\n",
      "Train Step: 562/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6114, MLM_LOSS : 0.6228, NSP_ACC : 0.6119, NSP_LOSS : 0.6125, \n",
      "BATCH_END - 561\n",
      "\n",
      "BATCH_BEGIN - 562\n",
      "Train Step: 563/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6121, MLM_LOSS : 0.6234, NSP_ACC : 0.6126, NSP_LOSS : 0.6132, \n",
      "BATCH_END - 562\n",
      "\n",
      "BATCH_BEGIN - 563\n",
      "Train Step: 564/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6120, MLM_LOSS : 0.6234, NSP_ACC : 0.6125, NSP_LOSS : 0.6131, \n",
      "BATCH_END - 563\n",
      "\n",
      "BATCH_BEGIN - 564\n",
      "Train Step: 565/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6120, MLM_LOSS : 0.6233, NSP_ACC : 0.6125, NSP_LOSS : 0.6131, \n",
      "BATCH_END - 564\n",
      "\n",
      "BATCH_BEGIN - 565\n",
      "Train Step: 566/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6125, MLM_LOSS : 0.6238, NSP_ACC : 0.6130, NSP_LOSS : 0.6136, \n",
      "BATCH_END - 565\n",
      "\n",
      "BATCH_BEGIN - 566\n",
      "Train Step: 567/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6127, MLM_LOSS : 0.6240, NSP_ACC : 0.6132, NSP_LOSS : 0.6138, \n",
      "BATCH_END - 566\n",
      "\n",
      "BATCH_BEGIN - 567\n",
      "Train Step: 568/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6132, MLM_LOSS : 0.6246, NSP_ACC : 0.6137, NSP_LOSS : 0.6143, \n",
      "BATCH_END - 567\n",
      "\n",
      "BATCH_BEGIN - 568\n",
      "Train Step: 569/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6132, MLM_LOSS : 0.6246, NSP_ACC : 0.6137, NSP_LOSS : 0.6143, \n",
      "BATCH_END - 568\n",
      "\n",
      "BATCH_BEGIN - 569\n",
      "Train Step: 570/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6132, MLM_LOSS : 0.6246, NSP_ACC : 0.6137, NSP_LOSS : 0.6143, \n",
      "BATCH_END - 569\n",
      "\n",
      "BATCH_BEGIN - 570\n",
      "Train Step: 571/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6131, MLM_LOSS : 0.6245, NSP_ACC : 0.6136, NSP_LOSS : 0.6142, \n",
      "BATCH_END - 570\n",
      "\n",
      "BATCH_BEGIN - 571\n",
      "Train Step: 572/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6131, MLM_LOSS : 0.6245, NSP_ACC : 0.6136, NSP_LOSS : 0.6142, \n",
      "BATCH_END - 571\n",
      "\n",
      "BATCH_BEGIN - 572\n",
      "Train Step: 573/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6131, MLM_LOSS : 0.6245, NSP_ACC : 0.6136, NSP_LOSS : 0.6142, \n",
      "BATCH_END - 572\n",
      "\n",
      "BATCH_BEGIN - 573\n",
      "Train Step: 574/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6130, MLM_LOSS : 0.6244, NSP_ACC : 0.6135, NSP_LOSS : 0.6141, \n",
      "BATCH_END - 573\n",
      "\n",
      "BATCH_BEGIN - 574\n",
      "Train Step: 575/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6133, MLM_LOSS : 0.6247, NSP_ACC : 0.6138, NSP_LOSS : 0.6144, \n",
      "23 EPOCH_END\n",
      "\n",
      "24 EPOCH BEGIN\n",
      "BATCH_BEGIN - 575\n",
      "Train Step: 576/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6138, MLM_LOSS : 0.6252, NSP_ACC : 0.6143, NSP_LOSS : 0.6149, \n",
      "BATCH_END - 575\n",
      "\n",
      "BATCH_BEGIN - 576\n",
      "Train Step: 577/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6133, MLM_LOSS : 0.6247, NSP_ACC : 0.6138, NSP_LOSS : 0.6144, \n",
      "BATCH_END - 576\n",
      "\n",
      "BATCH_BEGIN - 577\n",
      "Train Step: 578/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6129, MLM_LOSS : 0.6243, NSP_ACC : 0.6134, NSP_LOSS : 0.6140, \n",
      "BATCH_END - 577\n",
      "\n",
      "BATCH_BEGIN - 578\n",
      "Train Step: 579/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6135, MLM_LOSS : 0.6249, NSP_ACC : 0.6140, NSP_LOSS : 0.6146, \n",
      "BATCH_END - 578\n",
      "\n",
      "BATCH_BEGIN - 579\n",
      "Train Step: 580/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6132, MLM_LOSS : 0.6246, NSP_ACC : 0.6138, NSP_LOSS : 0.6143, \n",
      "BATCH_END - 579\n",
      "\n",
      "BATCH_BEGIN - 580\n",
      "Train Step: 581/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6135, MLM_LOSS : 0.6249, NSP_ACC : 0.6140, NSP_LOSS : 0.6146, \n",
      "BATCH_END - 580\n",
      "\n",
      "BATCH_BEGIN - 581\n",
      "Train Step: 582/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6136, MLM_LOSS : 0.6250, NSP_ACC : 0.6141, NSP_LOSS : 0.6147, \n",
      "BATCH_END - 581\n",
      "\n",
      "BATCH_BEGIN - 582\n",
      "Train Step: 583/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6141, MLM_LOSS : 0.6255, NSP_ACC : 0.6147, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 582\n",
      "\n",
      "BATCH_BEGIN - 583\n",
      "Train Step: 584/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6142, MLM_LOSS : 0.6256, NSP_ACC : 0.6147, NSP_LOSS : 0.6153, \n",
      "BATCH_END - 583\n",
      "\n",
      "BATCH_BEGIN - 584\n",
      "Train Step: 585/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6147, MLM_LOSS : 0.6261, NSP_ACC : 0.6152, NSP_LOSS : 0.6158, \n",
      "BATCH_END - 584\n",
      "\n",
      "BATCH_BEGIN - 585\n",
      "Train Step: 586/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6143, MLM_LOSS : 0.6257, NSP_ACC : 0.6148, NSP_LOSS : 0.6154, \n",
      "BATCH_END - 585\n",
      "\n",
      "BATCH_BEGIN - 586\n",
      "Train Step: 587/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6145, MLM_LOSS : 0.6259, NSP_ACC : 0.6150, NSP_LOSS : 0.6156, \n",
      "BATCH_END - 586\n",
      "\n",
      "BATCH_BEGIN - 587\n",
      "Train Step: 588/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6258, NSP_ACC : 0.6149, NSP_LOSS : 0.6156, \n",
      "BATCH_END - 587\n",
      "\n",
      "BATCH_BEGIN - 588\n",
      "Train Step: 589/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6139, MLM_LOSS : 0.6253, NSP_ACC : 0.6144, NSP_LOSS : 0.6150, \n",
      "BATCH_END - 588\n",
      "\n",
      "BATCH_BEGIN - 589\n",
      "Train Step: 590/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6139, MLM_LOSS : 0.6252, NSP_ACC : 0.6144, NSP_LOSS : 0.6150, \n",
      "BATCH_END - 589\n",
      "\n",
      "BATCH_BEGIN - 590\n",
      "Train Step: 591/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6140, MLM_LOSS : 0.6253, NSP_ACC : 0.6145, NSP_LOSS : 0.6151, \n",
      "BATCH_END - 590\n",
      "\n",
      "BATCH_BEGIN - 591\n",
      "Train Step: 592/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6258, NSP_ACC : 0.6149, NSP_LOSS : 0.6155, \n",
      "BATCH_END - 591\n",
      "\n",
      "BATCH_BEGIN - 592\n",
      "Train Step: 593/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6149, MLM_LOSS : 0.6262, NSP_ACC : 0.6153, NSP_LOSS : 0.6160, \n",
      "BATCH_END - 592\n",
      "\n",
      "BATCH_BEGIN - 593\n",
      "Train Step: 594/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6147, MLM_LOSS : 0.6261, NSP_ACC : 0.6152, NSP_LOSS : 0.6158, \n",
      "BATCH_END - 593\n",
      "\n",
      "BATCH_BEGIN - 594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 595/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6147, MLM_LOSS : 0.6261, NSP_ACC : 0.6152, NSP_LOSS : 0.6158, \n",
      "BATCH_END - 594\n",
      "\n",
      "BATCH_BEGIN - 595\n",
      "Train Step: 596/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6146, MLM_LOSS : 0.6260, NSP_ACC : 0.6151, NSP_LOSS : 0.6157, \n",
      "BATCH_END - 595\n",
      "\n",
      "BATCH_BEGIN - 596\n",
      "Train Step: 597/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6146, MLM_LOSS : 0.6260, NSP_ACC : 0.6151, NSP_LOSS : 0.6157, \n",
      "BATCH_END - 596\n",
      "\n",
      "BATCH_BEGIN - 597\n",
      "Train Step: 598/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6145, MLM_LOSS : 0.6259, NSP_ACC : 0.6150, NSP_LOSS : 0.6156, \n",
      "BATCH_END - 597\n",
      "\n",
      "BATCH_BEGIN - 598\n",
      "Train Step: 599/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6147, MLM_LOSS : 0.6261, NSP_ACC : 0.6152, NSP_LOSS : 0.6158, \n",
      "BATCH_END - 598\n",
      "\n",
      "BATCH_BEGIN - 599\n",
      "Train Step: 600/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6145, MLM_LOSS : 0.6259, NSP_ACC : 0.6150, NSP_LOSS : 0.6156, \n",
      "24 EPOCH_END\n",
      "\n",
      "25 EPOCH BEGIN\n",
      "BATCH_BEGIN - 600\n",
      "Train Step: 601/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6147, MLM_LOSS : 0.6261, NSP_ACC : 0.6152, NSP_LOSS : 0.6158, \n",
      "BATCH_END - 600\n",
      "\n",
      "BATCH_BEGIN - 601\n",
      "Train Step: 602/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6148, MLM_LOSS : 0.6262, NSP_ACC : 0.6153, NSP_LOSS : 0.6159, \n",
      "BATCH_END - 601\n",
      "\n",
      "BATCH_BEGIN - 602\n",
      "Train Step: 603/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6258, NSP_ACC : 0.6149, NSP_LOSS : 0.6155, \n",
      "BATCH_END - 602\n",
      "\n",
      "BATCH_BEGIN - 603\n",
      "Train Step: 604/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6150, MLM_LOSS : 0.6263, NSP_ACC : 0.6155, NSP_LOSS : 0.6161, \n",
      "BATCH_END - 603\n",
      "\n",
      "BATCH_BEGIN - 604\n",
      "Train Step: 605/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6150, MLM_LOSS : 0.6264, NSP_ACC : 0.6155, NSP_LOSS : 0.6161, \n",
      "BATCH_END - 604\n",
      "\n",
      "BATCH_BEGIN - 605\n",
      "Train Step: 606/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6148, MLM_LOSS : 0.6262, NSP_ACC : 0.6153, NSP_LOSS : 0.6159, \n",
      "BATCH_END - 605\n",
      "\n",
      "BATCH_BEGIN - 606\n",
      "Train Step: 607/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6147, MLM_LOSS : 0.6260, NSP_ACC : 0.6152, NSP_LOSS : 0.6158, \n",
      "BATCH_END - 606\n",
      "\n",
      "BATCH_BEGIN - 607\n",
      "Train Step: 608/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6150, MLM_LOSS : 0.6264, NSP_ACC : 0.6155, NSP_LOSS : 0.6161, \n",
      "BATCH_END - 607\n",
      "\n",
      "BATCH_BEGIN - 608\n",
      "Train Step: 609/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6152, MLM_LOSS : 0.6266, NSP_ACC : 0.6157, NSP_LOSS : 0.6163, \n",
      "BATCH_END - 608\n",
      "\n",
      "BATCH_BEGIN - 609\n",
      "Train Step: 610/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6151, MLM_LOSS : 0.6265, NSP_ACC : 0.6156, NSP_LOSS : 0.6162, \n",
      "BATCH_END - 609\n",
      "\n",
      "BATCH_BEGIN - 610\n",
      "Train Step: 611/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6155, MLM_LOSS : 0.6269, NSP_ACC : 0.6161, NSP_LOSS : 0.6166, \n",
      "BATCH_END - 610\n",
      "\n",
      "BATCH_BEGIN - 611\n",
      "Train Step: 612/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6159, MLM_LOSS : 0.6272, NSP_ACC : 0.6164, NSP_LOSS : 0.6170, \n",
      "BATCH_END - 611\n",
      "\n",
      "BATCH_BEGIN - 612\n",
      "Train Step: 613/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6164, MLM_LOSS : 0.6278, NSP_ACC : 0.6169, NSP_LOSS : 0.6175, \n",
      "BATCH_END - 612\n",
      "\n",
      "BATCH_BEGIN - 613\n",
      "Train Step: 614/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6163, MLM_LOSS : 0.6276, NSP_ACC : 0.6168, NSP_LOSS : 0.6173, \n",
      "BATCH_END - 613\n",
      "\n",
      "BATCH_BEGIN - 614\n",
      "Train Step: 615/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6164, MLM_LOSS : 0.6277, NSP_ACC : 0.6169, NSP_LOSS : 0.6175, \n",
      "BATCH_END - 614\n",
      "\n",
      "BATCH_BEGIN - 615\n",
      "Train Step: 616/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6163, MLM_LOSS : 0.6277, NSP_ACC : 0.6168, NSP_LOSS : 0.6174, \n",
      "BATCH_END - 615\n",
      "\n",
      "BATCH_BEGIN - 616\n",
      "Train Step: 617/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6156, MLM_LOSS : 0.6270, NSP_ACC : 0.6161, NSP_LOSS : 0.6167, \n",
      "BATCH_END - 616\n",
      "\n",
      "BATCH_BEGIN - 617\n",
      "Train Step: 618/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6161, MLM_LOSS : 0.6275, NSP_ACC : 0.6166, NSP_LOSS : 0.6172, \n",
      "BATCH_END - 617\n",
      "\n",
      "BATCH_BEGIN - 618\n",
      "Train Step: 619/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6165, MLM_LOSS : 0.6278, NSP_ACC : 0.6170, NSP_LOSS : 0.6176, \n",
      "BATCH_END - 618\n",
      "\n",
      "BATCH_BEGIN - 619\n",
      "Train Step: 620/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6167, MLM_LOSS : 0.6281, NSP_ACC : 0.6172, NSP_LOSS : 0.6178, \n",
      "BATCH_END - 619\n",
      "\n",
      "BATCH_BEGIN - 620\n",
      "Train Step: 621/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6172, MLM_LOSS : 0.6286, NSP_ACC : 0.6177, NSP_LOSS : 0.6183, \n",
      "BATCH_END - 620\n",
      "\n",
      "BATCH_BEGIN - 621\n",
      "Train Step: 622/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6173, MLM_LOSS : 0.6287, NSP_ACC : 0.6178, NSP_LOSS : 0.6184, \n",
      "BATCH_END - 621\n",
      "\n",
      "BATCH_BEGIN - 622\n",
      "Train Step: 623/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6172, MLM_LOSS : 0.6286, NSP_ACC : 0.6177, NSP_LOSS : 0.6183, \n",
      "BATCH_END - 622\n",
      "\n",
      "BATCH_BEGIN - 623\n",
      "Train Step: 624/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6177, MLM_LOSS : 0.6291, NSP_ACC : 0.6182, NSP_LOSS : 0.6188, \n",
      "BATCH_END - 623\n",
      "\n",
      "BATCH_BEGIN - 624\n",
      "Train Step: 625/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6173, MLM_LOSS : 0.6287, NSP_ACC : 0.6178, NSP_LOSS : 0.6184, \n",
      "25 EPOCH_END\n",
      "\n",
      "26 EPOCH BEGIN\n",
      "BATCH_BEGIN - 625\n",
      "Train Step: 626/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6177, MLM_LOSS : 0.6290, NSP_ACC : 0.6182, NSP_LOSS : 0.6188, \n",
      "BATCH_END - 625\n",
      "\n",
      "BATCH_BEGIN - 626\n",
      "Train Step: 627/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6179, MLM_LOSS : 0.6292, NSP_ACC : 0.6184, NSP_LOSS : 0.6190, \n",
      "BATCH_END - 626\n",
      "\n",
      "BATCH_BEGIN - 627\n",
      "Train Step: 628/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6185, MLM_LOSS : 0.6298, NSP_ACC : 0.6190, NSP_LOSS : 0.6196, \n",
      "BATCH_END - 627\n",
      "\n",
      "BATCH_BEGIN - 628\n",
      "Train Step: 629/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6182, MLM_LOSS : 0.6296, NSP_ACC : 0.6187, NSP_LOSS : 0.6193, \n",
      "BATCH_END - 628\n",
      "\n",
      "BATCH_BEGIN - 629\n",
      "Train Step: 630/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6183, MLM_LOSS : 0.6296, NSP_ACC : 0.6188, NSP_LOSS : 0.6194, \n",
      "BATCH_END - 629\n",
      "\n",
      "BATCH_BEGIN - 630\n",
      "Train Step: 631/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6183, MLM_LOSS : 0.6297, NSP_ACC : 0.6188, NSP_LOSS : 0.6194, \n",
      "BATCH_END - 630\n",
      "\n",
      "BATCH_BEGIN - 631\n",
      "Train Step: 632/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6188, MLM_LOSS : 0.6302, NSP_ACC : 0.6193, NSP_LOSS : 0.6199, \n",
      "BATCH_END - 631\n",
      "\n",
      "BATCH_BEGIN - 632\n",
      "Train Step: 633/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6187, MLM_LOSS : 0.6301, NSP_ACC : 0.6192, NSP_LOSS : 0.6198, \n",
      "BATCH_END - 632\n",
      "\n",
      "BATCH_BEGIN - 633\n",
      "Train Step: 634/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6181, MLM_LOSS : 0.6295, NSP_ACC : 0.6186, NSP_LOSS : 0.6192, \n",
      "BATCH_END - 633\n",
      "\n",
      "BATCH_BEGIN - 634\n",
      "Train Step: 635/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6180, MLM_LOSS : 0.6294, NSP_ACC : 0.6185, NSP_LOSS : 0.6192, \n",
      "BATCH_END - 634\n",
      "\n",
      "BATCH_BEGIN - 635\n",
      "Train Step: 636/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6183, MLM_LOSS : 0.6297, NSP_ACC : 0.6189, NSP_LOSS : 0.6195, \n",
      "BATCH_END - 635\n",
      "\n",
      "BATCH_BEGIN - 636\n",
      "Train Step: 637/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6184, MLM_LOSS : 0.6297, NSP_ACC : 0.6189, NSP_LOSS : 0.6195, \n",
      "BATCH_END - 636\n",
      "\n",
      "BATCH_BEGIN - 637\n",
      "Train Step: 638/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6183, MLM_LOSS : 0.6297, NSP_ACC : 0.6188, NSP_LOSS : 0.6194, \n",
      "BATCH_END - 637\n",
      "\n",
      "BATCH_BEGIN - 638\n",
      "Train Step: 639/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6180, MLM_LOSS : 0.6293, NSP_ACC : 0.6185, NSP_LOSS : 0.6191, \n",
      "BATCH_END - 638\n",
      "\n",
      "BATCH_BEGIN - 639\n",
      "Train Step: 640/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6185, MLM_LOSS : 0.6298, NSP_ACC : 0.6190, NSP_LOSS : 0.6196, \n",
      "BATCH_END - 639\n",
      "\n",
      "BATCH_BEGIN - 640\n",
      "Train Step: 641/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6182, MLM_LOSS : 0.6296, NSP_ACC : 0.6187, NSP_LOSS : 0.6194, \n",
      "BATCH_END - 640\n",
      "\n",
      "BATCH_BEGIN - 641\n",
      "Train Step: 642/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6179, MLM_LOSS : 0.6293, NSP_ACC : 0.6184, NSP_LOSS : 0.6190, \n",
      "BATCH_END - 641\n",
      "\n",
      "BATCH_BEGIN - 642\n",
      "Train Step: 643/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6173, MLM_LOSS : 0.6287, NSP_ACC : 0.6178, NSP_LOSS : 0.6184, \n",
      "BATCH_END - 642\n",
      "\n",
      "BATCH_BEGIN - 643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 644/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6176, MLM_LOSS : 0.6290, NSP_ACC : 0.6181, NSP_LOSS : 0.6187, \n",
      "BATCH_END - 643\n",
      "\n",
      "BATCH_BEGIN - 644\n",
      "Train Step: 645/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6171, MLM_LOSS : 0.6285, NSP_ACC : 0.6176, NSP_LOSS : 0.6182, \n",
      "BATCH_END - 644\n",
      "\n",
      "BATCH_BEGIN - 645\n",
      "Train Step: 646/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6166, MLM_LOSS : 0.6279, NSP_ACC : 0.6171, NSP_LOSS : 0.6177, \n",
      "BATCH_END - 645\n",
      "\n",
      "BATCH_BEGIN - 646\n",
      "Train Step: 647/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6158, MLM_LOSS : 0.6272, NSP_ACC : 0.6163, NSP_LOSS : 0.6170, \n",
      "BATCH_END - 646\n",
      "\n",
      "BATCH_BEGIN - 647\n",
      "Train Step: 648/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6155, MLM_LOSS : 0.6268, NSP_ACC : 0.6160, NSP_LOSS : 0.6166, \n",
      "BATCH_END - 647\n",
      "\n",
      "BATCH_BEGIN - 648\n",
      "Train Step: 649/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6150, MLM_LOSS : 0.6264, NSP_ACC : 0.6155, NSP_LOSS : 0.6161, \n",
      "BATCH_END - 648\n",
      "\n",
      "BATCH_BEGIN - 649\n",
      "Train Step: 650/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6150, MLM_LOSS : 0.6263, NSP_ACC : 0.6155, NSP_LOSS : 0.6161, \n",
      "26 EPOCH_END\n",
      "\n",
      "27 EPOCH BEGIN\n",
      "BATCH_BEGIN - 650\n",
      "Train Step: 651/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6154, MLM_LOSS : 0.6268, NSP_ACC : 0.6159, NSP_LOSS : 0.6165, \n",
      "BATCH_END - 650\n",
      "\n",
      "BATCH_BEGIN - 651\n",
      "Train Step: 652/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6158, MLM_LOSS : 0.6272, NSP_ACC : 0.6163, NSP_LOSS : 0.6169, \n",
      "BATCH_END - 651\n",
      "\n",
      "BATCH_BEGIN - 652\n",
      "Train Step: 653/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6163, MLM_LOSS : 0.6277, NSP_ACC : 0.6168, NSP_LOSS : 0.6174, \n",
      "BATCH_END - 652\n",
      "\n",
      "BATCH_BEGIN - 653\n",
      "Train Step: 654/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6168, MLM_LOSS : 0.6281, NSP_ACC : 0.6173, NSP_LOSS : 0.6179, \n",
      "BATCH_END - 653\n",
      "\n",
      "BATCH_BEGIN - 654\n",
      "Train Step: 655/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6167, MLM_LOSS : 0.6281, NSP_ACC : 0.6172, NSP_LOSS : 0.6178, \n",
      "BATCH_END - 654\n",
      "\n",
      "BATCH_BEGIN - 655\n",
      "Train Step: 656/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6171, MLM_LOSS : 0.6285, NSP_ACC : 0.6177, NSP_LOSS : 0.6183, \n",
      "BATCH_END - 655\n",
      "\n",
      "BATCH_BEGIN - 656\n",
      "Train Step: 657/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6169, MLM_LOSS : 0.6283, NSP_ACC : 0.6175, NSP_LOSS : 0.6181, \n",
      "BATCH_END - 656\n",
      "\n",
      "BATCH_BEGIN - 657\n",
      "Train Step: 658/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6168, MLM_LOSS : 0.6282, NSP_ACC : 0.6173, NSP_LOSS : 0.6179, \n",
      "BATCH_END - 657\n",
      "\n",
      "BATCH_BEGIN - 658\n",
      "Train Step: 659/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6167, MLM_LOSS : 0.6280, NSP_ACC : 0.6172, NSP_LOSS : 0.6178, \n",
      "BATCH_END - 658\n",
      "\n",
      "BATCH_BEGIN - 659\n",
      "Train Step: 660/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6159, MLM_LOSS : 0.6273, NSP_ACC : 0.6164, NSP_LOSS : 0.6170, \n",
      "BATCH_END - 659\n",
      "\n",
      "BATCH_BEGIN - 660\n",
      "Train Step: 661/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6160, MLM_LOSS : 0.6273, NSP_ACC : 0.6165, NSP_LOSS : 0.6171, \n",
      "BATCH_END - 660\n",
      "\n",
      "BATCH_BEGIN - 661\n",
      "Train Step: 662/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6159, MLM_LOSS : 0.6272, NSP_ACC : 0.6164, NSP_LOSS : 0.6170, \n",
      "BATCH_END - 661\n",
      "\n",
      "BATCH_BEGIN - 662\n",
      "Train Step: 663/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6163, MLM_LOSS : 0.6276, NSP_ACC : 0.6168, NSP_LOSS : 0.6174, \n",
      "BATCH_END - 662\n",
      "\n",
      "BATCH_BEGIN - 663\n",
      "Train Step: 664/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6167, MLM_LOSS : 0.6281, NSP_ACC : 0.6172, NSP_LOSS : 0.6178, \n",
      "BATCH_END - 663\n",
      "\n",
      "BATCH_BEGIN - 664\n",
      "Train Step: 665/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6165, MLM_LOSS : 0.6278, NSP_ACC : 0.6170, NSP_LOSS : 0.6176, \n",
      "BATCH_END - 664\n",
      "\n",
      "BATCH_BEGIN - 665\n",
      "Train Step: 666/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6159, MLM_LOSS : 0.6273, NSP_ACC : 0.6164, NSP_LOSS : 0.6170, \n",
      "BATCH_END - 665\n",
      "\n",
      "BATCH_BEGIN - 666\n",
      "Train Step: 667/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6156, MLM_LOSS : 0.6269, NSP_ACC : 0.6161, NSP_LOSS : 0.6167, \n",
      "BATCH_END - 666\n",
      "\n",
      "BATCH_BEGIN - 667\n",
      "Train Step: 668/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6149, MLM_LOSS : 0.6262, NSP_ACC : 0.6154, NSP_LOSS : 0.6160, \n",
      "BATCH_END - 667\n",
      "\n",
      "BATCH_BEGIN - 668\n",
      "Train Step: 669/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6147, MLM_LOSS : 0.6261, NSP_ACC : 0.6152, NSP_LOSS : 0.6159, \n",
      "BATCH_END - 668\n",
      "\n",
      "BATCH_BEGIN - 669\n",
      "Train Step: 670/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6142, MLM_LOSS : 0.6256, NSP_ACC : 0.6147, NSP_LOSS : 0.6153, \n",
      "BATCH_END - 669\n",
      "\n",
      "BATCH_BEGIN - 670\n",
      "Train Step: 671/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6135, MLM_LOSS : 0.6249, NSP_ACC : 0.6140, NSP_LOSS : 0.6146, \n",
      "BATCH_END - 670\n",
      "\n",
      "BATCH_BEGIN - 671\n",
      "Train Step: 672/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6137, MLM_LOSS : 0.6251, NSP_ACC : 0.6142, NSP_LOSS : 0.6148, \n",
      "BATCH_END - 671\n",
      "\n",
      "BATCH_BEGIN - 672\n",
      "Train Step: 673/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6139, MLM_LOSS : 0.6253, NSP_ACC : 0.6144, NSP_LOSS : 0.6150, \n",
      "BATCH_END - 672\n",
      "\n",
      "BATCH_BEGIN - 673\n",
      "Train Step: 674/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6143, MLM_LOSS : 0.6256, NSP_ACC : 0.6148, NSP_LOSS : 0.6154, \n",
      "BATCH_END - 673\n",
      "\n",
      "BATCH_BEGIN - 674\n",
      "Train Step: 675/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6257, NSP_ACC : 0.6149, NSP_LOSS : 0.6155, \n",
      "27 EPOCH_END\n",
      "\n",
      "28 EPOCH BEGIN\n",
      "BATCH_BEGIN - 675\n",
      "Train Step: 676/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6257, NSP_ACC : 0.6149, NSP_LOSS : 0.6156, \n",
      "BATCH_END - 675\n",
      "\n",
      "BATCH_BEGIN - 676\n",
      "Train Step: 677/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6139, MLM_LOSS : 0.6252, NSP_ACC : 0.6144, NSP_LOSS : 0.6150, \n",
      "BATCH_END - 676\n",
      "\n",
      "BATCH_BEGIN - 677\n",
      "Train Step: 678/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6141, MLM_LOSS : 0.6254, NSP_ACC : 0.6146, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 677\n",
      "\n",
      "BATCH_BEGIN - 678\n",
      "Train Step: 679/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6140, MLM_LOSS : 0.6254, NSP_ACC : 0.6145, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 678\n",
      "\n",
      "BATCH_BEGIN - 679\n",
      "Train Step: 680/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6257, NSP_ACC : 0.6149, NSP_LOSS : 0.6155, \n",
      "BATCH_END - 679\n",
      "\n",
      "BATCH_BEGIN - 680\n",
      "Train Step: 681/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6139, MLM_LOSS : 0.6252, NSP_ACC : 0.6144, NSP_LOSS : 0.6150, \n",
      "BATCH_END - 680\n",
      "\n",
      "BATCH_BEGIN - 681\n",
      "Train Step: 682/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6141, MLM_LOSS : 0.6254, NSP_ACC : 0.6146, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 681\n",
      "\n",
      "BATCH_BEGIN - 682\n",
      "Train Step: 683/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6137, MLM_LOSS : 0.6250, NSP_ACC : 0.6142, NSP_LOSS : 0.6148, \n",
      "BATCH_END - 682\n",
      "\n",
      "BATCH_BEGIN - 683\n",
      "Train Step: 684/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6140, MLM_LOSS : 0.6254, NSP_ACC : 0.6145, NSP_LOSS : 0.6152, \n",
      "BATCH_END - 683\n",
      "\n",
      "BATCH_BEGIN - 684\n",
      "Train Step: 685/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6139, MLM_LOSS : 0.6252, NSP_ACC : 0.6144, NSP_LOSS : 0.6150, \n",
      "BATCH_END - 684\n",
      "\n",
      "BATCH_BEGIN - 685\n",
      "Train Step: 686/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6139, MLM_LOSS : 0.6253, NSP_ACC : 0.6144, NSP_LOSS : 0.6151, \n",
      "BATCH_END - 685\n",
      "\n",
      "BATCH_BEGIN - 686\n",
      "Train Step: 687/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6137, MLM_LOSS : 0.6250, NSP_ACC : 0.6142, NSP_LOSS : 0.6148, \n",
      "BATCH_END - 686\n",
      "\n",
      "BATCH_BEGIN - 687\n",
      "Train Step: 688/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6142, MLM_LOSS : 0.6256, NSP_ACC : 0.6147, NSP_LOSS : 0.6154, \n",
      "BATCH_END - 687\n",
      "\n",
      "BATCH_BEGIN - 688\n",
      "Train Step: 689/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6143, MLM_LOSS : 0.6257, NSP_ACC : 0.6148, NSP_LOSS : 0.6155, \n",
      "BATCH_END - 688\n",
      "\n",
      "BATCH_BEGIN - 689\n",
      "Train Step: 690/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6143, MLM_LOSS : 0.6256, NSP_ACC : 0.6148, NSP_LOSS : 0.6154, \n",
      "BATCH_END - 689\n",
      "\n",
      "BATCH_BEGIN - 690\n",
      "Train Step: 691/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6143, MLM_LOSS : 0.6256, NSP_ACC : 0.6148, NSP_LOSS : 0.6154, \n",
      "BATCH_END - 690\n",
      "\n",
      "BATCH_BEGIN - 691\n",
      "Train Step: 692/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6137, MLM_LOSS : 0.6251, NSP_ACC : 0.6142, NSP_LOSS : 0.6149, \n",
      "BATCH_END - 691\n",
      "\n",
      "BATCH_BEGIN - 692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 693/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6137, MLM_LOSS : 0.6250, NSP_ACC : 0.6142, NSP_LOSS : 0.6148, \n",
      "BATCH_END - 692\n",
      "\n",
      "BATCH_BEGIN - 693\n",
      "Train Step: 694/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6141, MLM_LOSS : 0.6255, NSP_ACC : 0.6146, NSP_LOSS : 0.6153, \n",
      "BATCH_END - 693\n",
      "\n",
      "BATCH_BEGIN - 694\n",
      "Train Step: 695/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6257, NSP_ACC : 0.6149, NSP_LOSS : 0.6155, \n",
      "BATCH_END - 694\n",
      "\n",
      "BATCH_BEGIN - 695\n",
      "Train Step: 696/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6146, MLM_LOSS : 0.6259, NSP_ACC : 0.6151, NSP_LOSS : 0.6157, \n",
      "BATCH_END - 695\n",
      "\n",
      "BATCH_BEGIN - 696\n",
      "Train Step: 697/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6144, MLM_LOSS : 0.6258, NSP_ACC : 0.6150, NSP_LOSS : 0.6156, \n",
      "BATCH_END - 696\n",
      "\n",
      "BATCH_BEGIN - 697\n",
      "Train Step: 698/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6146, MLM_LOSS : 0.6260, NSP_ACC : 0.6152, NSP_LOSS : 0.6158, \n",
      "BATCH_END - 697\n",
      "\n",
      "BATCH_BEGIN - 698\n",
      "Train Step: 699/1250  \n",
      " - LOGS - \n",
      "training_loss : 0.0000, MLM_ACC : 0.6147, MLM_LOSS : 0.6261, NSP_ACC : 0.6153, NSP_LOSS : 0.6159, \n",
      "BATCH_END - 698\n",
      "\n",
      "BATCH_BEGIN - 699\n"
     ]
    }
   ],
   "source": [
    "history = {}\n",
    "\n",
    "for cb in callbacks :\n",
    "    cb[0].on_train_begin()\n",
    "\n",
    "while current_step < total_training_steps and not model.stop_training :\n",
    "    if (current_step % steps_per_epoch == 0) :\n",
    "        print(\"{} EPOCH BEGIN\".format(int(current_step // steps_per_epoch) + 1))\n",
    "        for cb in callbacks :\n",
    "            cb[0].on_epoch_begin(int(current_step // steps_per_epoch) + 1)\n",
    "\n",
    "    for cb in callbacks :\n",
    "        print(\"BATCH_BEGIN - {}\".format(current_step))\n",
    "        cb[0].on_batch_begin(current_step)\n",
    "        \n",
    "    steps = steps_to_run(current_step, steps_between_evals, steps_per_loop)\n",
    "    \n",
    "    if tf.config.list_physical_devices('GPU') :\n",
    "        for _ in steps :\n",
    "            train_single_step(train_iterator)\n",
    "    else :\n",
    "        train_step(train_iterator, tf.convert_to_tensor(steps, dtype = tf.int32))\n",
    "\n",
    "    train_loss = train_loss_metric.result().numpy().astype(float)\n",
    "    current_step += steps\n",
    "\n",
    "    logs = {'training_loss' : train_loss}\n",
    "\n",
    "    for t_metric in train_metrics :\n",
    "        logs[t_metric.name] = t_metric.result().numpy()\n",
    "\n",
    "    for key in logs.keys() :\n",
    "        if key in history.keys() :\n",
    "            history[key].append(logs[key])\n",
    "        else :\n",
    "            history[key] = [logs[key]]\n",
    "\n",
    "    training_status = 'Train Step: %d/%d  \\n - LOGS - \\n'%(current_step, total_training_steps)\n",
    "    \n",
    "    for key in logs.keys() :\n",
    "        training_status += \"{} : {:.4f}, \".format(key, logs[key])\n",
    "\n",
    "    print(training_status)\n",
    "\n",
    "    if current_step % steps_between_evals:\n",
    "        for cb in callbacks :\n",
    "            print(\"BATCH_END - {}\".format(current_step - 1))\n",
    "            cb[0].on_batch_end(current_step - 1, {'loss': train_loss})\n",
    "\n",
    "    if current_step % steps_per_epoch == 0:\n",
    "        for cb in callbacks :\n",
    "            print(\"{} EPOCH_END\".format(int(current_step / steps_per_epoch)))\n",
    "            cb[0].on_epoch_end(int(current_step / steps_per_epoch), logs)\n",
    "            \n",
    "    print()\n",
    "for cb in callbacks :\n",
    "    cb[0].on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['training_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['MLM_LOSS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['NSP_ACC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['NSP_LOSS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_res = sub_model([inputs['input_ids'], inputs['input_mask'][:, tf.newaxis, tf.newaxis, :], inputs['segment_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_x = model.layers[6]\n",
    "layer_x._output_type = 'predicitions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = layer_x([sub_res['sequence_output'], tf.cast(inputs['masked_lm_positions'], dtype = tf.int32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.math.exp(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
       "array([18107, 13143, 28954,  7298, 18576, 16403,  6968, 25705, 10808,\n",
       "       15261, 21295,  5490, 23695, 26680, 29248, 29065, 12442, 12442,\n",
       "       12442, 12442, 12442, 12442, 12442, 12442, 12442, 12442, 12442,\n",
       "       12442, 12442, 12442, 12442, 12442, 12442, 12442, 12442, 12442,\n",
       "       12442, 12442, 12442, 12442], dtype=int64)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(prediction[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
       "array([ 5587, 13174,  8540,    15,  6031, 21126,   233,  3167,  3095,\n",
       "        6227,  6480,  3167,  3053,  3010,  5719,  1539,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int64)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['masked_lm_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-3000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['머', '포츠', '부른', '##료', '##담', '길을', '6일', '4승', '눋', '##았던', '##참찬', '도망', '브뤼', '도요', '나라가', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자', '역사학자']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(tf.argmax(prediction[1], axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', '-', '##병원', '##이며', '##전투', '##이다', '비영리', '현재는', '상태에서', '##에', '##ital', '##d', '보병', '.', '성', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(inputs['masked_lm_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_labels = inputs['masked_lm_ids'][0]\n",
    "lm_output = output_logits[0]\n",
    "lm_label_weights = inputs['masked_lm_weights'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_accuracy = tf.keras.metrics.sparse_categorical_accuracy(lm_labels, lm_output)\n",
    "numerator = tf.reduce_sum(masked_lm_accuracy * lm_label_weights)\n",
    "denominator = tf.reduce_sum(lm_label_weights) + 1e-5\n",
    "masked_lm_accuracy = numerator / denominator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
