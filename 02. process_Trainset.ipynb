{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def save_pkl(df, file_path) :\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# classtransformers.BertTokenizerFast(\n",
    "# vocab_file, \n",
    "# tokenizer_file=None, \n",
    "# do_lower_case=True, \n",
    "# unk_token='[UNK]', \n",
    "# sep_token='[SEP]', \n",
    "# pad_token='[PAD]', \n",
    "# cls_token='[CLS]', \n",
    "# mask_token='[MASK]', \n",
    "# tokenize_chinese_chars=True, \n",
    "# strip_accents=None, **kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-6000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = load_pkl('./dt/wiki_kor_210520_pages-articles/wiki_ko_dict1_p.pkl')\n",
    "# dict2 = load_pkl('./dt/wiki_kor_210520_pages-articles/wiki_ko_dict2_p.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f3549f8a96459b985bd71f38cc0889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents_ids = list(dict1.keys())\n",
    "\n",
    "vocab_size = tokenizer_for_load.vocab_size\n",
    "\n",
    "train_set = []\n",
    "\n",
    "r_func = np.random.random\n",
    "\n",
    "for document_id, document_key in tqdm(enumerate(documents_ids)) :\n",
    "    \n",
    "    current_document = dict1[document_key]\n",
    "    \n",
    "    for statement_id, current_statement in enumerate(current_document[:-2]) :\n",
    "        \n",
    "        train_dict = {}\n",
    "        \n",
    "        current_statement = current_document[statement_id]\n",
    "\n",
    "        if r_func() > 0.5 :\n",
    "            nsp_token = [0, 1]\n",
    "            r_document_ind = document_id\n",
    "            \n",
    "            while r_document_ind == document_id :\n",
    "                r_document_ind = np.random.randint(0, len(documents_ids))\n",
    "\n",
    "            r_document = dict1[documents_ids[r_document_ind]]\n",
    "\n",
    "            r_statement_ind = np.random.randint(0, len(r_document))\n",
    "            next_statement = r_document[r_statement_ind]\n",
    "\n",
    "        else :\n",
    "            nsp_token = [1, 0]\n",
    "            next_statement = current_document[statement_id+1]\n",
    "        \n",
    "        tokenized_current = tokenizer_for_load(current_statement, return_tensors=\"tf\")\n",
    "        tokenized_next = tokenizer_for_load(next_statement, return_tensors=\"tf\")\n",
    "        \n",
    "        token_data = np.concatenate([tokenized_current['input_ids'].numpy()[0], tokenized_next['input_ids'].numpy()[0, 1:]])\n",
    "        \n",
    "        masked_data = deepcopy(token_data)\n",
    "        masked_position = []\n",
    "        for token_id in range(len(masked_data)) :\n",
    "            \n",
    "            if masked_data[token_id] in [0, 1, 2, 3, 4] :\n",
    "                continue\n",
    "                \n",
    "            if r_func() < 0.15 :\n",
    "                masked_position.append(token_id)\n",
    "                how_change = r_func()\n",
    "                if how_change > 0.2 :\n",
    "                    \n",
    "                    masked_data[token_id] = 4\n",
    "                    \n",
    "                elif how_change > 0.1 :\n",
    "                    random_token = np.random.randint(5, vocab_size)\n",
    "                    \n",
    "                    while masked_data[token_id] == random_token :\n",
    "                        random_token = np.random.randint(5, vocab_size)\n",
    "                    masked_data[token_id] = random_token\n",
    "                    \n",
    "                \n",
    "                else :\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "        \n",
    "        \n",
    "        train_dict['x'] = masked_data\n",
    "        train_dict['label'] = token_data\n",
    "        train_dict['NSP'] = nsp_token\n",
    "        train_dict['masked_position'] = masked_position\n",
    "        \n",
    "        train_set.append(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '특히', '국제', '분쟁', '조정을', '[MASK]', '[MASK]', '김일성', ',', '아이티', '##의', '세', '##드라', '##스', '장군', ',', '팔레', '[MASK]', '##스타', '##인의', '하마', '##스', ',', '보스니아', '##의', '세르비아', '##계', '정권', '##자라는', '미국', '정부에', '대해', '협상을', '거부', '[MASK]', '사태', '##의', '위기를', '초래', '##한', '인물', '및', '단체를', '직접', '만나', '[MASK]', '##의', '원인을', '사망한', '##적으로', '해결하기', '위해', '힘썼다', '.', '[SEP]', '죽음의', '외', '##인', '##부대', '##에서', '[MASK]', '나가', '[MASK]', '위해', '##선', '정해진', '기간을', '채', '##우', '##든지', ',', '임무', '수행', '##의', '대가로', '받는', '돈을', '[MASK]', '비용을', '[MASK]', '##는', '길', '##밖에', '[MASK]', '.', '[SEP]']\n",
      "['[CLS]', '특히', '국제', '분쟁', '조정을', '위해', '북한의', '김일성', ',', '아이티', '##의', '세', '##드라', '##스', '장군', ',', '팔레', '##인', '##스타', '##인의', '하마', '##스', ',', '보스니아', '##의', '세르비아', '##계', '정권', '같이', '미국', '정부에', '대해', '협상을', '거부', '##하면서', '사태', '##의', '위기를', '초래', '##한', '인물', '및', '단체를', '직접', '만나', '분쟁', '##의', '원인을', '근본', '##적으로', '해결하기', '위해', '힘썼다', '.', '[SEP]', '죽음의', '외', '##인', '##부대', '##에서', '빠져', '나가', '##기', '위해', '##선', '정해진', '기간을', '채', '##우', '##든지', ',', '임무', '수행', '##의', '대가로', '받는', '돈을', '모아', '비용을', '치르', '##는', '길', '##밖에', '없다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "chck_id = 29\n",
    "\n",
    "print(tokenizer_for_load.convert_ids_to_tokens(train_set[chck_id]['x']))\n",
    "print(tokenizer_for_load.convert_ids_to_tokens(train_set[chck_id]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(train_set, './dt/train_set1-masked-position.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict1 = load_pkl('./dt/wiki_kor_210520_pages-articles/wiki_ko_dict1_p.pkl')\n",
    "dict2 = load_pkl('./dt/wiki_kor_210520_pages-articles/wiki_ko_dict2_p.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfe5d7af3ba4557a645803dc42e7a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents_ids = list(dict2.keys())\n",
    "\n",
    "vocab_size = tokenizer_for_load.vocab_size\n",
    "\n",
    "train_set = []\n",
    "\n",
    "r_func = np.random.random\n",
    "\n",
    "for document_id, document_key in tqdm(enumerate(documents_ids)) :\n",
    "    \n",
    "    current_document = dict2[document_key]\n",
    "    \n",
    "    for statement_id, current_statement in enumerate(current_document[:-2]) :\n",
    "        \n",
    "        train_dict = {}\n",
    "        \n",
    "        current_statement = current_document[statement_id]\n",
    "\n",
    "        if r_func() > 0.5 :\n",
    "            nsp_token = [0, 1]\n",
    "            r_document_ind = document_id\n",
    "            \n",
    "            while r_document_ind == document_id :\n",
    "                r_document_ind = np.random.randint(0, len(documents_ids))\n",
    "\n",
    "            r_document = dict2[documents_ids[r_document_ind]]\n",
    "\n",
    "            r_statement_ind = np.random.randint(0, len(r_document))\n",
    "            next_statement = r_document[r_statement_ind]\n",
    "\n",
    "        else :\n",
    "            nsp_token = [1, 0]\n",
    "            next_statement = current_document[statement_id+1]\n",
    "        \n",
    "        tokenized_current = tokenizer_for_load(current_statement, return_tensors=\"tf\")\n",
    "        tokenized_next = tokenizer_for_load(next_statement, return_tensors=\"tf\")\n",
    "        \n",
    "        token_data = np.concatenate([tokenized_current['input_ids'].numpy()[0], tokenized_next['input_ids'].numpy()[0, 1:]])\n",
    "        \n",
    "        masked_data = deepcopy(token_data)\n",
    "        \n",
    "        for token_id in range(len(masked_data)) :\n",
    "            \n",
    "            if masked_data[token_id] in [0, 1, 2, 3, 4] :\n",
    "                continue\n",
    "                \n",
    "            if r_func() < 0.15 :\n",
    "                masked_position.append(token_id)\n",
    "                how_change = r_func()\n",
    "                if how_change > 0.2 :\n",
    "                    \n",
    "                    masked_data[token_id] = 4\n",
    "                    \n",
    "                elif how_change > 0.1 :\n",
    "                    random_token = np.random.randint(5, vocab_size)\n",
    "                    \n",
    "                    while masked_data[token_id] == random_token :\n",
    "                        random_token = np.random.randint(5, vocab_size)\n",
    "                    masked_data[token_id] = random_token\n",
    "                    \n",
    "                \n",
    "                else :\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "        \n",
    "        \n",
    "        train_dict['x'] = masked_data\n",
    "        train_dict['label'] = token_data\n",
    "        train_dict['NSP'] = nsp_token\n",
    "        train_dict['masked_position'] = masked_position\n",
    "        \n",
    "        train_set.append(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(train_set, './dt/train_set2-masked-position.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = load_pkl('./dt/train_set1-masked-position.pkl')\n",
    "train2 = deepcopy(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train1 + train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(train, './dt/train_set-masked-position.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def save_pkl(df, file_path) :\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_pkl('./dt/train_set-masked-position.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(filter(lambda x: len(x['x']) <= max_seq_len, train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(x, './dt/train_set-maksed-position_under_{}.pkl'.format(max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(x[:10000], './dt/train_set-maksed-position-sample-10000.pkl'.format(max_seq_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
