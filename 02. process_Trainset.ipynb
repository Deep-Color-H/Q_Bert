{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def save_pkl(df, file_path) :\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# classtransformers.BertTokenizerFast(\n",
    "# vocab_file, \n",
    "# tokenizer_file=None, \n",
    "# do_lower_case=True, \n",
    "# unk_token='[UNK]', \n",
    "# sep_token='[SEP]', \n",
    "# pad_token='[PAD]', \n",
    "# cls_token='[CLS]', \n",
    "# mask_token='[MASK]', \n",
    "# tokenize_chinese_chars=True, \n",
    "# strip_accents=None, **kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-6000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = load_pkl('./dt/wiki_kor_210520_pages-articles/wiki_ko_dict1_p.pkl')\n",
    "# dict2 = load_pkl('./dt/wiki_kor_210520_pages-articles/wiki_ko_dict2_p.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e468e454f38f4ccf9117bb254183a4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents_ids = list(dict1.keys())\n",
    "\n",
    "vocab_size = tokenizer_for_load.vocab_size\n",
    "\n",
    "train_set = []\n",
    "\n",
    "r_func = np.random.random\n",
    "\n",
    "for document_id, document_key in tqdm(enumerate(documents_ids)) :\n",
    "    \n",
    "    current_document = dict1[document_key]\n",
    "    \n",
    "    for statement_id, current_statement in enumerate(current_document[:-2]) :\n",
    "        \n",
    "        train_dict = {}\n",
    "        \n",
    "        current_statement = current_document[statement_id]\n",
    "\n",
    "        if r_func() > 0.5 :\n",
    "            nsp_token = [0, 1]\n",
    "            r_document_ind = document_id\n",
    "            \n",
    "            while r_document_ind == document_id :\n",
    "                r_document_ind = np.random.randint(0, len(documents_ids))\n",
    "\n",
    "            r_document = dict1[documents_ids[r_document_ind]]\n",
    "\n",
    "            r_statement_ind = np.random.randint(0, len(r_document))\n",
    "            next_statement = r_document[r_statement_ind]\n",
    "\n",
    "        else :\n",
    "            nsp_token = [1, 0]\n",
    "            next_statement = current_document[statement_id+1]\n",
    "        \n",
    "        tokenized_current = tokenizer_for_load(current_statement, return_tensors=\"tf\")\n",
    "        tokenized_next = tokenizer_for_load(next_statement, return_tensors=\"tf\")\n",
    "        \n",
    "        token_data = np.concatenate([tokenized_current['input_ids'].numpy()[0], tokenized_next['input_ids'].numpy()[0, 1:]])\n",
    "        \n",
    "        masked_data = deepcopy(token_data)\n",
    "        \n",
    "        for token_id in range(len(masked_data)) :\n",
    "            \n",
    "            if masked_data[token_id] in [0, 1, 2, 3, 4] :\n",
    "                continue\n",
    "                \n",
    "            if r_func() < 0.15 :\n",
    "                \n",
    "                how_change = r_func()\n",
    "                if how_change > 0.2 :\n",
    "                    \n",
    "                    masked_data[token_id] = 4\n",
    "                    \n",
    "                elif how_change > 0.1 :\n",
    "                    \n",
    "                    masked_data[token_id] = np.random.randint(5, vocab_size)\n",
    "                \n",
    "                else :\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "        \n",
    "        \n",
    "        train_dict['x'] = masked_data\n",
    "        train_dict['label'] = token_data\n",
    "        train_dict['NSP'] = nsp_token\n",
    "        \n",
    "        train_set.append(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '특히', '국제', '분쟁', '조정을', '위해', '북한의', '김일성', ',', '아이티', '##의', '세', '##드라', '##스', '장군', '[MASK]', '팔레', '##인', '##스타', '##인의', '하마', '##스', ',', '보스니아', '[MASK]', '세르비아', '##계', '[MASK]', '같이', '미국', '정부에', '대해', '협상을', '거부', '##하면서', '사태', '[MASK]', '위기를', '초래', '##한', '인물', '및', '단체를', '직접', '만나', '분쟁', '##의', '원인을', '근본', '##적으로', '해결하기', '위해', '힘썼다', '냄새', '[SEP]', '[MASK]', '46', '.', '80', '##10', '##km2이고', ',', '인구는', '2015년', '8월', '기준으로', '5', ',', '66', '##2명이다', '.', '[SEP]']\n",
      "['[CLS]', '특히', '국제', '분쟁', '조정을', '위해', '북한의', '김일성', ',', '아이티', '##의', '세', '##드라', '##스', '장군', ',', '팔레', '##인', '##스타', '##인의', '하마', '##스', ',', '보스니아', '##의', '세르비아', '##계', '정권', '같이', '미국', '정부에', '대해', '협상을', '거부', '##하면서', '사태', '##의', '위기를', '초래', '##한', '인물', '및', '단체를', '직접', '만나', '분쟁', '##의', '원인을', '근본', '##적으로', '해결하기', '위해', '힘썼다', '.', '[SEP]', '넓이는', '46', '.', '80', '##10', '##km2이고', ',', '인구는', '2015년', '8월', '기준으로', '5', ',', '66', '##2명이다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "chck_id = 29\n",
    "\n",
    "print(tokenizer_for_load.convert_ids_to_tokens(train_set[chck_id]['x']))\n",
    "print(tokenizer_for_load.convert_ids_to_tokens(train_set[chck_id]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(train_set, './dt/train_set1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict1 = load_pkl('./dt/wiki_kor_210520_pages-articles/wiki_ko_dict1_p.pkl')\n",
    "dict2 = load_pkl('./dt/wiki_kor_210520_pages-articles/wiki_ko_dict2_p.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499b086f56c145e799edba9557344cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents_ids = list(dict2.keys())\n",
    "\n",
    "vocab_size = tokenizer_for_load.vocab_size\n",
    "\n",
    "train_set = []\n",
    "\n",
    "r_func = np.random.random\n",
    "\n",
    "for document_id, document_key in tqdm(enumerate(documents_ids)) :\n",
    "    \n",
    "    current_document = dict2[document_key]\n",
    "    \n",
    "    for statement_id, current_statement in enumerate(current_document[:-2]) :\n",
    "        \n",
    "        train_dict = {}\n",
    "        \n",
    "        current_statement = current_document[statement_id]\n",
    "\n",
    "        if r_func() > 0.5 :\n",
    "            nsp_token = [0, 1]\n",
    "            r_document_ind = document_id\n",
    "            \n",
    "            while r_document_ind == document_id :\n",
    "                r_document_ind = np.random.randint(0, len(documents_ids))\n",
    "\n",
    "            r_document = dict2[documents_ids[r_document_ind]]\n",
    "\n",
    "            r_statement_ind = np.random.randint(0, len(r_document))\n",
    "            next_statement = r_document[r_statement_ind]\n",
    "\n",
    "        else :\n",
    "            nsp_token = [1, 0]\n",
    "            next_statement = current_document[statement_id+1]\n",
    "        \n",
    "        tokenized_current = tokenizer_for_load(current_statement, return_tensors=\"tf\")\n",
    "        tokenized_next = tokenizer_for_load(next_statement, return_tensors=\"tf\")\n",
    "        \n",
    "        token_data = np.concatenate([tokenized_current['input_ids'].numpy()[0], tokenized_next['input_ids'].numpy()[0, 1:]])\n",
    "        \n",
    "        masked_data = deepcopy(token_data)\n",
    "        \n",
    "        for token_id in range(len(masked_data)) :\n",
    "            \n",
    "            if masked_data[token_id] in [0, 1, 2, 3, 4] :\n",
    "                continue\n",
    "                \n",
    "            if r_func() < 0.15 :\n",
    "                \n",
    "                how_change = r_func()\n",
    "                if how_change > 0.2 :\n",
    "                    \n",
    "                    masked_data[token_id] = 4\n",
    "                    \n",
    "                elif how_change > 0.1 :\n",
    "                    \n",
    "                    masked_data[token_id] = np.random.randint(5, vocab_size)\n",
    "                \n",
    "                else :\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "        \n",
    "        \n",
    "        train_dict['x'] = masked_data\n",
    "        train_dict['label'] = token_data\n",
    "        train_dict['NSP'] = nsp_token\n",
    "        \n",
    "        train_set.append(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(train_set, './dt/train_set2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = load_pkl('./dt/train_set1.pkl')\n",
    "train2 = deepcopy(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train1 + train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(train, './dt/train_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(map(lambda x: len(x['x']), train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6838, 55.00080972449242)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x), np.max(x), np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180094"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = 0\n",
    "for k in list(filter(lambda x : x>255, cnt.keys())) :\n",
    "   cumsum += cnt[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2238"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(filter(lambda x: len(x['x']) <= 255, train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(x, './dt/train_set_under_255.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
