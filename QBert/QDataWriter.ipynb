{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "원 Bert Code에 의하면\n",
    "\n",
    "\n",
    "# Traning Instance Parameter들은\n",
    "  - input_files : txt 파일명 목록\n",
    "          # Input file format:\n",
    "          # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "          # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "          # sentence boundaries for the \"next sentence prediction\" task).\n",
    "          # (2) Blank lines between documents. Document boundaries are needed so\n",
    "          # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  - tokenizer : Tokenizer\n",
    "  - FLAGS.max_seq_length : 최대 문장 길이\n",
    "  - FLAGS.dupe_factor : Duplicate Factor, \n",
    "  - FLAGS.short_seq_prob : ??\n",
    "  - FLAGS.masked_lm_prob : MLM을 위한 MASK 확률\n",
    "  - FLAGS.max_predictions_per_seq : 한 번에 예측할 MASK 개수\n",
    "  - rng : rng = random.Random(FLAGS.random_seed)\n",
    "  - FLAGS.do_whole_word_mask : Bert 기능 중 하나, WordPiece로 나뉜 단어 중 사람의 인지하에 있는 단어는 연속으로 MASK할 것인지\n",
    "  - FLAGS.max_ngram_size : ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            token = token if isinstance(token, str) else token.decode('utf8')\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_files,\n",
    "                              tokenizer,\n",
    "                              max_seq_length,\n",
    "                              dupe_factor,\n",
    "                              short_seq_prob,\n",
    "                              masked_lm_prob,\n",
    "                              max_predictions_per_seq,\n",
    "                              rng) :\n",
    "    \n",
    "    all_documents = [[]]\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    \n",
    "    for input_file in input_files :\n",
    "        \n",
    "        inputs = load_pkl(input_file)\n",
    "        keys = list(inputs.keys())\n",
    "        \n",
    "        for key in keys :\n",
    "            tokens = [ tokenizer(x)['input_ids'] for x in inputs[key] if x ]\n",
    "\n",
    "            all_documents.append(tokens)\n",
    "        \n",
    "    \n",
    "    all_documents = [x for x in all_documents if x]\n",
    "#     rng.shuffle(all_documents)\n",
    "    \n",
    "    instances = []\n",
    "    \n",
    "    for _ in range(dupe_factor):\n",
    "        for document_index in range(len(all_documents)):\n",
    "            instances.extend(\n",
    "                create_instances_from_document(\n",
    "                      all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "                      masked_lm_prob, max_predictions_per_seq, rng, vocab_size))\n",
    "\n",
    "#     rng.shuffle(instances)\n",
    "    return instances\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, rng, vocab_size) :\n",
    "    \n",
    "    # 15% 이상으론 Masking하지 않겠다.\n",
    "    # 15%는 반드시 Masking 하겠다.\n",
    "    max_masked_tokens = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "    \n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    masked_grams = []\n",
    "    masked_tokens = [False] * num_tokens\n",
    "    \n",
    "    while sum(masked_tokens) < max_masked_tokens : # and sum(len(s) for s in ngrams.values())):\n",
    "    \n",
    "        # Choose a random n-gram of the given size.\n",
    "        idx = random.choices(range(1, num_tokens))[0] # masked_idx\n",
    "        \n",
    "        if tokens[idx] in [0, 1, 2, 3, 4] : # [PAD], [SEQ], [CLS], [MASK], [UNK] 마스킹 제외\n",
    "            continue\n",
    "        \n",
    "        # Check if any of the tokens in this gram have already been masked.\n",
    "        if masked_tokens[idx]:\n",
    "            continue\n",
    "\n",
    "        # Found a usable n-gram!  Mark its tokens as masked and add it to return.\n",
    "        masked_tokens[idx] = True\n",
    "        masked_grams.append(idx)\n",
    "        \n",
    "    #  output_ngrams [token[idx1], token[idx2], token[idx3], ... token[idx_max_predictions_per_seq]]\n",
    "    \n",
    "    masked_lms = []\n",
    "    output_tokens = list(tokens)\n",
    "    \n",
    "    for gram_idx in masked_grams :\n",
    "        \n",
    "        \n",
    "        if rng.random() < 0.8 :\n",
    "            replace_action = lambda idx: 4 # [\"MASK\"]\n",
    "        else :\n",
    "            if rng.random() < 0.5 :\n",
    "                replace_action = lambda idx : idx\n",
    "            else :\n",
    "                replace_action = lambda idx : rng.choice(range(5, vocab_size)) # [PAD], [SEQ], [CLS], [MASK], [UNK] 제외\n",
    "                \n",
    "        output_tokens[gram_idx] = replace_action(gram_idx)\n",
    "        masked_lms.append([gram_idx, tokens[gram_idx]])\n",
    "        \n",
    "    assert len(masked_lms) <= max_masked_tokens\n",
    "    \n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    \n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p[0])\n",
    "        masked_lm_labels.append(p[1])\n",
    "        \n",
    "    return output_tokens, masked_lm_positions, masked_lm_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainingInstance(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
    "               is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "\n",
    "    def __str__(self):\n",
    "        \n",
    "        s = \"\"\n",
    "        s += \"tokens: %s\\n\" % (\" \".join([str(x) for x in self.tokens]))\n",
    "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "        s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "            [str(x) for x in self.masked_lm_positions]))\n",
    "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "            [str(x) for x in self.masked_lm_labels]))\n",
    "        s += \"\\n\"\n",
    "        \n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(all_documents\n",
    "                                    , document_index\n",
    "                                    , max_seq_length\n",
    "                                    , short_seq_prob\n",
    "                                    , masked_lm_prob\n",
    "                                    , max_predictions_per_seq\n",
    "                                    , rng\n",
    "                                    , vocab_size) :\n",
    "    \n",
    "    current_document = all_documents[document_index]\n",
    "    tokens = []\n",
    "    # Sequence 길이 제한\n",
    "    max_num_tokens = max_seq_length - 3 \n",
    "    target_seq_length = max_num_tokens\n",
    "    \n",
    "    if rng.random() < short_seq_prob:\n",
    "        target_seq_length = rng.randint(2, max_num_tokens)\n",
    "    \n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(current_document) :\n",
    "\n",
    "        current_statement = current_document[i][1:-1] # [CLS], [SEP] 제거\n",
    "        \n",
    "        current_chunk.append(current_statement)\n",
    "        current_length += len(current_statement)\n",
    "        \n",
    "        if i == len(current_document) - 1 or current_length >= target_seq_length:\n",
    "            \n",
    "            if current_chunk:\n",
    "            # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "            # (first) sentence.\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "\n",
    "        \n",
    "                is_random_next = False\n",
    "                tokens_b = []\n",
    "                \n",
    "                if (len(current_chunk) == 1) or (rng.random() > 0.5) :\n",
    "\n",
    "                    is_random_next = True \n",
    "                    target_b_length = target_seq_length - len(tokens_a) \n",
    "                    \n",
    "                    r_document_ind = document_index\n",
    "                    while r_document_ind == document_index :\n",
    "                        r_document_ind = np.random.randint(0, len(all_documents))\n",
    "\n",
    "                    r_document = all_documents[r_document_ind]\n",
    "                    random_start = rng.randint(0, len(r_document) - 1)\n",
    "                    \n",
    "                    for j in range(random_start, len(r_document)):\n",
    "                        tokens_b.extend(r_document[j][1:-1])\n",
    "                        if len(tokens_b) >= target_b_length:\n",
    "                            break\n",
    "\n",
    "                    r_statement_ind = np.random.randint(0, len(r_document))\n",
    "                    next_statement = r_document[r_statement_ind][1:-1]\n",
    "                    \n",
    "                    num_unused_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unused_segments\n",
    "                    \n",
    "                else :\n",
    "\n",
    "                    is_random_next = False #isNext\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "                assert len(tokens_a) >= 1\n",
    "                assert len(tokens_b) >= 1\n",
    "\n",
    "                tokens = []\n",
    "                segment_ids = []\n",
    "                tokens.append(2)\n",
    "                segment_ids.append(0)\n",
    "                \n",
    "                for token in tokens_a:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                tokens.append(3)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                for token in tokens_b:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(1)\n",
    "                tokens.append(3)\n",
    "                segment_ids.append(1)\n",
    "                \n",
    "                (tokens, masked_lm_positions\n",
    "                 , masked_lm_labels) = create_masked_lm_predictions(\n",
    "                    tokens, masked_lm_prob, max_predictions_per_seq, rng\n",
    "                                    , vocab_size)\n",
    "                \n",
    "                instance = TrainingInstance(\n",
    "                    tokens = tokens,\n",
    "                    segment_ids = segment_ids,\n",
    "                    is_random_next = is_random_next,\n",
    "                    masked_lm_positions = masked_lm_positions,\n",
    "                    masked_lm_labels = masked_lm_labels)\n",
    "                \n",
    "                instances.append(instance)\n",
    "                \n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "\n",
    "        # current_statement\n",
    "        # next_statement\n",
    "        # is_random_next\n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "    return feature\n",
    "\n",
    "\n",
    "def create_float_feature(values):\n",
    "    feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files,\n",
    "                                    gzip_compress, use_v2_feature_names):\n",
    "    \"\"\"Creates TF example files from `TrainingInstance`s.\"\"\"\n",
    "    writers = []\n",
    "    for output_file in output_files:\n",
    "        writers.append(\n",
    "            tf.io.TFRecordWriter(\n",
    "                output_file, options=\"GZIP\" if gzip_compress else \"\"))\n",
    "\n",
    "    writer_index = 0\n",
    "\n",
    "    total_written = 0\n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        input_ids = instance.tokens\n",
    "#         input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        masked_lm_positions = list(instance.masked_lm_positions)\n",
    "        masked_lm_ids = instance.masked_lm_labels\n",
    "        masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "        while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "            masked_lm_positions.append(0)\n",
    "            masked_lm_ids.append(0)\n",
    "            masked_lm_weights.append(0.0)\n",
    "\n",
    "        next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        if use_v2_feature_names:\n",
    "            features[\"input_word_ids\"] = create_int_feature(input_ids)\n",
    "            features[\"input_type_ids\"] = create_int_feature(segment_ids)\n",
    "        else:\n",
    "            features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "            features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "\n",
    "        features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "        features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "        features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "        features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "        features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "        writers[writer_index].write(tf_example.SerializeToString())\n",
    "        writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "        total_written += 1\n",
    "\n",
    "    for writer in writers:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from transformers import BertTokenizerFast\n",
    "import random\n",
    "\n",
    "file_path = ['../dt/wiki_ko_dict_sample.pkl']\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('../model/BertTokenizer-3000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)\n",
    "\n",
    "instances = create_training_instances(input_files=file_path\n",
    "                              , tokenizer = tokenizer_for_load\n",
    "                              , max_seq_length = 256\n",
    "                              , dupe_factor = 2\n",
    "                              , short_seq_prob = .15\n",
    "                              , masked_lm_prob = .15\n",
    "                              , max_predictions_per_seq = 255\n",
    "                              , rng = random.Random(123))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "write_instance_to_example_files(instances = instances\n",
    "                                , tokenizer =  tokenizer_for_load\n",
    "                                , max_seq_length = 256\n",
    "                                , max_predictions_per_seq = 255\n",
    "                                , output_files = ['./Test_Examples.tfrecords']\n",
    "                                , gzip_compress = False\n",
    "                                , use_v2_feature_names = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filenames = ['./Test_Examples.tfrecords']\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([256], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([256], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([256], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([255], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([255], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([255], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "parsed_dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "siz = 0\n",
    "for raw_record in parsed_dataset.batch(1):\n",
    "    siz +=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
