{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "원 Bert Code에 의하면\n",
    "\n",
    "\n",
    "# Traning Instance Parameter들은\n",
    "  - input_files : txt 파일명 목록\n",
    "          # Input file format:\n",
    "          # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "          # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "          # sentence boundaries for the \"next sentence prediction\" task).\n",
    "          # (2) Blank lines between documents. Document boundaries are needed so\n",
    "          # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  - tokenizer : Tokenizer\n",
    "  - FLAGS.max_seq_length : 최대 문장 길이\n",
    "  - FLAGS.dupe_factor : Duplicate Factor, \n",
    "  - FLAGS.short_seq_prob : ??\n",
    "  - FLAGS.masked_lm_prob : MLM을 위한 MASK 확률\n",
    "  - FLAGS.max_predictions_per_seq : 한 번에 예측할 MASK 개수\n",
    "  - rng : rng = random.Random(FLAGS.random_seed)\n",
    "  - FLAGS.do_whole_word_mask : Bert 기능 중 하나, WordPiece로 나뉜 단어 중 사람의 인지하에 있는 단어는 연속으로 MASK할 것인지\n",
    "  - FLAGS.max_ngram_size : ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_files,\n",
    "                              tokenizer,\n",
    "                              max_seq_length,\n",
    "                              dupe_factor,\n",
    "                              short_seq_prob,\n",
    "                              masked_lm_prob,\n",
    "                              max_predictions_per_seq,\n",
    "                              rng,\n",
    "                              do_whole_word_mask=False,\n",
    "                              max_ngram_size=None) :\n",
    "    \n",
    "    all_documents = [[]]\n",
    "    \n",
    "    for input_file in input_files :\n",
    "        \n",
    "        inputs = load_pkl(input_file)\n",
    "        keys = list(inputs.keys())\n",
    "        \n",
    "        for key in keys :\n",
    "            tokens = [ tokenizer(x)['input_ids'] for x in inputs[key] if x ]\n",
    "\n",
    "            all_documents.append(tokens)\n",
    "        \n",
    "    \n",
    "    all_documents = [x for x in all_documents if x]\n",
    "    rng.shuffle(all_documents)\n",
    "    \n",
    "#     vocab_words = list(tokenizer.vocab.keys())\n",
    "#     instances = []\n",
    "    \n",
    "#     for _ in range(dupe_factor):\n",
    "#         for document_index in range(len(all_documents)):\n",
    "#             instances.extend(\n",
    "#                 create_instances_from_document(\n",
    "#                       all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "#                       masked_lm_prob, max_predictions_per_seq, vocab_words, rng,\n",
    "#                       do_whole_word_mask, max_ngram_size))\n",
    "\n",
    "#     rng.shuffle(instances)\n",
    "    return all_documents\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizerFast' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c6e7c6257799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer_for_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertTokenizerFast' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "tokenizer_for_load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizerFast\n",
    "import random\n",
    "\n",
    "file_path = ['../dt/wiki_ko_dict_sample.pkl']\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('../model/BertTokenizer-3000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)\n",
    "ans = create_training_instances(input_files=file_path\n",
    "                              , tokenizer = tokenizer_for_load\n",
    "                              , max_seq_length = 130\n",
    "                              , dupe_factor = 2\n",
    "                              , short_seq_prob = .15\n",
    "                              , masked_lm_prob = .15\n",
    "                              , max_predictions_per_seq = 255\n",
    "                              , rng = random.Random(123)\n",
    "                              , do_whole_word_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_for_load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 2817, 3625, 3105, 3], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_for_load(\"한규인\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한', '##규', '##인은', '과연', '무엇을', '어떻게']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_for_load.tokenize('한규인은 과연 무엇을', \"어떻게\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng,\n",
    "    do_whole_word_mask=False, max_ngram_size=None):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
