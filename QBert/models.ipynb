{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"Creates a positional embedding.\n",
    "    Example:\n",
    "    ```python\n",
    "    position_embedding = PositionEmbedding(max_length=100)\n",
    "    inputs = tf.keras.Input((100, 32), dtype=tf.float32)\n",
    "    outputs = position_embedding(inputs)\n",
    "    ```\n",
    "    Args:\n",
    "    max_length: The maximum size of the dynamic sequence.\n",
    "    initializer: The initializer to use for the embedding weights. Defaults to\n",
    "      \"glorot_uniform\".\n",
    "    seq_axis: The axis of the input tensor where we add the embeddings.\n",
    "    Reference: This layer creates a positional embedding as described in\n",
    "    [BERT: Pre-training of Deep Bidirectional Transformers for Language\n",
    "    Understanding](https://arxiv.org/abs/1810.04805).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_length, initializer=\"glorot_uniform\", seq_axis=1,  **kwargs):\n",
    "\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        \n",
    "        if max_length is None:\n",
    "            raise ValueError(\"`max_length` must be an Integer, not `None`.\")\n",
    "        \n",
    "        self._max_length = max_length\n",
    "        self._initializer = tf.keras.initializers.get(initializer)\n",
    "        self._seq_axis = seq_axis\n",
    "\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"max_length\": self._max_length,\n",
    "            \"initializer\": tf.keras.initializers.serialize(self._initializer),\n",
    "            \"seq_axis\": self._seq_axis,\n",
    "        }\n",
    "        base_config = super(PositionEmbedding, self).get_config()\n",
    "    \n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        dimension_list = input_shape.as_list()\n",
    "\n",
    "        seq_length = dimension_list[self._seq_axis]\n",
    "        width = dimension_list[-1]\n",
    "\n",
    "        if self._max_length is not None:\n",
    "            weight_sequence_length = self._max_length\n",
    "        else:\n",
    "            weight_sequence_length = seq_length\n",
    "\n",
    "        self._position_embeddings = self.add_weight(\"PositionEmbeddings\", shape=[weight_sequence_length, width], initializer=self._initializer)\n",
    "\n",
    "        super(PositionEmbedding, self).build(input_shape)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        actual_seq_len = input_shape[self._seq_axis]\n",
    "        position_embeddings = self._position_embeddings[:actual_seq_len, :]\n",
    "        \n",
    "        new_shape = [1 for _ in inputs.get_shape().as_list()]\n",
    "        new_shape[self._seq_axis] = actual_seq_len\n",
    "        new_shape[-1] = position_embeddings.get_shape().as_list()[-1]\n",
    "        \n",
    "        position_embeddings = tf.reshape(position_embeddings, new_shape)\n",
    "        \n",
    "        return tf.broadcast_to(position_embeddings, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer) :\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, name = 'multi_head_attention') :\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__(name = name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        assert d_model == (num_heads * self.depth)\n",
    "        \n",
    "        self.w_q = tf.keras.layers.Dense(self.d_model, name = \"{}/Q_WEIGHT\".format(self.name))\n",
    "        self.w_k = tf.keras.layers.Dense(self.d_model, name = \"{}/K_WEIGHT\".format(self.name))\n",
    "        self.w_v = tf.keras.layers.Dense(self.d_model, name = \"{}/V_WEIGHT\".format(self.name))\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model, name = \"{}/OUTPUT_DENSE\".format(self.name))\n",
    "        \n",
    "    def split_head(self, l, batch_size) :\n",
    "        outputs = tf.reshape(l, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(outputs, perm = [0, 2, 1, 3])\n",
    "    \n",
    "    def scaled_dot_product(self, query, key, value, mask) :\n",
    "        \n",
    "        d_k = tf.cast(self.depth, dtype = tf.float32)\n",
    "        \n",
    "        dot_score = tf.matmul(query, key, transpose_b = True) / tf.math.sqrt(d_k / self.num_heads)\n",
    "        \n",
    "        if mask is not None :\n",
    "            dot_score += mask * -1e9\n",
    "        \n",
    "        attention_score = tf.nn.softmax(dot_score)\n",
    "        outputs = tf.matmul(attention_score, value)\n",
    "        \n",
    "        return outputs, attention_score\n",
    "    \n",
    "    \n",
    "    def call(self, inputs) :\n",
    "        \n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        \n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        \n",
    "        # inputs : (batch, seq_len, d_model)\n",
    "        query = self.w_q(query)\n",
    "        key   = self.w_k(key)\n",
    "        value = self.w_v(value)\n",
    "        \n",
    "        # q, k, v\n",
    "        # (batch, seq_len, d_model) -> (batch, num_heads, seq_len, depth)\n",
    "        query = self.split_head(query, batch_size)\n",
    "        key = self.split_head(key, batch_size)\n",
    "        value = self.split_head(value, batch_size)\n",
    "        \n",
    "        # scaled_dot_product\n",
    "        outputs, _ = self.scaled_dot_product(query, key, value, mask)\n",
    "        \n",
    "        outputs = tf.reshape(outputs, (batch_size, -1, self.d_model))\n",
    "        outputs = self.dense(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name = 'encoder_layer') :\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name = '{}/Input'.format(name))\n",
    "    \n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = '{}/padding_mask'.format(name))\n",
    "\n",
    "    attention = MultiHeadAttention(d_model = d_model, num_heads = num_heads, name = \"{}/MHA\".format(name))({\n",
    "        'query' : inputs, 'key' : inputs, 'value' : inputs,\n",
    "        'mask': padding_mask\n",
    "    })\n",
    "\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout, name = \"{}/Dropout1\".format(name))(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6, name = \"{}/LM1\".format(name))(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units = dff, activation = custom_gelu, name = \"{}/FFN1\".format(name))(attention)\n",
    "    outputs = tf.keras.layers.Dense(units = d_model, name = \"{}/FFN2\".format(name))(outputs)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout, name = \"{}/Dropout2\".format(name))(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6, name = \"{}/LM2\".format(name))(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs = [inputs, padding_mask], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qbert_encoder(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name = 'qbert'):\n",
    "\n",
    "    input_ids = tf.keras.Input(shape = (None, ), name = 'BertInput/inputs')\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = 'BertInput/padding_mask')\n",
    "    segments = tf.keras.Input(shape = (None, ), name = 'BertInput/segments')\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model, name = 'Bert/Embedding')(input_ids)\n",
    "    embeddings += PositionEmbedding(max_seq_len)(embeddings)\n",
    "    embeddings += tf.keras.layers.Embedding(3, d_model, name = 'Bert/SegEmbedding')(segments) # sentence A or sentence B\n",
    "    \n",
    "    output = tf.keras.layers.Dropout(rate = dropout, name = 'Bert/Dropout_Embedding')(embeddings)\n",
    "    output = tf.keras.layers.LayerNormalization(epsilon = 1e-6, name = 'Bert/LM_Embedding')(output)\n",
    "    \n",
    "    encode_outputs = []\n",
    "    for i in range(num_layers) :\n",
    "\n",
    "        output = encoder_layer(dff = dff\n",
    "                                , d_model=d_model\n",
    "                                , num_heads = num_heads\n",
    "                                , dropout = dropout\n",
    "                                , name = 'Encoding_Layer_{}'.format(i))([output, padding_mask])\n",
    "        \n",
    "        encode_outputs.append(output)\n",
    "    \n",
    "    outputs['sequence_output'] = encode_outputs[-1]\n",
    "    outputs['hidden_states'] = encode_outputs\n",
    "    \n",
    "    pooler_output = tf.keras.layers.Dense(d_model, name = 'Bert/pooler_layer')(encode_outputs[-1][:, 0, :])\n",
    "    outputs['pooled_output'] = pooler_output\n",
    "    \n",
    "    return tf.keras.Model(inputs = [input_ids, padding_mask, segments], outputs = outputs, name = name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_models_fn(vocab_size\n",
    "                       , hidden_size\n",
    "                       , type_vocab_size\n",
    "                       , num_layers\n",
    "                       , num_attention_heads\n",
    "                       , max_seq_length\n",
    "                       , max_predictions_per_seq\n",
    "                       , dropout_rate\n",
    "                       , inner_dim \n",
    "                       , initializer) :\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,)\n",
    "                                           , name='input_word_ids', dtype=tf.int32)\n",
    "    \n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,)\n",
    "                                       , name='input_mask', dtype=tf.int32)\n",
    "    \n",
    "    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,)\n",
    "                                           , name='input_type_ids', dtype=tf.int32)\n",
    "    \n",
    "    masked_lm_positions = tf.keras.layers.Input(shape=(max_predictions_per_seq,)\n",
    "                                                ,  name='masked_lm_positions', dtype=tf.int32)\n",
    "    \n",
    "    masked_lm_ids = tf.keras.layers.Input(shape=(max_predictions_per_seq,)\n",
    "                                          , name='masked_lm_ids', dtype=tf.int32)\n",
    "    \n",
    "    masked_lm_weights = tf.keras.layers.Input(shape=(max_predictions_per_seq,)\n",
    "                                              , name='masked_lm_weights', dtype=tf.int32)\n",
    "    \n",
    "    next_sentence_labels = tf.keras.layers.Input(shape=(1, )\n",
    "                                                 , name = 'next_sentence_labels', dtype = tf.int32)\n",
    "    \n",
    "    bert_encoder = qbert_encoder(\n",
    "        vocab_size, max_seq_length, num_layers,\n",
    "        inner_dim, hidden_size, num_attention_heads,\n",
    "        dropout_rate, name = 'qbert_encoder')\n",
    "    \n",
    "    \n",
    "    input_mask_r = input_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    encoder_output = bert_encoder([input_word_ids, input_mask_r, input_type_ids])\n",
    "    \n",
    "    embedding = bert_encoder.layers[1].weights[0]\n",
    "    \n",
    "    seg_output = encoder_output['sequence_output']\n",
    "    \n",
    "    cls_output = encoder_output['pooled_output']\n",
    "    \n",
    "    lm_output = LmLayer(embedding = embedding,\n",
    "                        output = 'logits')([seg_output, masked_lm_positions])\n",
    "    \n",
    "    sentence_output = Classification(input_width = hidden_size,\n",
    "                                     num_classes = 2)(cls_output)\n",
    "    \n",
    "    loss_metric_layer = BertPretrainLossAndMetricLayer(vocab_size)\n",
    "    \n",
    "    losses = loss_metric_layer(lm_output_logits = lm_output\n",
    "                               ,sentence_output_logits = sentence_output\n",
    "                               ,lm_label_ids = masked_lm_ids \n",
    "                               ,lm_label_weights = masked_lm_weights\n",
    "                               ,sentence_labels = next_sentence_labels)\n",
    "    \n",
    "    inputs = {'input_ids' : input_word_ids,\n",
    "              'input_mask' : input_mask,\n",
    "              'segment_ids' : input_type_ids,\n",
    "              'masked_lm_ids' : masked_lm_ids,\n",
    "              'masked_lm_positions': masked_lm_positions,\n",
    "              'masked_lm_weights' : masked_lm_weights,\n",
    "              'next_sentence_labels' : next_sentence_labels}\n",
    "    \n",
    "    pretrain_model = tf.keras.Model(inputs = inputs, outputs = losses)\n",
    "    \n",
    "    return pretrain_model, bert_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Returns layer that computes custom loss and metrics for pretraining.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, **kwargs):\n",
    "        super(BertPretrainLossAndMetricLayer, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self.config = {\n",
    "            'vocab_size': vocab_size,\n",
    "        }\n",
    "\n",
    "    def _add_metrics(self, lm_output, lm_labels, lm_label_weights,\n",
    "                   lm_example_loss, sentence_output, sentence_labels,\n",
    "                   next_sentence_loss):\n",
    "        \"\"\"Adds metrics.\"\"\"\n",
    "        masked_lm_accuracy = tf.keras.metrics.sparse_categorical_accuracy(\n",
    "            lm_labels, lm_output)\n",
    "        numerator = tf.reduce_sum(masked_lm_accuracy * lm_label_weights)\n",
    "        denominator = tf.reduce_sum(lm_label_weights) + 1e-5\n",
    "        masked_lm_accuracy = numerator / denominator\n",
    "        self.add_metric(\n",
    "            masked_lm_accuracy, name='MLM_ACC', aggregation='mean')\n",
    "\n",
    "        self.add_metric(lm_example_loss, name='MLM_LOSS', aggregation='mean')\n",
    "\n",
    "        if sentence_labels is not None:\n",
    "            next_sentence_accuracy = tf.keras.metrics.sparse_categorical_accuracy(\n",
    "              sentence_labels, sentence_output)\n",
    "            self.add_metric(\n",
    "              next_sentence_accuracy,\n",
    "              name='NSP_ACC',\n",
    "              aggregation='mean')\n",
    "\n",
    "        if next_sentence_loss is not None:\n",
    "            self.add_metric(\n",
    "              next_sentence_loss, name='NSP_LOSS', aggregation='mean')\n",
    "\n",
    "    def call(self,\n",
    "               lm_output_logits,\n",
    "               sentence_output_logits,\n",
    "               lm_label_ids,\n",
    "               lm_label_weights,\n",
    "               sentence_labels=None):\n",
    "\n",
    "        \"\"\"Implements call() for the layer.\"\"\"\n",
    "        lm_label_weights = tf.cast(lm_label_weights, tf.float32)\n",
    "        lm_output_logits = tf.cast(lm_output_logits, tf.float32)\n",
    "\n",
    "        lm_prediction_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            lm_label_ids, lm_output_logits, from_logits=True)\n",
    "        lm_numerator_loss = tf.reduce_sum(lm_prediction_losses * lm_label_weights)\n",
    "        lm_denominator_loss = tf.reduce_sum(lm_label_weights)\n",
    "        mask_label_loss = tf.math.divide_no_nan(lm_numerator_loss,\n",
    "                                                lm_denominator_loss)\n",
    "\n",
    "        if sentence_labels is not None:\n",
    "            sentence_output_logits = tf.cast(sentence_output_logits, tf.float32)\n",
    "            sentence_loss = tf.keras.losses.sparse_categorical_crossentropy(sentence_labels, sentence_output_logits, from_logits=True)\n",
    "            sentence_loss = tf.reduce_mean(sentence_loss)\n",
    "            loss = mask_label_loss + sentence_loss\n",
    "        else:\n",
    "            sentence_loss = None\n",
    "            loss = mask_label_loss\n",
    "\n",
    "        batch_shape = tf.slice(tf.shape(lm_label_ids), [0], [1])\n",
    "        # TODO(hongkuny): Avoids the hack and switches add_loss.\n",
    "        final_loss = tf.fill(batch_shape, loss)\n",
    "\n",
    "        self._add_metrics(lm_output_logits, lm_label_ids, lm_label_weights,\n",
    "                          mask_label_loss, sentence_output_logits, sentence_labels,\n",
    "                          sentence_loss)\n",
    "        \n",
    "        return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LmLayer(tf.keras.layers.Layer) :\n",
    "    \n",
    "    def __init__(self, embedding, output) :\n",
    "        \n",
    "        super(LmLayer, self).__init__()\n",
    "        \n",
    "        self.embedding_table = embedding\n",
    "        \n",
    "        if output not in ('predictions', 'logits'):\n",
    "            raise ValueError(\n",
    "                ('Unknown `output` value \"%s\". `output` can be either \"logits\" or '\n",
    "                 '\"predictions\"') % output)\n",
    "        self._output_type = output\n",
    "        \n",
    "        self._vocab_size, hidden_size = self.embedding_table.shape\n",
    "        self.dense = tf.keras.layers.Dense(hidden_size, activation = custom_gelu, name = 'transform/dense')\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(axis = -1, epsilon = 1e-12, name = 'transform/LayerNorm')\n",
    "        self.bias = self.add_weight('transform/bias', shape = (self._vocab_size), initializer = 'zeros', trainable = True)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs) :\n",
    "        \n",
    "        seg_output, masked_lm_positions = inputs[0], inputs[1]\n",
    "        \n",
    "        sequence_shape = tf.shape(seg_output) # [batch_size, seq_length, dff]\n",
    "        batch_size, seq_length = sequence_shape[0], sequence_shape[1] \n",
    "        width = seg_output.shape.as_list()[2] or sequence_shape[2]\n",
    "\n",
    "        flat_offsets = tf.reshape(\n",
    "            tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "        flat_positions = tf.reshape(masked_lm_positions + flat_offsets, [-1])\n",
    "        flat_sequence_tensor = tf.reshape(seg_output,\n",
    "                                          [batch_size * seq_length, width])\n",
    "        gathered_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "        \n",
    "        output = self.dense(gathered_tensor)\n",
    "        output = self.layer_norm(output)\n",
    "        output = tf.matmul(output, self.embedding_table, transpose_b = True)\n",
    "        \n",
    "        logits = tf.nn.bias_add(output, self.bias)\n",
    "        masked_positions_length = masked_lm_positions.shape.as_list()[1] or tf.shape(masked_lm_positions)[1]\n",
    "        \n",
    "        logits = tf.reshape(logits, shape = [ -1, masked_positions_length, self._vocab_size])\n",
    "        \n",
    "        if self._output_type == 'predictions' :\n",
    "            return logits\n",
    "        \n",
    "        return tf.nn.log_softmax(logits)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "               input_width,\n",
    "               num_classes,\n",
    "               initializer='glorot_uniform',\n",
    "               output='logits',\n",
    "               **kwargs):\n",
    "\n",
    "        cls_output = tf.keras.layers.Input(shape=(input_width,), name='cls_output', dtype=tf.float32)\n",
    "\n",
    "        logits = tf.keras.layers.Dense( \n",
    "            num_classes,\n",
    "            activation=None,\n",
    "            kernel_initializer=initializer,\n",
    "            name='predictions/transform/logits')(cls_output)\n",
    "\n",
    "        if output == 'logits':\n",
    "            output_tensors = logits\n",
    "            \n",
    "        elif output == 'predictions':\n",
    "            policy = tf.keras.mixed_precision.global_policy()\n",
    "            \n",
    "            if policy.name == 'mixed_bfloat16':\n",
    "                # b/158514794: bf16 is not stable with post-softmax cross-entropy.\n",
    "                policy = tf.float32\n",
    "                output_tensors = tf.keras.layers.Activation(\n",
    "                  tf.nn.log_softmax, dtype=policy)(logits)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                  ('Unknown `output` value \"%s\". `output` can be either \"logits\" or '\n",
    "                   '\"predictions\"') % output)\n",
    "\n",
    "        super(Classification, self).__init__(inputs=[cls_output], outputs=output_tensors, **kwargs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 255\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pretrain_model, bert_encoder = get_bert_models_fn(vocab_size\n",
    "                 , hidden_size\n",
    "                 , type_vocab_size\n",
    "                 , num_layers\n",
    "                 , num_attention_heads\n",
    "                 , max_seq_length\n",
    "                 , max_predictions_per_seq\n",
    "                 , dropout_rate\n",
    "                 , inner_dim \n",
    "                 , initializer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
