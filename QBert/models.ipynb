{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"Creates a positional embedding.\n",
    "    Example:\n",
    "    ```python\n",
    "    position_embedding = PositionEmbedding(max_length=100)\n",
    "    inputs = tf.keras.Input((100, 32), dtype=tf.float32)\n",
    "    outputs = position_embedding(inputs)\n",
    "    ```\n",
    "    Args:\n",
    "    max_length: The maximum size of the dynamic sequence.\n",
    "    initializer: The initializer to use for the embedding weights. Defaults to\n",
    "      \"glorot_uniform\".\n",
    "    seq_axis: The axis of the input tensor where we add the embeddings.\n",
    "    Reference: This layer creates a positional embedding as described in\n",
    "    [BERT: Pre-training of Deep Bidirectional Transformers for Language\n",
    "    Understanding](https://arxiv.org/abs/1810.04805).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_length, initializer=\"glorot_uniform\", seq_axis=1,  **kwargs):\n",
    "\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        \n",
    "        if max_length is None:\n",
    "            raise ValueError(\"`max_length` must be an Integer, not `None`.\")\n",
    "        \n",
    "        self._max_length = max_length\n",
    "        self._initializer = tf.keras.initializers.get(initializer)\n",
    "        self._seq_axis = seq_axis\n",
    "\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"max_length\": self._max_length,\n",
    "            \"initializer\": tf.keras.initializers.serialize(self._initializer),\n",
    "            \"seq_axis\": self._seq_axis,\n",
    "        }\n",
    "        base_config = super(PositionEmbedding, self).get_config()\n",
    "    \n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        dimension_list = input_shape.as_list()\n",
    "\n",
    "        seq_length = dimension_list[self._seq_axis]\n",
    "        width = dimension_list[-1]\n",
    "\n",
    "        if self._max_length is not None:\n",
    "            weight_sequence_length = self._max_length\n",
    "        else:\n",
    "            weight_sequence_length = seq_length\n",
    "\n",
    "        self._position_embeddings = self.add_weight(\"embeddings\", shape=[weight_sequence_length, width], initializer=self._initializer)\n",
    "\n",
    "        super(PositionEmbedding, self).build(input_shape)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        actual_seq_len = input_shape[self._seq_axis]\n",
    "        position_embeddings = self._position_embeddings[:actual_seq_len, :]\n",
    "        \n",
    "        new_shape = [1 for _ in inputs.get_shape().as_list()]\n",
    "        new_shape[self._seq_axis] = actual_seq_len\n",
    "        new_shape[-1] = position_embeddings.get_shape().as_list()[-1]\n",
    "        \n",
    "        position_embeddings = tf.reshape(position_embeddings, new_shape)\n",
    "        \n",
    "        return tf.broadcast_to(position_embeddings, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer) :\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, name = 'multi_head_attention') :\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__(name = name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.depth = d_model // num_heads\n",
    "        assert d_model == (num_heads * self.depth)\n",
    "        \n",
    "        self.w_q = tf.keras.layers.Dense(self.d_model)\n",
    "        self.w_k = tf.keras.layers.Dense(self.d_model)\n",
    "        self.w_v = tf.keras.layers.Dense(self.d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_head(self, l, batch_size) :\n",
    "        outputs = tf.reshape(l, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(outputs, perm = [0, 2, 1, 3])\n",
    "    \n",
    "    def scaled_dot_product(self, query, key, value, mask) :\n",
    "        \n",
    "        d_k = tf.cast(self.depth, dtype = tf.float32)\n",
    "        \n",
    "        dot_score = tf.matmul(query, key, transpose_b = True) / tf.math.sqrt(d_k / self.num_heads)\n",
    "        \n",
    "        if mask is not None :\n",
    "            dot_score += mask * -1e9\n",
    "        \n",
    "        attention_score = tf.nn.softmax(dot_score)\n",
    "        outputs = tf.matmul(attention_score, value)\n",
    "        \n",
    "        return outputs, attention_score\n",
    "    \n",
    "    \n",
    "    def call(self, inputs) :\n",
    "        \n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        \n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        \n",
    "        # inputs : (batch, seq_len, d_model)\n",
    "        query = self.w_q(query)\n",
    "        key   = self.w_k(key)\n",
    "        value = self.w_v(value)\n",
    "        \n",
    "        # q, k, v\n",
    "        # (batch, seq_len, d_model) -> (batch, num_heads, seq_len, depth)\n",
    "        query = self.split_head(query, batch_size)\n",
    "        key = self.split_head(key, batch_size)\n",
    "        value = self.split_head(value, batch_size)\n",
    "        \n",
    "        # scaled_dot_product\n",
    "        outputs, _ = self.scaled_dot_product(query, key, value, mask)\n",
    "        \n",
    "        outputs = tf.reshape(outputs, (batch_size, -1, self.d_model))\n",
    "        outputs = self.dense(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name = 'encoder_layer') :\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name = 'inputs')\n",
    "\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = 'padding_mask')\n",
    "\n",
    "    attention = MultiHeadAttention(d_model = d_model, num_heads = num_heads)({\n",
    "        'query' : inputs, 'key' : inputs, 'value' : inputs,\n",
    "        'mask': padding_mask\n",
    "    })\n",
    "\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units = dff, activation = custom_gelu)(attention)\n",
    "    outputs = tf.keras.layers.Dense(units = d_model)(outputs)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs = [inputs, padding_mask], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qbert_encoder(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name = 'qbert'):\n",
    "\n",
    "    input_ids = tf.keras.Input(shape = (None, ), name = 'inputs')\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = 'padding_mask')\n",
    "    segments = tf.keras.Input(shape = (None, ), name = 'segments')\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(input_ids)\n",
    "    embeddings += PositionEmbedding(max_seq_len)(embeddings)\n",
    "    embeddings += tf.keras.layers.Embedding(3, d_model)(segments) # sentence A or sentence B\n",
    "    \n",
    "    output = tf.keras.layers.Dropout(rate = dropout)(embeddings)\n",
    "    output = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(output)\n",
    "    \n",
    "    encode_outputs = []\n",
    "    for i in range(num_layers) :\n",
    "\n",
    "        output = encoder_layer(dff = dff\n",
    "                                , d_model=d_model\n",
    "                                , num_heads = num_heads\n",
    "                                , dropout = dropout\n",
    "                                , name = 'encoding_layer_{}'.format(i))([output, padding_mask])\n",
    "        \n",
    "        encode_outputs.append(output)\n",
    "    \n",
    "    outputs['sequence_output'] = encode_outputs[-1]\n",
    "    outputs['hidden_states'] = encode_outputs\n",
    "    \n",
    "    pooler_output = tf.keras.layers.Dense(d_model, name = 'pooler_layer')(encode_outputs[-1])\n",
    "    outputs['pooled_output'] = pooler_output\n",
    "    \n",
    "    return tf.keras.Model(inputs = [input_ids, padding_mask, segments], outputs = outputs, name = name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_models_fn(vocab_size\n",
    "                       , hidden_size\n",
    "                       , type_vocab_size\n",
    "                       , num_layers\n",
    "                       , num_attention_heads\n",
    "                       , max_seq_length\n",
    "                       , max_predictions_per_seq\n",
    "                       , dropout_rate\n",
    "                       , inner_dim \n",
    "                       , initializer) :\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), name='input_word_ids', dtype=tf.int32)\n",
    "    \n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), name='input_mask', dtype=tf.int32)\n",
    "    \n",
    "    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), name='input_type_ids', dtype=tf.int32)\n",
    "    \n",
    "    masked_lm_positions = tf.keras.layers.Input(shape=(max_predictions_per_seq,),  name='masked_lm_positions', dtype=tf.int32)\n",
    "    \n",
    "    masked_lm_ids = tf.keras.layers.Input(shape=(max_predictions_per_seq,), name='masked_lm_ids', dtype=tf.int32)\n",
    "    \n",
    "    masked_lm_weights = tf.keras.layers.Input(shape=(max_predictions_per_seq,), name='masked_lm_weights', dtype=tf.int32)\n",
    "    \n",
    "    next_sentence_labels = tf.keras.layers.Input(shape=(1, ), name = 'next_sentence_labels', dtype = tf.int32)\n",
    "    \n",
    "    bert_encoder = qbert_encoder(\n",
    "        vocab_size, max_seq_length, num_layers,\n",
    "        inner_dim, hidden_size, num_attention_heads,\n",
    "        dropout_rate, name = 'qbert_encoder')\n",
    "    \n",
    "    input_mask = input_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    encoder_output = bert_encoder([input_word_ids, input_mask, input_type_ids])\n",
    "    \n",
    "    seg_output = encoder_output['']\n",
    "    \n",
    "    cls_output = encoder_output['']\n",
    "    \n",
    "    lm_output = LmLayer([seg_output, masked_lm_positions])\n",
    "    sentence_output = classificationLayer(cls_output)\n",
    "    \n",
    "    losses = LossMetricLayer([lm_output, sentence_output, masked_lm_ids, masked_lm_weights, next_sentence_labels])\n",
    "    \n",
    "    \n",
    "    pretrain_model = tf.keras.Model(inputs = [input_word_ids, input_mask, input_type_ids], outputs = loss)\n",
    "    \n",
    "    return pretrain_model, bert_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model, bert_encoder = get_bert_models_fn(vocab_size\n",
    "                 , hidden_size\n",
    "                 , type_vocab_size\n",
    "                 , num_layers\n",
    "                 , num_attention_heads\n",
    "                 , max_seq_length\n",
    "                 , max_predictions_per_seq\n",
    "                 , dropout_rate\n",
    "                 , inner_dim \n",
    "                 , initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
