{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mirrored_cross_device_ops(all_reduce_alg, num_packs):\n",
    "    \"\"\"Return a CrossDeviceOps based on all_reduce_alg and num_packs.\n",
    "    Args:\n",
    "        all_reduce_alg: a string specifying which cross device op to pick, or None.\n",
    "        num_packs: an integer specifying number of packs for the cross device op.\n",
    "    Returns:\n",
    "        tf.distribute.CrossDeviceOps object or None.\n",
    "    Raises:\n",
    "        ValueError: if `all_reduce_alg` not in [None, \"nccl\", \"hierarchical_copy\"].\n",
    "    \"\"\"\n",
    "    if all_reduce_alg is None:\n",
    "        return None\n",
    "    mirrored_all_reduce_options = {\n",
    "                  \"nccl\": tf.distribute.NcclAllReduce,\n",
    "                  \"hierarchical_copy\": tf.distribute.HierarchicalCopyAllReduce\n",
    "                }\n",
    "    if all_reduce_alg not in mirrored_all_reduce_options:\n",
    "        raise ValueError(\n",
    "                \"When used with `mirrored`, valid values for all_reduce_alg are \"\n",
    "                \"[`nccl`, `hierarchical_copy`].  Supplied value: {}\".format(\n",
    "                    all_reduce_alg))\n",
    "    cross_device_ops_class = mirrored_all_reduce_options[all_reduce_alg]\n",
    "    return cross_device_ops_class(num_packs=num_packs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpu_initialize(tpu_address):\n",
    "    \"\"\"Initializes TPU for TF 2.x training.\n",
    "    Args:\n",
    "        tpu_address: string, bns address of master TPU worker.\n",
    "    Returns:\n",
    "        A TPUClusterResolver.\n",
    "    \"\"\"\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "                                                              tpu=tpu_address)\n",
    "    if tpu_address not in (\"\", \"local\"):\n",
    "        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    return cluster_resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_strategy(distribution_strategy=\"mirrored\",\n",
    "                              num_gpus=0,\n",
    "                              all_reduce_alg=None,\n",
    "                              num_packs=1,\n",
    "                              tpu_address=None,\n",
    "                              **kwargs):\n",
    "    \"\"\"Return a DistributionStrategy for running the model.\n",
    "    Args:\n",
    "    distribution_strategy: a string specifying which distribution strategy to\n",
    "      use. Accepted values are \"off\", \"one_device\", \"mirrored\",\n",
    "      \"parameter_server\", \"multi_worker_mirrored\", and \"tpu\" -- case\n",
    "      insensitive. \"off\" means not to use Distribution Strategy; \"tpu\" means to\n",
    "      use TPUStrategy using `tpu_address`.\n",
    "    num_gpus: Number of GPUs to run this model.\n",
    "    all_reduce_alg: Optional. Specifies which algorithm to use when performing\n",
    "      all-reduce. For `MirroredStrategy`, valid values are \"nccl\" and\n",
    "      \"hierarchical_copy\". For `MultiWorkerMirroredStrategy`, valid values are\n",
    "      \"ring\" and \"nccl\".  If None, DistributionStrategy will choose based on\n",
    "      device topology.\n",
    "    num_packs: Optional.  Sets the `num_packs` in `tf.distribute.NcclAllReduce`\n",
    "      or `tf.distribute.HierarchicalCopyAllReduce` for `MirroredStrategy`.\n",
    "    tpu_address: Optional. String that represents TPU to connect to. Must not be\n",
    "      None if `distribution_strategy` is set to `tpu`.\n",
    "    **kwargs: Additional kwargs for internal usages.\n",
    "    Returns:\n",
    "    tf.distribute.DistibutionStrategy object.\n",
    "    Raises:\n",
    "    ValueError: if `distribution_strategy` is \"off\" or \"one_device\" and\n",
    "      `num_gpus` is larger than 1; or `num_gpus` is negative or if\n",
    "      `distribution_strategy` is `tpu` but `tpu_address` is not specified.\n",
    "    \"\"\"\n",
    "    del kwargs\n",
    "    \n",
    "    if num_gpus < 0:\n",
    "        raise ValueError(\"`num_gpus` can not be negative.\")\n",
    "\n",
    "    if not isinstance(distribution_strategy, str):\n",
    "        msg = (\"distribution_strategy must be a string but got: %s.\" %\n",
    "               (distribution_strategy,))\n",
    "        \n",
    "        if distribution_strategy == False:  # pylint: disable=singleton-comparison,g-explicit-bool-comparison\n",
    "            msg += (\" If you meant to pass the string 'off', make sure you add \"\n",
    "                    \"quotes around 'off' so that yaml interprets it as a string \"\n",
    "                    \"instead of a bool.\")\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    distribution_strategy = distribution_strategy.lower()\n",
    "    \n",
    "#     if distribution_strategy == \"off\":\n",
    "#         if num_gpus > 1:\n",
    "#             raise ValueError(\"When {} GPUs are specified, distribution_strategy \"\n",
    "#                            \"flag cannot be set to `off`.\".format(num_gpus))\n",
    "#         return None\n",
    "\n",
    "    if distribution_strategy == \"tpu\":\n",
    "        # When tpu_address is an empty string, we communicate with local TPUs.\n",
    "        cluster_resolver = tpu_initialize(tpu_address)\n",
    "        return tf.distribute.TPUStrategy(cluster_resolver)\n",
    "\n",
    "#     if distribution_strategy == \"multi_worker_mirrored\":\n",
    "#         return tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "#             communication=_collective_communication(all_reduce_alg))\n",
    "\n",
    "#     if distribution_strategy == \"one_device\":\n",
    "#         if num_gpus == 0:\n",
    "#             return tf.distribute.OneDeviceStrategy(\"device:CPU:0\")\n",
    "#         if num_gpus > 1:\n",
    "#             raise ValueError(\"`OneDeviceStrategy` can not be used for more than \"\n",
    "#                            \"one device.\")\n",
    "#         return tf.distribute.OneDeviceStrategy(\"device:GPU:0\")\n",
    "\n",
    "    if distribution_strategy == \"mirrored\":\n",
    "        if num_gpus == 0:\n",
    "            devices = [\"device:CPU:0\"]\n",
    "        else:\n",
    "            devices = [\"device:GPU:%d\" % i for i in range(num_gpus)]\n",
    "        return tf.distribute.MirroredStrategy(devices=devices,\n",
    "                        cross_device_ops=_mirrored_cross_device_ops(all_reduce_alg, num_packs))\n",
    "\n",
    "#     if distribution_strategy == \"parameter_server\":\n",
    "#         cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "#         return tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\n",
    "\n",
    "    raise ValueError(\"Unrecognized Distribution Strategy: %r\" %\n",
    "                   distribution_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-be2df751dfb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAdamWeightDecay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     def __init__(self,\n\u001b[0;32m      4\u001b[0m                \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                \u001b[0mbeta_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class AdamWeightDecay(tf.keras.optimizers.Adam):\n",
    "\n",
    "    def __init__(self,\n",
    "               learning_rate=0.001,\n",
    "               beta_1=0.9,\n",
    "               beta_2=0.999,\n",
    "               epsilon=1e-7,\n",
    "               amsgrad=False,\n",
    "               weight_decay_rate=0.0,\n",
    "               include_in_weight_decay=None,\n",
    "               exclude_from_weight_decay=None,\n",
    "               gradient_clip_norm=1.0,\n",
    "               name='AdamWeightDecay',\n",
    "               **kwargs):\n",
    "        super(AdamWeightDecay, self).__init__(learning_rate, beta_1, beta_2,\n",
    "                                          epsilon, amsgrad, name, **kwargs)\n",
    "        self.weight_decay_rate = weight_decay_rate\n",
    "        self.gradient_clip_norm = gradient_clip_norm\n",
    "        self._include_in_weight_decay = include_in_weight_decay\n",
    "        self._exclude_from_weight_decay = exclude_from_weight_decay\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Creates an optimizer from its config with WarmUp custom object.\"\"\"\n",
    "        custom_objects = {'WarmUp': WarmUp}\n",
    "        return super(AdamWeightDecay, cls).from_config(config, custom_objects=custom_objects)\n",
    "\n",
    "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "        super(AdamWeightDecay, self)._prepare_local(var_device, var_dtype,\n",
    "                                            apply_state)\n",
    "        apply_state[(var_device, var_dtype)]['weight_decay_rate'] = tf.constant(\n",
    "            self.weight_decay_rate, name='adam_weight_decay_rate')\n",
    "\n",
    "    def _decay_weights_op(self, var, learning_rate, apply_state):\n",
    "        do_decay = self._do_use_weight_decay(var.name)\n",
    "        if do_decay:\n",
    "            return var.assign_sub(\n",
    "              learning_rate * var *\n",
    "              apply_state[(var.device, var.dtype.base_dtype)]['weight_decay_rate'],\n",
    "              use_locking=self._use_locking)\n",
    "        return tf.no_op()\n",
    "\n",
    "    def apply_gradients(self,\n",
    "                  grads_and_vars,\n",
    "                  name=None,\n",
    "                  experimental_aggregate_gradients=True):\n",
    "        grads, tvars = list(zip(*grads_and_vars))\n",
    "        if experimental_aggregate_gradients and self.gradient_clip_norm > 0.0:\n",
    "            # when experimental_aggregate_gradients = False, apply_gradients() no\n",
    "            # longer implicitly allreduce gradients, users manually allreduce gradient\n",
    "            # and passed the allreduced grads_and_vars. For now, the\n",
    "            # clip_by_global_norm will be moved to before the explicit allreduce to\n",
    "            # keep the math the same as TF 1 and pre TF 2.2 implementation.\n",
    "            (grads, _) = tf.clip_by_global_norm(grads, clip_norm=self.gradient_clip_norm)\n",
    "            return super(AdamWeightDecay, self).apply_gradients(\n",
    "                zip(grads, tvars),\n",
    "                name=name,\n",
    "                experimental_aggregate_gradients=experimental_aggregate_gradients)\n",
    "\n",
    "    def _get_lr(self, var_device, var_dtype, apply_state):\n",
    "        \"\"\"Retrieves the learning rate with the given state.\"\"\"\n",
    "        if apply_state is None:\n",
    "            return self._decayed_lr_t[var_dtype], {}\n",
    "\n",
    "        apply_state = apply_state or {}\n",
    "        coefficients = apply_state.get((var_device, var_dtype))\n",
    "        if coefficients is None:\n",
    "            coefficients = self._fallback_apply_state(var_device, var_dtype)\n",
    "            apply_state[(var_device, var_dtype)] = coefficients\n",
    "\n",
    "        return coefficients['lr_t'], dict(apply_state=apply_state)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state)\n",
    "        decay = self._decay_weights_op(var, lr_t, apply_state)\n",
    "        with tf.control_dependencies([decay]):\n",
    "            return super(AdamWeightDecay, self)._resource_apply_dense(grad, var, **kwargs)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "        lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state)\n",
    "        decay = self._decay_weights_op(var, lr_t, apply_state)\n",
    "        with tf.control_dependencies([decay]):\n",
    "            return super(AdamWeightDecay, self)._resource_apply_sparse(grad, var, indices, **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(AdamWeightDecay, self).get_config()\n",
    "        config.update({'weight_decay_rate': self.weight_decay_rate,   })\n",
    "        return config\n",
    "\n",
    "    def _do_use_weight_decay(self, param_name):\n",
    "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "        if self.weight_decay_rate == 0:\n",
    "            return False\n",
    "\n",
    "        if self._include_in_weight_decay:\n",
    "            for r in self._include_in_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return True\n",
    "\n",
    "        if self._exclude_from_weight_decay:\n",
    "            for r in self._exclude_from_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                      return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Applies a warmup schedule on a given learning rate decay schedule.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 initial_learning_rate,\n",
    "                 decay_schedule_fn,\n",
    "                 warmup_steps,\n",
    "                 power=1.0,\n",
    "                 name=None):\n",
    "        super(WarmUp, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.power = power\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or 'WarmUp') as name:\n",
    "          # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n",
    "          # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "            global_step_float = tf.cast(step, tf.float32)\n",
    "            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "            warmup_percent_done = global_step_float / warmup_steps_float\n",
    "            warmup_learning_rate = (\n",
    "                self.initial_learning_rate *\n",
    "                tf.math.pow(warmup_percent_done, self.power))\n",
    "        return tf.cond(\n",
    "              global_step_float < warmup_steps_float,\n",
    "              lambda: warmup_learning_rate,\n",
    "              lambda: self.decay_schedule_fn(step),\n",
    "              name=name)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'initial_learning_rate': self.initial_learning_rate,\n",
    "            'decay_schedule_fn': self.decay_schedule_fn,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'power': self.power,\n",
    "            'name': self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
