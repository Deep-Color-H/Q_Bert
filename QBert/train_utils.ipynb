{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mirrored_cross_device_ops(all_reduce_alg, num_packs):\n",
    "    \"\"\"Return a CrossDeviceOps based on all_reduce_alg and num_packs.\n",
    "    Args:\n",
    "        all_reduce_alg: a string specifying which cross device op to pick, or None.\n",
    "        num_packs: an integer specifying number of packs for the cross device op.\n",
    "    Returns:\n",
    "        tf.distribute.CrossDeviceOps object or None.\n",
    "    Raises:\n",
    "        ValueError: if `all_reduce_alg` not in [None, \"nccl\", \"hierarchical_copy\"].\n",
    "    \"\"\"\n",
    "    if all_reduce_alg is None:\n",
    "        return None\n",
    "    mirrored_all_reduce_options = {\n",
    "                  \"nccl\": tf.distribute.NcclAllReduce,\n",
    "                  \"hierarchical_copy\": tf.distribute.HierarchicalCopyAllReduce\n",
    "                }\n",
    "    if all_reduce_alg not in mirrored_all_reduce_options:\n",
    "        raise ValueError(\n",
    "                \"When used with `mirrored`, valid values for all_reduce_alg are \"\n",
    "                \"[`nccl`, `hierarchical_copy`].  Supplied value: {}\".format(\n",
    "                    all_reduce_alg))\n",
    "    cross_device_ops_class = mirrored_all_reduce_options[all_reduce_alg]\n",
    "    return cross_device_ops_class(num_packs=num_packs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpu_initialize(tpu_address):\n",
    "    \"\"\"Initializes TPU for TF 2.x training.\n",
    "    Args:\n",
    "        tpu_address: string, bns address of master TPU worker.\n",
    "    Returns:\n",
    "        A TPUClusterResolver.\n",
    "    \"\"\"\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "                                                              tpu=tpu_address)\n",
    "    if tpu_address not in (\"\", \"local\"):\n",
    "        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    return cluster_resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_strategy(distribution_strategy=\"mirrored\",\n",
    "                              num_gpus=0,\n",
    "                              all_reduce_alg=None,\n",
    "                              num_packs=1,\n",
    "                              tpu_address=None,\n",
    "                              **kwargs):\n",
    "    \"\"\"Return a DistributionStrategy for running the model.\n",
    "    Args:\n",
    "    distribution_strategy: a string specifying which distribution strategy to\n",
    "      use. Accepted values are \"off\", \"one_device\", \"mirrored\",\n",
    "      \"parameter_server\", \"multi_worker_mirrored\", and \"tpu\" -- case\n",
    "      insensitive. \"off\" means not to use Distribution Strategy; \"tpu\" means to\n",
    "      use TPUStrategy using `tpu_address`.\n",
    "    num_gpus: Number of GPUs to run this model.\n",
    "    all_reduce_alg: Optional. Specifies which algorithm to use when performing\n",
    "      all-reduce. For `MirroredStrategy`, valid values are \"nccl\" and\n",
    "      \"hierarchical_copy\". For `MultiWorkerMirroredStrategy`, valid values are\n",
    "      \"ring\" and \"nccl\".  If None, DistributionStrategy will choose based on\n",
    "      device topology.\n",
    "    num_packs: Optional.  Sets the `num_packs` in `tf.distribute.NcclAllReduce`\n",
    "      or `tf.distribute.HierarchicalCopyAllReduce` for `MirroredStrategy`.\n",
    "    tpu_address: Optional. String that represents TPU to connect to. Must not be\n",
    "      None if `distribution_strategy` is set to `tpu`.\n",
    "    **kwargs: Additional kwargs for internal usages.\n",
    "    Returns:\n",
    "    tf.distribute.DistibutionStrategy object.\n",
    "    Raises:\n",
    "    ValueError: if `distribution_strategy` is \"off\" or \"one_device\" and\n",
    "      `num_gpus` is larger than 1; or `num_gpus` is negative or if\n",
    "      `distribution_strategy` is `tpu` but `tpu_address` is not specified.\n",
    "    \"\"\"\n",
    "    del kwargs\n",
    "    \n",
    "    if num_gpus < 0:\n",
    "        raise ValueError(\"`num_gpus` can not be negative.\")\n",
    "\n",
    "    if not isinstance(distribution_strategy, str):\n",
    "        msg = (\"distribution_strategy must be a string but got: %s.\" %\n",
    "               (distribution_strategy,))\n",
    "        \n",
    "        if distribution_strategy == False:  # pylint: disable=singleton-comparison,g-explicit-bool-comparison\n",
    "            msg += (\" If you meant to pass the string 'off', make sure you add \"\n",
    "                    \"quotes around 'off' so that yaml interprets it as a string \"\n",
    "                    \"instead of a bool.\")\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    distribution_strategy = distribution_strategy.lower()\n",
    "    \n",
    "#     if distribution_strategy == \"off\":\n",
    "#         if num_gpus > 1:\n",
    "#             raise ValueError(\"When {} GPUs are specified, distribution_strategy \"\n",
    "#                            \"flag cannot be set to `off`.\".format(num_gpus))\n",
    "#         return None\n",
    "\n",
    "    if distribution_strategy == \"tpu\":\n",
    "        # When tpu_address is an empty string, we communicate with local TPUs.\n",
    "        cluster_resolver = tpu_initialize(tpu_address)\n",
    "        return tf.distribute.TPUStrategy(cluster_resolver)\n",
    "\n",
    "#     if distribution_strategy == \"multi_worker_mirrored\":\n",
    "#         return tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "#             communication=_collective_communication(all_reduce_alg))\n",
    "\n",
    "#     if distribution_strategy == \"one_device\":\n",
    "#         if num_gpus == 0:\n",
    "#             return tf.distribute.OneDeviceStrategy(\"device:CPU:0\")\n",
    "#         if num_gpus > 1:\n",
    "#             raise ValueError(\"`OneDeviceStrategy` can not be used for more than \"\n",
    "#                            \"one device.\")\n",
    "#         return tf.distribute.OneDeviceStrategy(\"device:GPU:0\")\n",
    "\n",
    "    if distribution_strategy == \"mirrored\":\n",
    "        if num_gpus == 0:\n",
    "            devices = [\"device:CPU:0\"]\n",
    "        else:\n",
    "            devices = [\"device:GPU:%d\" % i for i in range(num_gpus)]\n",
    "        return tf.distribute.MirroredStrategy(devices=devices,\n",
    "                        cross_device_ops=_mirrored_cross_device_ops(all_reduce_alg, num_packs))\n",
    "\n",
    "#     if distribution_strategy == \"parameter_server\":\n",
    "#         cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "#         return tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\n",
    "\n",
    "    raise ValueError(\"Unrecognized Distribution Strategy: %r\" %\n",
    "                   distribution_strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
