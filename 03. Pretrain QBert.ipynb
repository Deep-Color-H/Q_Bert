{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import qbert_model\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def save_pkl(df, file_path) :\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "def create_padding_mask(x):\n",
    "    init_shape = x.shape\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, key의 문장 길이)\n",
    "    return np.array(mask).reshape(init_shape[0], 1,1, init_shape[1])\n",
    "\n",
    "def ind_to_weight(masked_pos, seq_len) :\n",
    "    return tf.reduce_sum(tf.one_hot(masked_pos, seq_len), axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModule(tf.keras.Model) :\n",
    "\n",
    "    def __init__(self, vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name) :\n",
    "        super(BertModule, self).__init__()\n",
    "        self.Bert = qbert_model(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)\n",
    "        self.dense_cls = tf.keras.layers.Dense(2, activation = 'softmax', use_bias = False)\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def call(self, inputs) :\n",
    "        \n",
    "        x, mask, lm, nsp, weight = inputs[0], inputs[1], inputs[2], inputs[3], inputs[4]\n",
    "    \n",
    "        bert_outputs = self.Bert([x, mask])\n",
    "\n",
    "        y_pred = bert_outputs['sequence_output']\n",
    "\n",
    "        decode_matrix = tf.linalg.pinv(self.Bert.layers[1].weights[0])\n",
    "\n",
    "        pred_lm =  tf.math.softmax(tf.matmul(y_pred, decode_matrix))\n",
    "        pred_cls = self.dense_cls(y_pred[:, 0])\n",
    "\n",
    "        true_y_lm = tf.cast(tf.one_hot(tf.cast(lm, dtype = tf.int32), depth = self.vocab_size), dtype = tf.float32)\n",
    "\n",
    "        lm_losses = (tf.reduce_sum(true_y_lm * -tf.math.log(pred_lm), axis = 2))\n",
    "#         lm_losses = lm_losses * weight\n",
    "        lm_losses = tf.reduce_mean(lm_losses, axis = 1)\n",
    "\n",
    "        nsp = tf.cast(nsp, dtype = tf.float32)\n",
    "        cls_losses = tf.reduce_mean(tf.reduce_sum(nsp * -tf.math.log(pred_cls), axis = 1))\n",
    "\n",
    "        total_loss = lm_losses + cls_losses\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def get_pretrained_result(self, inputs) :\n",
    "        \n",
    "        x, mask = inputs[0], inputs[1]\n",
    "    \n",
    "        bert_outputs = self.Bert([x, mask])\n",
    "\n",
    "        y_pred = bert_outputs['sequence_output']\n",
    "\n",
    "        decode_matrix = tf.linalg.pinv(self.Bert.layers[1].weights[0])\n",
    "\n",
    "        pred_lm =  tf.math.softmax(tf.matmul(y_pred, decode_matrix))\n",
    "        pred_cls = self.dense_cls(y_pred[:, 0])\n",
    "        \n",
    "        return pred_lm, pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = load_pkl('./dt/train_set_under_255.pkl')\n",
    "# train = load_pkl('./dt/train_set-maksed-position-sample-10000.pkl')\n",
    "train = load_pkl('./dt/train_set-masked-position.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(filter(lambda x: len(x['x']) <= 130, train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = train[:batch_size * 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000\n",
    "max_seq_len = 130\n",
    "num_layers = 3\n",
    "dff = 256\n",
    "d_model = 100\n",
    "num_heads = 5\n",
    "dropout = .1\n",
    "name = 'qbert_210603'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = pad_sequences([ x['x'] for x in train ], max_seq_len, padding = 'post')\n",
    "y = pad_sequences([ x['label'] for x in train ] , max_seq_len, padding = 'post')\n",
    "nsp = np.asarray([ x['NSP'] for x in train ])\n",
    "\n",
    "masked_lm_weight = np.array([ ind_to_weight(x['masked_position'], max_seq_len) for x in train])\n",
    "\n",
    "mask = create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 1e-4\n",
    "batch_size = 256\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainBert = BertModule(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainBert.compile(optimizer=optimizer, loss ='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"qbert_210603\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    3200000     inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_1 (TensorFlowOp [(None, None, 100)]  0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "position_embedding_1 (PositionE (None, None, 100)    13000       tf_op_layer_Mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_7 (TensorFlow [(None, None, 100)]  0           tf_op_layer_Mul_1[0][0]          \n",
      "                                                                 position_embedding_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, 100)    0           tf_op_layer_AddV2_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "padding_mask (InputLayer)       [(None, 1, 1, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoding_layer_0 (Model)        (None, None, 100)    92356       dropout_7[0][0]                  \n",
      "                                                                 padding_mask[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoding_layer_1 (Model)        (None, None, 100)    92356       encoding_layer_0[1][0]           \n",
      "                                                                 padding_mask[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoding_layer_2 (Model)        (None, None, 100)    92356       encoding_layer_1[1][0]           \n",
      "                                                                 padding_mask[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, None, 100)    10100       encoding_layer_0[1][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,500,168\n",
      "Trainable params: 3,500,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrainBert.Bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_y = np.array([ 0 for _ in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_25/kernel:0', 'dense_25/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_25/kernel:0', 'dense_25/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_25/kernel:0', 'dense_25/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_25/kernel:0', 'dense_25/bias:0'] when minimizing the loss.\n",
      "1982/1982 [==============================] - 2196s 1s/step - loss: 1.7110\n"
     ]
    }
   ],
   "source": [
    "hist = pretrainBert.fit(batch_size = batch_size, epochs = epochs\n",
    "                        , x = [x, mask, y, nsp, masked_lm_weight[:]], y = false_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model/BertPretrained-210603-130-3-100-5.pt\\assets\n"
     ]
    }
   ],
   "source": [
    "today = '210603'\n",
    "\n",
    "pretrainBert.save('./model/BertPretrained-{}-{}-{}-{}-{}.pt'.format(today, max_seq_len, num_layers, d_model, num_heads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set = train[np.random.randint(0, len(train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-6000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 밀도 ##행렬 ##을 대각 ##화 ##하면 그 각 원소는 확률 formula _ 6 ##가 완비 , 이는 위의 통계 ##역학 [MASK] 정의 [MASK] 동등 ##하다 . [SEP] 콩 [MASK] [MASK] ##스트 퍼 ##블리 ##케이션 ##즈가 [MASK] 있으며 , 캘리포니아 샌프란시스코 ##에서 출판된 ##툉 [MASK] [SEP]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_statement = ' '.join(tokenizer_for_load.convert_ids_to_tokens(sample_train_set['x']))\n",
    "train_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 밀도 ##행렬 ##을 대각 ##화 ##하면 그 각 원소는 확률 formula _ 6 ##가 되므로 , 이는 위의 통계 ##역학 ##적 정의 ##와 동등 ##하다 . [SEP] 콩 ##데 나 ##스트 퍼 ##블리 ##케이션 ##즈가 소유하고 있으며 , 캘리포니아 샌프란시스코 ##에서 출판된 ##다 . [SEP]'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_statement = ' '.join(tokenizer_for_load.convert_ids_to_tokens(sample_train_set['label']))\n",
    "train_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set = train[np.random.randint(0, len(train))]\n",
    "\n",
    "train_x = tf.reshape(sample_train_set['x'], (1, -1))\n",
    "train_x = pad_sequences(train_x, max_seq_len, padding = 'post')\n",
    "mask = create_padding_mask(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, nls = pretrainBert.get_pretrained_result([train_x, mask, \"\", \"\", \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"뉴저지 뉴저지 뉴저지 뉴저지 뉴저지 뉴저지 ' ' ' ' 뉴저지 뉴저지 뉴저지 뉴저지 뉴저지 뉴저지 뉴저지 뉴저지 뉴저지 ##m 뉴저지 ##m ##m 뉴저지 뉴저지 뉴저지 책임을 책임을 책임을 이때 이때 책임을 책임을 책임을 책임을 책임을 책임을 이때 책임을 책임을 책임을 책임을 이때 이때 책임을 책임을 책임을 이때 이때 이때 이때 이때 ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' 책임을 ' 책임을 책임을 책임을 책임을 책임을 책임을 ##소프트 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 책임을 ##m ##m ##m ##m ##m ##m ##m ' ' ##m ##m ##m ##m ##m ##m ##m ##m 이탈리아의 ##m ##m ##m ##m ##m ##m ##m ##m\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_statement = ' '.join(tokenizer_for_load.convert_ids_to_tokens(tf.argmax(lm, axis = 2)[0]))\n",
    "train_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.5446929, 0.4553071]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'embedding_19/embeddings:0' shape=(32000, 100) dtype=float32, numpy=\n",
       "array([[ 0.00675264,  0.02614969,  0.04783888, ...,  0.00074808,\n",
       "         0.0081974 ,  0.02426926],\n",
       "       [ 0.02625655,  0.00214106,  0.02599093, ..., -0.0249519 ,\n",
       "        -0.02679936,  0.01629817],\n",
       "       [ 0.00712447, -0.02990005,  0.02621973, ..., -0.00580653,\n",
       "        -0.00704775,  0.00537025],\n",
       "       ...,\n",
       "       [-0.0472474 ,  0.00386853, -0.00768339, ...,  0.02705099,\n",
       "        -0.03506238,  0.0258582 ],\n",
       "       [-0.00262028,  0.02314945,  0.00940512, ...,  0.02038808,\n",
       "         0.02099202, -0.02432241],\n",
       "       [ 0.02712569, -0.00255732, -0.04550922, ..., -0.00697222,\n",
       "        -0.04350788,  0.0189664 ]], dtype=float32)>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrainBert.Bert.layers[1].weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
