{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from QBert.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import qbert_model\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def save_pkl(df, file_path) :\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, key의 문장 길이)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = load_pkl('./dt/train_set_under_255.pkl')\n",
    "train = load_pkl('./dt/train_set_parse-10000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000\n",
    "max_seq_len = 255\n",
    "num_layers = 12\n",
    "dff = 768\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "dropout = .1\n",
    "name = 'qbert_210602'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModule(tf.keras.Model) :\n",
    "    \n",
    "    def __init__(self, vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name) :\n",
    "        super(BertModule, self).__init__()\n",
    "        \n",
    "        self.Bert = qbert_model(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)\n",
    "        \n",
    "        self.dense_cls = tf.keras.layers.Dense(2, activation = 'softmax', use_bias = False)\n",
    "    \n",
    "    def call(self, inputs) :\n",
    "        \n",
    "        input, mask, masked_position = inputs['input'], inputs['mask'], inputs['masked_position']\n",
    "        bert_outputs = self.Bert([input, mask])\n",
    "        \n",
    "        y_pred = bert_outputs['sequence_output']\n",
    "        \n",
    "        decode_matrix = tf.linalg.pinv(self.Bert.layers[1].weights[0])\n",
    "        \n",
    "        pred_lm =  tf.math.softmax(tf.matmul(y_pred, decode_matrix))\n",
    "        pred_cls = self.dense_cls(y_pred[:, 0])\n",
    "        \n",
    "        numerator = tf.reduce_sum(label_weights * pred_lm)\n",
    "        denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "        pred_lm = numerator / denominator\n",
    "        \n",
    "        return [ pred_lm, pred_cls]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pretrainBert = BertModule(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Batch_size = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_input = pad_sequences(tf.reshape(train[0]['x'], (1, -1)), max_seq_len, padding = 'post')\n",
    "test_mask = create_padding_mask(test_input)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_y = pad_sequences(tf.reshape(train[0]['label'], (1, -1)), max_seq_len, padding = 'post')\n",
    "test_nsp = train[0]['NSP']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred_lm, pred_cls = pretrainBert({'input' : test_input, \n",
    "                                   'mask' : test_mask})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_nsp * -tf.math.log(pred_cls)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss_lm = tf.reduce_mean(tf.reduce_sum(tf.one_hot(test_y, depth = vocab_size) * -tf.math.log(pred_lm), axis = 2))\n",
    "loss_cls = tf.reduce_mean(tf.reduce_sum(test_nsp * -tf.math.log(pred_cls), axis = 1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "losses = loss_lm + loss_cls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Batch_size = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size = 5\n",
    "\n",
    "x = pad_sequences([ x['x'] for x in train ], max_seq_len, padding = 'post')\n",
    "y = pad_sequences([ x['label'] for x in train ] , max_seq_len, padding = 'post')\n",
    "nsp = [ x['NSP'] for x in train ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_x = x[:batch_size]\n",
    "batch_y = y[:batch_size]\n",
    "batch_nsp = nsp[:batch_size]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_mask = create_padding_mask(batch_x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred_lm, pred_cls = pretrainBert({'input' : batch_x, \n",
    "                                   'mask' : batch_mask})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = BertLoss()(y_true = [batch_y, batch_nsp], y_pred = [pred_lm, pred_cls])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss_LM(tf.keras.losses.Loss) :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        super(BertLoss_LM, self).__init__()\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        \n",
    "        batch_y = tf.cast(tf.one_hot(tf.cast(y_true, dtype = tf.int32), depth = vocab_size), dtype = tf.float32)\n",
    "        pred_lm = y_pred\n",
    "        \n",
    "        loss_lm = tf.reduce_mean(tf.reduce_sum(batch_y * -tf.math.log(pred_lm), axis = 2))\n",
    "#         loss_cls = tf.reduce_mean(tf.reduce_sum(batch_nsp * -tf.math.log(pred_cls), axis = 1))\n",
    "        \n",
    "        return loss_lm\n",
    "\n",
    "    \n",
    "class BertLoss_CLS(tf.keras.losses.Loss) :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        super(BertLoss_CLS, self).__init__()\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_nsp = y_true\n",
    "        pred_cls = y_pred\n",
    "        \n",
    "#         loss_lm = tf.reduce_mean(tf.reduce_sum(tf.one_hot(batch_y, depth = vocab_size) * -tf.math.log(pred_lm), axis = 2))\n",
    "        loss_cls = tf.reduce_mean(tf.reduce_sum(batch_nsp * -tf.math.log(pred_cls), axis = 1))\n",
    "        \n",
    "        return loss_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(filter(lambda x: len(x['x']) <= 130, train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000\n",
    "max_seq_len = 130\n",
    "num_layers = 3\n",
    "dff = 256\n",
    "d_model = 100\n",
    "num_heads = 5\n",
    "dropout = .1\n",
    "name = 'qbert_210602'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "x = pad_sequences([ x['x'] for x in train ], max_seq_len, padding = 'post')\n",
    "y = pad_sequences([ x['label'] for x in train ] , max_seq_len, padding = 'post')\n",
    "nsp = np.asarray([ x['NSP'] for x in train ])\n",
    "\n",
    "mask = create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 1e-3\n",
    "batch_size = 10\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "loss_fn = [ BertLoss_LM(), BertLoss_CLS() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainBert = BertModule(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainBert.compile(loss=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_6/kernel:0', 'dense_6/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_6/kernel:0', 'dense_6/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_6/kernel:0', 'dense_6/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_6/kernel:0', 'dense_6/bias:0'] when minimizing the loss.\n",
      "992/992 [==============================] - 1628s 2s/step - loss: 5.6437 - output_1_loss: 4.9171 - output_2_loss: 0.7266\n"
     ]
    }
   ],
   "source": [
    "hist = pretrainBert.fit(batch_size = batch_size, callbacks = None, epochs = epochs\n",
    "                             , x = {'input' : x,\n",
    "                                     'mask' : mask}\n",
    "                             , y = [ y, nsp ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model/BertPretrained-210602-130-3-100-5.pt\\assets\n"
     ]
    }
   ],
   "source": [
    "pretrainBert.save('./model/BertPretrained-210602-{}-{}-{}-{}.pt'.format(max_seq_len, num_layers, d_model, num_heads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set = train[np.random.randint(0, len(train))]\n",
    "sample_train_set = train[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_pkl('./dt/train_set_under_255.pkl')[np.random.randint(10000, 1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-6000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 특히 국제 분쟁 조정을 위해 북한의 김일성 , 아이티 ##의 세 ##드라 ##스 장군 , 팔레 ##인 ##스타 ##인의 하마 ##스 , 보스니아 ##의 세르비아 ##계 정권 같이 미국 정부에 대해 협상을 거부 ##하면서 사태 ##의 위기를 초래 ##한 인물 및 단체를 직접 만나 분쟁 ##의 원인을 근본 ##적으로 해결하기 위해 힘썼다 . [SEP] 넓이는 46 . 80 ##10 ##km2이고 , 인구는 2015년 8월 기준으로 5 , 66 ##2명이다 . [SEP]'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_statement = ' '.join(tokenizer_for_load.convert_ids_to_tokens(sample_train_set['label']))\n",
    "train_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_set['NSP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set = train[np.random.randint(0, len(train))]\n",
    "\n",
    "train_x = tf.reshape(sample_train_set['x'], (1, -1))\n",
    "train_x = pad_sequences(train_x, max_seq_len, padding = 'post')\n",
    "mask = create_padding_mask(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, nls = pretrainBert({\"input\" : train_x, \n",
    "                        \"mask\" : mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 130), dtype=int64, numpy=\n",
       "array([[17328, 17360, 20083,  5483,  2019, 18409,  6030, 25117,  9202,\n",
       "        14692, 26121,  3494, 27229,  4178,  2666, 25338, 13402,  1101,\n",
       "        22674, 24924,  7654, 21774,  3057, 10126, 17184, 29053,    15,\n",
       "        16914, 29689,   123, 18975,  1919, 29793, 14770, 25498, 15708,\n",
       "        22909,  9951, 27557, 13215,   965,  9987,  2350, 20155, 14333,\n",
       "         4858, 26958, 14489,  3173,  4480, 22968,  3308, 23452, 20518,\n",
       "        31073,  2973,  8616, 13132, 23579, 12193, 29040, 26558, 31106,\n",
       "        26844, 22248, 13991,  3920, 22616, 23662, 31055, 16463, 16199,\n",
       "        20434, 12297, 10795, 27195,  3488, 10549, 15916, 25133,  9855,\n",
       "         1907, 23976, 13786, 13825, 31484, 15073, 20471, 29217, 18239,\n",
       "         3095, 23133,  1123,  2396, 17538, 16958,  7579,  9146,  7860,\n",
       "        26091,  1059, 24044, 23068,  9914, 10082, 15687,  7680,  2289,\n",
       "         1450, 27952, 31394, 25124,  4905,  3505,  3409, 17645,  8407,\n",
       "        26843,   476,  1066, 17645, 30803,  2797, 20071, 31684,  3041,\n",
       "        21286,  5901, 15513, 16674]], dtype=int64)>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(lm, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.882038  , 0.11796198]], dtype=float32)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
