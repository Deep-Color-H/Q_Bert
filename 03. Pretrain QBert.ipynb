{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import train_utils, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해당 파일은 bert.run_pretraining.run_bert_pretrain을 구현하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Parameter는 FLAG 형식에서 직접 정의해주는 방식으로 변경하고, Main에서 직접 정의하도록 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Strategy 정의\n",
    "2. Input_Files 정의\n",
    "3. Bert Config 정의\n",
    "   - bert_config (1. Core_Model - Transformer Encoder - Sub Model)\n",
    "        - vocab_size\n",
    "        - type_vocab_size\n",
    "        - hidden_size\n",
    "        - max_seq_length\n",
    "        - initializer\n",
    "        - kernel_initializer\n",
    "        - initializer_range\n",
    "        - dropout_rate\n",
    "        - num_attention_heads\n",
    "        - intermediate_size\n",
    "        - intermediate_activation\n",
    "        - hidden_act\n",
    "        - attention_dropout_rate\n",
    "        - num_hidden_instances\n",
    "        - pooled_output_dim\n",
    "   - bert_config (2. Pretrained_Model - input to losses)\n",
    "        - (중복 생략)\n",
    "        - max_predictions_per_seq\n",
    "        \n",
    "3. Get Bert Model\n",
    "4. Training Config 정의\n",
    "5. Training\n",
    "6. Test\n",
    "\n",
    "\n",
    "   - init_checkpoint  # Used to initialize only the BERT submodel.\n",
    "   - max_seq_length\n",
    "   -  \n",
    "   - masked_lm\n",
    "   - model_dir\n",
    "   - num_steps_per_epoch\n",
    "   - steps_per_loop\n",
    "   - num_train_epochs\n",
    "   - learning_rate\n",
    "   - warmup_steps\n",
    "   - end_lr\n",
    "   - optimizer_type\n",
    "   - train_batch_size\n",
    "   - use_next_sentence_label\n",
    "   - train_summary_interval\n",
    "   - custom_callbacks\n",
    "   - explicit_allreduce\n",
    "   - pre_allreduce_callbacks\n",
    "   - allreduce_bytes_per_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-3000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 512 # Transformer hidden Layers\n",
    "type_vocab_size = 2 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 3\n",
    "num_attention_heads = 8\n",
    "max_seq_length = 256 # 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 512 * 4\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q_Bert00.tfrecords', 'Q_Bert01.tfrecords']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ x for x in os.listdir('.') if x.startswith(\"Q_Bert\") & x.endswith('.tfrecords') ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len([ x for x in os.listdir('.') if x.startswith(\"Q_Bert\") & x.endswith('.tfrecords') ] ) != 10 :\n",
    "    time.sleep(600)\n",
    "    \n",
    "time.sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['./Q_Bert{}.tfrecords'.format(str(x).zfill(2)) for x in range(10)]\n",
    "filenames = ['./Test_Examples3.tfrecords']\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "# keys = feature_description.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "def _select_data_from_record(record):\n",
    "    \"\"\"Filter out features to use for pretraining.\"\"\"\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids'],\n",
    "        'masked_lm_positions': record['masked_lm_positions'],\n",
    "        'masked_lm_ids': record['masked_lm_ids'],\n",
    "        'masked_lm_weights': record['masked_lm_weights'],\n",
    "    }\n",
    "    if use_next_sentence_label:\n",
    "        x['next_sentence_labels'] = record['next_sentence_labels']\n",
    "    if use_position_id:\n",
    "        x['position_ids'] = record['position_ids']\n",
    "\n",
    "    # TODO(hongkuny): Remove the fake labels after migrating bert pretraining.\n",
    "    if output_fake_labels:\n",
    "        return (x, record['masked_lm_weights'])\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "GLOBAL_BATCH_SIZE = 16\n",
    "# BATCH_SIZE_PER_REPLICA = np.ceil(GLOBAL_BATCH_SIZE // strategy.num_replicas_in_sync)\n",
    "\n",
    "use_next_sentence_label = True\n",
    "output_fake_labels = True\n",
    "use_position_id = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "train_dataset = train_dataset.interleave(tf.data.TFRecordDataset\n",
    "                                         , cycle_length = tf.data.experimental.AUTOTUNE\n",
    "                                         , num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "dataset_inputs = train_dataset.map(_parse_function,\n",
    "                                   num_parallel_calls=tf.data.experimental.AUTOTUNE) # String to Example\n",
    "dataset_inputs_with_labels = dataset_inputs.map(_select_data_from_record,\n",
    "                                                num_parallel_calls=tf.data.experimental.AUTOTUNE) # Example to InputData\n",
    "## 본래대로라면 그냥 써도 되지만, 현재 Label이 없는 데이터이기 때문에\n",
    "## max_predictions_per_seq 길이의 허위 정답 (Fake_y)를 삽입하는 mapping function이다.\n",
    "\n",
    "dataset = dataset_inputs_with_labels\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(10000, reshuffle_each_iteration = True)\n",
    "dataset = dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback\n",
    "\n",
    "## model checkpoint\n",
    "t = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "checkpoint_dir = './training_checkpoints_{}'.format(t)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "model_cp = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),\n",
    "\n",
    "## Learning Rate Print\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    model_cp\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "## Learning Rate Decay\n",
    "\n",
    "# lr = 1e-4 warmup stage (step <= 10000)\n",
    "# Decay linearly\n",
    "\n",
    "init_lr = 1e-3\n",
    "warmup_steps = 10000\n",
    "num_train_steps = 1000000\n",
    "end_lr = 0\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "      initial_learning_rate=init_lr,\n",
    "      decay_steps=num_train_steps,\n",
    "      end_learning_rate=end_lr)\n",
    "\n",
    "lr_schedule = train_utils.WarmUp(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_schedule_fn=lr_schedule,\n",
    "        warmup_steps=warmup_steps)\n",
    "\n",
    "optimizer = train_utils.AdamWeightDecay( \n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-6,\n",
    "    exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\n",
    "\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(lr = init_lr, beta_1 = 0.9, beta_2 = 0.999, epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def loss_fn(fake_y, losses, **unused_args) :\n",
    "    \n",
    "    return tf.reduce_mean(losses, axis = -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, sub_model = models.get_bert_models_fn(vocab_size\n",
    "                                         , hidden_size\n",
    "                                         , type_vocab_size\n",
    "                                         , num_layers\n",
    "                                         , num_attention_heads\n",
    "                                         , max_seq_length\n",
    "                                         , max_predictions_per_seq\n",
    "                                         , dropout_rate\n",
    "                                         , inner_dim \n",
    "                                         , initializer)\n",
    "model.compile(optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.4573 - MLM_ACC: 0.0000e+00 - MLM_LOSS: 10.5972 - NSP_ACC: 0.5625 - NSP_LOSS: 0.8601\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - ETA: 0s - loss: 11.4856 - MLM_ACC: 0.0000e+00 - MLM_LOSS: 10.6017 - NSP_ACC: 0.5000 - NSP_LOSS: 0.8839"
     ]
    }
   ],
   "source": [
    "hist = model.fit(dataset.repeat(),\n",
    "                  epochs=2, #40,\n",
    "                  callbacks=callbacks,\n",
    "                  steps_per_epoch = 1)#100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======MLM TEST======\n",
      "['기자회', '##지게', ',', '전', '라고', 'william', '시나리오', '##는', '##하다가', 'william', '관한', '##권에', '오리지널', '##와', '그의', 'william', '단지', '##에', '(', '그의', '2012년', '퍼져', '널리', '\"', '라는', '##있는', 'william', 'william', 'william', 'william', 'william', 'william', 'william', 'william', 'william', 'william', 'william', 'william', 'william', 'william']\n",
      "['기자회', '##지게', '##et', '전', '라고', '깁', ')', '##는', '##들이', 'william', '##ance', 'ne', '##ir', '##와', '##작인', '##d', '단지', '##견', '(', '그의', '2012년', '퍼져', '널리', '\"', '라는', '##있는', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "REAL_ACC : 0.6153843998908997\n"
     ]
    }
   ],
   "source": [
    "print(\"======MLM TEST======\")\n",
    "inputs, labels = next(iter(dataset))\n",
    "sub_res = model.layers[4]([inputs['input_ids'], inputs['input_mask'][:, tf.newaxis, tf.newaxis, :], inputs['segment_ids']])\n",
    "\n",
    "layer_x = model.layers[6]\n",
    "layer_x._output_type = 'predicitions'\n",
    "output_logits = layer_x([sub_res['sequence_output'], tf.cast(inputs['masked_lm_positions'], dtype = tf.int32)])\n",
    "\n",
    "prediction = tf.math.exp(output_logits)\n",
    "print(tokenizer_for_load.convert_ids_to_tokens(tf.argmax(output_logits[0], axis = 1)))\n",
    "print(tokenizer_for_load.convert_ids_to_tokens(inputs['masked_lm_ids'][0]))\n",
    "\n",
    "lm_labels = inputs['masked_lm_ids'][0]\n",
    "lm_output = output_logits[0]\n",
    "lm_label_weights = inputs['masked_lm_weights'][0]\n",
    "\n",
    "masked_lm_accuracy = tf.keras.metrics.sparse_categorical_accuracy(lm_labels, lm_output)\n",
    "numerator = tf.reduce_sum(masked_lm_accuracy * lm_label_weights)\n",
    "denominator = tf.reduce_sum(lm_label_weights) + 1e-5\n",
    "masked_lm_accuracy = numerator / denominator\n",
    "\n",
    "print(\"REAL_ACC : {}\".format(masked_lm_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
