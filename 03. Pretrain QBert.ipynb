{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\LGCNS\\Documents\\GitHub\\Q_Bert\\QBert\\train_utils.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\LGCNS\\Documents\\GitHub\\Q_Bert\\QBert\\models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import train_utils, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해당 파일은 bert.run_pretraining.run_bert_pretrain을 구현하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Parameter는 FLAG 형식에서 직접 정의해주는 방식으로 변경하고, Main에서 직접 정의하도록 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Strategy 정의\n",
    "2. Input_Files 정의\n",
    "3. Bert Config 정의\n",
    "   - bert_config (1. Core_Model - Transformer Encoder - Sub Model)\n",
    "        - vocab_size\n",
    "        - type_vocab_size\n",
    "        - hidden_size\n",
    "        - max_seq_length\n",
    "        - initializer\n",
    "        - kernel_initializer\n",
    "        - initializer_range\n",
    "        - dropout_rate\n",
    "        - num_attention_heads\n",
    "        - intermediate_size\n",
    "        - intermediate_activation\n",
    "        - hidden_act\n",
    "        - attention_dropout_rate\n",
    "        - num_hidden_instances\n",
    "        - pooled_output_dim\n",
    "   - bert_config (2. Pretrained_Model - input to losses)\n",
    "        - (중복 생략)\n",
    "        - max_predictions_per_seq\n",
    "        \n",
    "3. Get Bert Model\n",
    "4. Training Config 정의\n",
    "5. Training\n",
    "6. Test\n",
    "\n",
    "\n",
    "   - init_checkpoint  # Used to initialize only the BERT submodel.\n",
    "   - max_seq_length\n",
    "   -  \n",
    "   - masked_lm\n",
    "   - model_dir\n",
    "   - num_steps_per_epoch\n",
    "   - steps_per_loop\n",
    "   - num_train_epochs\n",
    "   - learning_rate\n",
    "   - warmup_steps\n",
    "   - end_lr\n",
    "   - optimizer_type\n",
    "   - train_batch_size\n",
    "   - use_next_sentence_label\n",
    "   - train_summary_interval\n",
    "   - custom_callbacks\n",
    "   - explicit_allreduce\n",
    "   - pre_allreduce_callbacks\n",
    "   - allreduce_bytes_per_pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices() # device 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "\n",
      "장치의 수: 1\n"
     ]
    }
   ],
   "source": [
    "# Strategy 정의\n",
    "\n",
    "distribution_strategy = 'mirrored' # 'tpu'\n",
    "num_gpus = 0\n",
    "all_reduce_alg = None\n",
    "\n",
    "if distribution_strategy == 'tpu' :\n",
    "    tpu_address = \"\"\n",
    "else :\n",
    "    tpu_address = None\n",
    "\n",
    "\n",
    "\n",
    "strategy = train_utils.get_distribution_strategy(\n",
    "                  distribution_strategy=distribution_strategy,\n",
    "                  num_gpus=num_gpus,\n",
    "                  all_reduce_alg=all_reduce_alg,\n",
    "                  tpu_address=tpu_address)\n",
    "\n",
    "print ('\\n장치의 수: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['./Test_Examples.tfrecords', './Test_Examples.tfrecords']\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([256], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([256], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([256], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([255], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([255], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([255], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "# keys = feature_description.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "BUFFER_SIZE = 672\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    train_dataset = tf.data.TFRecordDataset(filenames)\n",
    "    train_dataset = train_dataset.map(_parse_function)\n",
    "    train_dataset_iters = train_dataset.shuffle(BUFFER_SIZE).batch(1) \n",
    "    \n",
    "    train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope() :\n",
    "\n",
    "    model, sub_model = models.get_bert_models_fn(vocab_size\n",
    "                 , hidden_size\n",
    "                 , type_vocab_size\n",
    "                 , num_layers\n",
    "                 , num_attention_heads\n",
    "                 , max_seq_length\n",
    "                 , max_predictions_per_seq\n",
    "                 , dropout_rate\n",
    "                 , inner_dim \n",
    "                 , initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.training.Model object at 0x0000023DA942A940>\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_dataset_iters.take(1) :\n",
    "    print(model([inputs['input_ids'], inputs['input_mask'], inputs['segment_ids']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000\n",
    "max_seq_len = 130 # 512\n",
    "num_layers = 6 # 12\n",
    "dff = 384 * 2 # 768 * 4\n",
    "d_model = 384 # 768\n",
    "num_heads = 6 # 12\n",
    "dropout = .1\n",
    "name = 'qbert_210603'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
