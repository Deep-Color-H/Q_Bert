{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\LGCNS\\Documents\\GitHub\\Q_Bert\\QBert\\train_utils.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\LGCNS\\Documents\\GitHub\\Q_Bert\\QBert\\models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import train_utils, models\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 256 # 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['./Test_Examples.tfrecords' ]\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "# keys = feature_description.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "def _select_data_from_record(record):\n",
    "    \"\"\"Filter out features to use for pretraining.\"\"\"\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids'],\n",
    "        'masked_lm_positions': record['masked_lm_positions'],\n",
    "        'masked_lm_ids': record['masked_lm_ids'],\n",
    "        'masked_lm_weights': record['masked_lm_weights'],\n",
    "    }\n",
    "    if use_next_sentence_label:\n",
    "        x['next_sentence_labels'] = record['next_sentence_labels']\n",
    "    if use_position_id:\n",
    "        x['position_ids'] = record['position_ids']\n",
    "\n",
    "    # TODO(hongkuny): Remove the fake labels after migrating bert pretraining.\n",
    "    if output_fake_labels:\n",
    "        return (x, record['masked_lm_weights'])\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "GLOBAL_BATCH_SIZE = 4\n",
    "#BATCH_SIZE_PER_REPLICA = np.ceil(GLOBAL_BATCH_SIZE // strategy.num_replicas_in_sync)\n",
    "\n",
    "use_next_sentence_label = True\n",
    "output_fake_labels = True\n",
    "use_position_id = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "train_dataset = train_dataset.interleave(tf.data.TFRecordDataset, cycle_length = -1)\n",
    "\n",
    "dataset_inputs = train_dataset.map(_parse_function) # String to Example\n",
    "dataset_inputs_with_labels = dataset_inputs.map(_select_data_from_record) # Example to InputData\n",
    "## 본래대로라면 그냥 써도 되지만, 현재 Label이 없는 데이터이기 때문에\n",
    "## max_predictions_per_seq 길이의 허위 정답 (Fake_y)를 삽입하는 mapping function이다.\n",
    "\n",
    "dataset = dataset_inputs_with_labels\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(10000, reshuffle_each_iteration = True)\n",
    "dataset = dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "## Learning Rate Decay\n",
    "\n",
    "# lr = 1e-4 warmup stage (step <= 10000)\n",
    "# Decay linearly\n",
    "\n",
    "init_lr = 1e-4\n",
    "warmup_steps = 10000\n",
    "num_train_steps = 20 * 50000\n",
    "end_lr = 0\n",
    "\n",
    "decay_schedule_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "      initial_learning_rate=init_lr,\n",
    "      decay_steps=num_train_steps,\n",
    "      end_learning_rate=end_lr)\n",
    "\n",
    "lr_schedule = train_utils.WarmUp(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_schedule_fn=decay_schedule_fn,\n",
    "        warmup_steps=warmup_steps)\n",
    "\n",
    "optimizer = train_utils.AdamWeightDecay( \n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-6,\n",
    "    exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def loss_fn(fake_y, losses, **unused_args) :\n",
    "    \n",
    "    return tf.reduce_mean(losses, axis = -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, sub_model = models.get_bert_models_fn(vocab_size\n",
    "                                         , hidden_size\n",
    "                                         , type_vocab_size\n",
    "                                         , num_layers\n",
    "                                         , num_attention_heads\n",
    "                                         , max_seq_length\n",
    "                                         , max_predictions_per_seq\n",
    "                                         , dropout_rate\n",
    "                                         , inner_dim \n",
    "                                         , initializer)\n",
    "model.compile(optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x24cb295fe80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(filepath='./training_checkpoints_load/ckpt_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_res = sub_model([inputs['input_ids'], inputs['input_mask'][:, tf.newaxis, tf.newaxis, :], inputs['segment_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_mask (InputLayer)         [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_word_ids (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [(None, 1, 1, 256)]  0           input_mask[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_type_ids (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "qbert_encoder (Model)           multiple             110421504   input_word_ids[0][0]             \n",
      "                                                                 tf_op_layer_strided_slice_7[0][0]\n",
      "                                                                 input_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "masked_lm_positions (InputLayer [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lm_layer_3 (LmLayer)            (None, 40, 32000)    25200128    qbert_encoder[1][13]             \n",
      "                                                                 masked_lm_positions[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "masked_lm_ids (InputLayer)      [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_lm_weights (InputLayer)  [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "next_sentence_labels (InputLaye [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "classification_3 (Classificatio (None, 2)            1538        qbert_encoder[1][12]             \n",
      "__________________________________________________________________________________________________\n",
      "bert_pretrain_loss_and_metric_l (None,)              0           lm_layer_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 111,047,170\n",
      "Trainable params: 111,047,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.keras.metrics.sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0], dtype=int64)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(tf.nn.softmax(tf.math.exp(model.layers[-2](sub_res['pooled_output'][0:1]))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0], dtype=int64)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['next_sentence_labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_x = model.layers[6]\n",
    "layer_x._output_type = 'predicitions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = layer_x([sub_res['sequence_output'], tf.cast(inputs['masked_lm_positions'], dtype = tf.int32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.math.exp(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
       "array([15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 15, 15], dtype=int64)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(prediction[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
       "array([   15, 26911,  7719, 14883,    15, 14161,  7126, 24650,    75,\n",
       "        3146,  7966,    17,  3374,   134,  1779,  1779,  6353,  1779,\n",
       "        3147, 16103,  3032,  5578,  1021,    11, 12982,  5596,  5777,\n",
       "        2002,    11, 11153,  6797,  6797,  9634,  7832,  7729, 18880,\n",
       "       18880,  5621,     0,     0], dtype=int64)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['masked_lm_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-3000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', ',', ',', ',']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(tf.argmax(prediction[2], axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##부터', '밀란', '##10', '승리를', ',', '12', '보', '2월', '팔레', '차지하였다', '.', '2', '##데', '23일', 'uefa', '2003', '##에서', '부상으로', '122', '3', '기여하였다', '밀려', '##하였는데', '-', '##드', '뛰', '시즌에', '슈퍼컵', '##혔', '##차', ',', '시즌', '-', '##내며', '다비', '##코와', '##스와의', '활약', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_for_load.convert_ids_to_tokens(inputs['masked_lm_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
