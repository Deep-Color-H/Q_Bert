{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import train_utils, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해당 파일은 bert.run_pretraining.run_bert_pretrain을 구현하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Parameter는 FLAG 형식에서 직접 정의해주는 방식으로 변경하고, Main에서 직접 정의하도록 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Strategy 정의\n",
    "2. Input_Files 정의\n",
    "3. Bert Config 정의\n",
    "   - bert_config (1. Core_Model - Transformer Encoder - Sub Model)\n",
    "        - vocab_size\n",
    "        - type_vocab_size\n",
    "        - hidden_size\n",
    "        - max_seq_length\n",
    "        - initializer\n",
    "        - kernel_initializer\n",
    "        - initializer_range\n",
    "        - dropout_rate\n",
    "        - num_attention_heads\n",
    "        - intermediate_size\n",
    "        - intermediate_activation\n",
    "        - hidden_act\n",
    "        - attention_dropout_rate\n",
    "        - num_hidden_instances\n",
    "        - pooled_output_dim\n",
    "   - bert_config (2. Pretrained_Model - input to losses)\n",
    "        - (중복 생략)\n",
    "        - max_predictions_per_seq\n",
    "        \n",
    "3. Get Bert Model\n",
    "4. Training Config 정의\n",
    "5. Training\n",
    "6. Test\n",
    "\n",
    "\n",
    "   - init_checkpoint  # Used to initialize only the BERT submodel.\n",
    "   - max_seq_length\n",
    "   -  \n",
    "   - masked_lm\n",
    "   - model_dir\n",
    "   - num_steps_per_epoch\n",
    "   - steps_per_loop\n",
    "   - num_train_epochs\n",
    "   - learning_rate\n",
    "   - warmup_steps\n",
    "   - end_lr\n",
    "   - optimizer_type\n",
    "   - train_batch_size\n",
    "   - use_next_sentence_label\n",
    "   - train_summary_interval\n",
    "   - custom_callbacks\n",
    "   - explicit_allreduce\n",
    "   - pre_allreduce_callbacks\n",
    "   - allreduce_bytes_per_pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 256 # 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label에 관계없이 Loss를 Predict하는 모델이므로\n",
    "# 결과값의 reduce_mean(over batch data)을 반환한다.\n",
    "\n",
    "def loss_fn(fake_y, losses, **unused_args) :\n",
    "    \n",
    "    return tf.reduce_mean(losses, axis = -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "# keys = feature_description.keys()\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_data_from_record(record):\n",
    "    \"\"\"Filter out features to use for pretraining.\"\"\"\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids'],\n",
    "        'masked_lm_positions': record['masked_lm_positions'],\n",
    "        'masked_lm_ids': record['masked_lm_ids'],\n",
    "        'masked_lm_weights': record['masked_lm_weights'],\n",
    "    }\n",
    "    if use_next_sentence_label:\n",
    "        x['next_sentence_labels'] = record['next_sentence_labels']\n",
    "    if use_position_id:\n",
    "        x['position_ids'] = record['position_ids']\n",
    "\n",
    "    # TODO(hongkuny): Remove the fake labels after migrating bert pretraining.\n",
    "    if output_fake_labels:\n",
    "        return (x, record['masked_lm_weights'])\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dir(dataset)\n",
    "\n",
    "*'apply'* : 데이터셋 전체에 Function을 적용할 때 사용\n",
    "\n",
    "*as_numpy_iterator* : 데이터셋의 Element들을 Numpy array로 반환, list(dataset.as_numpy_iterator()) 이렇게 하면 거의 기존에 알던 데이터셋이 나옴\n",
    "\n",
    "*batch* : BatchSize 크기만큼의 Dataset Iteration을 만듬, drop_remainder를 이용하여 마지막 batch가 batch_size크기가 안되면 Drop시킬 수 있음.\n",
    "\n",
    "*cache* : Mapping 이후에, cache가 선언된다면 첫번째 Epoch를 하면서 처리했던 mapping을 유지하고 있습니다. 따라서 cache는 시간이 오래걸리지만 Memory를 많이 소비하지 않는 Function 뒤에 쓰면 좋습니다.\n",
    "\n",
    " 'filter',\n",
    "\n",
    "'from_generator',\n",
    " 'from_tensor_slices',\n",
    " 'from_tensors',\n",
    " 'interleave',\n",
    " 'list_files',\n",
    " 'map',\n",
    "*prefetch* : \n",
    " 'range',\n",
    " 'reduce',\n",
    " 'repeat',\n",
    " 'shard',\n",
    " 'shuffle',\n",
    " 'skip',\n",
    " 'take',\n",
    " 'unbatch',\n",
    " 'window',\n",
    " 'with_options',\n",
    " 'zip']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Ideal Dataset 처리과정\n",
    "\n",
    "optimized_timeline = timelined_benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(  # 데이터 읽기 병렬화\n",
    "        dataset_generator_fun,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(  # 매핑된 함수 벡터화\n",
    "        _batch_map_num_items,\n",
    "        drop_remainder=True)\n",
    "    .map(  # 맵 변환 병렬화\n",
    "        time_consumming_map,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .cache()  # 데이터 캐시\n",
    "    .map(  # 메모리 사용량 줄이기\n",
    "        memory_consumming_map,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .prefetch(  # 프로듀서와 컨슈머 작업 오버랩\n",
    "        tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .unbatch(),\n",
    "    5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_next_sentence_label = True\n",
    "output_fake_labels = True\n",
    "use_position_id = False\n",
    "\n",
    "filenames = ['./Test_Examples.tfrecords']\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "train_dataset = train_dataset.interleave(tf.data.TFRecordDataset,\n",
    "                                         cycle_length = -1)\n",
    "\n",
    "# train_dataset = train_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_inputs = train_dataset.map(_parse_function) # String to Example\n",
    "dataset_inputs_with_labels = dataset_inputs.map(_select_data_from_record) # Example to InputData\n",
    "## 본래대로라면 그냥 써도 되지만, 현재 Label이 없는 데이터이기 때문에\n",
    "## max_predictions_per_seq 길이의 허위 정답 (Fake_y)를 삽입하는 mapping function이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_inputs_with_labels\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(10000, reshuffle_each_iteration = True)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, sub_model = models.get_bert_models_fn(vocab_size\n",
    "                                             , hidden_size\n",
    "                                             , type_vocab_size\n",
    "                                             , num_layers\n",
    "                                             , num_attention_heads\n",
    "                                             , max_seq_length\n",
    "                                             , max_predictions_per_seq\n",
    "                                             , dropout_rate\n",
    "                                             , inner_dim \n",
    "                                             , initializer)\n",
    "\n",
    "#####################################################################################################################\n",
    "# model : pretrained model\n",
    "#  - Input들을 받아서 Loss까지 계산한다.\n",
    "# sub_model : bert_encoder\n",
    "#  - input_ids, input_mask, segmend_id를 주면 Encoding한 결과를 가져온다.\n",
    "#  - outputs = ['sequence_output' : 마지막 Encoder Layer의 결과값들이 기록\n",
    "#               'hidden_states' : 모든 Encoder Layer의 결과값들이 기록\n",
    "#               'pooled_output' : 마지막 Encoder Layer의 첫번째 예측결과에 Dense를 추가한 output ]\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "optimizer = tf.keras.optimizers.Adam(lr = 1e-3)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss=loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 체크포인트를 저장할 체크포인트 디렉터리를 지정합니다.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# 체크포인트 파일의 이름\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=1)\n",
    "\n",
    "# 학습률을 점점 줄이기 위한 함수\n",
    "# 필요한 함수를 직접 정의하여 사용할 수 있습니다.\n",
    "def decay(epoch):\n",
    "    if epoch < 3:\n",
    "        return 1e-3\n",
    "    elif epoch >= 3 and epoch < 7:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-5\n",
    "\n",
    "callbacks = [\n",
    "#     tensorboard_callback\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True)\n",
    "    , tf.keras.callbacks.LearningRateScheduler(decay)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer MultiHeadAttention has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 154s 15s/step - loss: 9.4618 - masked_lm_accuracy: 0.0320 - lm_example_loss: 8.2965 - next_sentence_accuracy: 0.4625 - next_sentence_loss: 1.1652 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 155s 16s/step - loss: 9.3454 - masked_lm_accuracy: 0.0417 - lm_example_loss: 8.3828 - next_sentence_accuracy: 0.5000 - next_sentence_loss: 0.9626 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 155s 15s/step - loss: 9.4548 - masked_lm_accuracy: 0.0354 - lm_example_loss: 8.6259 - next_sentence_accuracy: 0.5875 - next_sentence_loss: 0.8289 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 155s 16s/step - loss: 9.1867 - masked_lm_accuracy: 0.0258 - lm_example_loss: 8.3890 - next_sentence_accuracy: 0.5125 - next_sentence_loss: 0.7977 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 157s 16s/step - loss: 9.3062 - masked_lm_accuracy: 0.0343 - lm_example_loss: 8.6084 - next_sentence_accuracy: 0.6125 - next_sentence_loss: 0.6978 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(dataset, epochs = 5, callbacks=callbacks, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices() # device 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "\n",
      "장치의 수: 1\n"
     ]
    }
   ],
   "source": [
    "# Strategy 정의\n",
    "\n",
    "distribution_strategy = 'mirrored' # 'tpu'\n",
    "num_gpus = 0\n",
    "all_reduce_alg = None\n",
    "\n",
    "if distribution_strategy == 'tpu' :\n",
    "    tpu_address = \"\"\n",
    "else :\n",
    "    tpu_address = None\n",
    "\n",
    "\n",
    "\n",
    "strategy = train_utils.get_distribution_strategy(\n",
    "                  distribution_strategy=distribution_strategy,\n",
    "                  num_gpus=num_gpus,\n",
    "                  all_reduce_alg=all_reduce_alg,\n",
    "                  tpu_address=tpu_address)\n",
    "\n",
    "print ('\\n장치의 수: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_model (core_model 필요 Config)\n",
    "\n",
    "vocab_size = 32000 # \n",
    "hidden_size = 768 # Transformer hidden Layers\n",
    "type_vocab_size = 12 #: The number of types that the 'type_ids' input can take.\n",
    "num_layers = 12\n",
    "num_attention_heads = 12\n",
    "max_seq_length = 256 # 512\n",
    "dropout_rate = .1\n",
    "# attention_dropout_rate = .1\n",
    "inner_dim = 3072\n",
    "# hidden_act = 'gelu'\n",
    "initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "# Pretrain Model 필요 Config\n",
    "max_predictions_per_seq = 40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['./Test_Examples.tfrecords']\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "    'masked_lm_positions': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_ids': tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "    'masked_lm_weights': tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "    'next_sentence_labels': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "\n",
    "# keys = feature_description.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "def _select_data_from_record(record):\n",
    "    \"\"\"Filter out features to use for pretraining.\"\"\"\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids'],\n",
    "        'masked_lm_positions': record['masked_lm_positions'],\n",
    "        'masked_lm_ids': record['masked_lm_ids'],\n",
    "        'masked_lm_weights': record['masked_lm_weights'],\n",
    "    }\n",
    "    if use_next_sentence_label:\n",
    "        x['next_sentence_labels'] = record['next_sentence_labels']\n",
    "    if use_position_id:\n",
    "        x['position_ids'] = record['position_ids']\n",
    "\n",
    "    # TODO(hongkuny): Remove the fake labels after migrating bert pretraining.\n",
    "    if output_fake_labels:\n",
    "        return (x, record['masked_lm_weights'])\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = 672\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 32\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "use_next_sentence_label = True\n",
    "output_fake_labels = True\n",
    "use_position_id = False\n",
    "\n",
    "batch_size = 8\n",
    "lr = 1e-3\n",
    "\n",
    "filenames = ['./Test_Examples.tfrecords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    train_dataset = train_dataset.interleave(tf.data.TFRecordDataset, cycle_length = -1)\n",
    "    \n",
    "    dataset_inputs = train_dataset.map(_parse_function) # String to Example\n",
    "    dataset_inputs_with_labels = dataset_inputs.map(_select_data_from_record) # Example to InputData\n",
    "    ## 본래대로라면 그냥 써도 되지만, 현재 Label이 없는 데이터이기 때문에\n",
    "    ## max_predictions_per_seq 길이의 허위 정답 (Fake_y)를 삽입하는 mapping function이다.\n",
    "    \n",
    "    dataset = dataset_inputs_with_labels\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(10000, reshuffle_each_iteration = True)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    train_dist_dataset = strategy.experimental_distribute_dataset(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5e7c0bdbfd92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     model, sub_model = models.get_bert_models_fn(vocab_size\n\u001b[0;32m      4\u001b[0m                  \u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[1;33m,\u001b[0m \u001b[0mtype_vocab_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'strategy' is not defined"
     ]
    }
   ],
   "source": [
    "with strategy.scope() :\n",
    "\n",
    "    model, sub_model = models.get_bert_models_fn(vocab_size\n",
    "                                             , hidden_size\n",
    "                                             , type_vocab_size\n",
    "                                             , num_layers\n",
    "                                             , num_attention_heads\n",
    "                                             , max_seq_length\n",
    "                                             , max_predictions_per_seq\n",
    "                                             , dropout_rate\n",
    "                                             , inner_dim \n",
    "                                             , initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:541 train_step  **\n        self.trainable_variables)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1804 _minimize\n        trainable_variables))\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:521 _aggregate_gradients\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:1219 _filter_grads\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['embedding_2/embeddings:0', 'position_embedding_1/embeddings:0', 'embedding_3/embeddings:0', 'layer_normalization_25/gamma:0', 'layer_normalization_25/beta:0', 'multi_head_attention_12/dense_72/kernel:0', 'multi_head_attention_12/dense_72/bias:0', 'multi_head_attention_12/dense_73/kernel:0', 'multi_head_attention_12/dense_73/bias:0', 'multi_head_attention_12/dense_74/kernel:0', 'multi_head_attention_12/dense_74/bias:0', 'multi_head_attention_12/dense_75/kernel:0', 'multi_head_attention_12/dense_75/bias:0', 'layer_normalization_26/gamma:0', 'layer_normalization_26/beta:0', 'dense_76/kernel:0', 'dense_76/bias:0', 'dense_77/kernel:0', 'dense_77/bias:0', 'layer_normalization_27/gamma:0', 'layer_normalization_27/beta:0', 'multi_head_attention_13/dense_78/kernel:0', 'multi_head_attention_13/dense_78/bias:0', 'multi_head_attention_13/dense_79/kernel:0', 'multi_head_attention_13/dense_79/bias:0', 'multi_head_attention_13/dense_80/kernel:0', 'multi_head_attention_13/dense_80/bias:0', 'multi_head_attention_13/dense_81/kernel:0', 'multi_head_attention_13/dense_81/bias:0', 'layer_normalization_28/gamma:0', 'layer_normalization_28/beta:0', 'dense_82/kernel:0', 'dense_82/bias:0', 'dense_83/kernel:0', 'dense_83/bias:0', 'layer_normalization_29/gamma:0', 'layer_normalization_29/beta:0', 'multi_head_attention_14/dense_84/kernel:0', 'multi_head_attention_14/dense_84/bias:0', 'multi_head_attention_14/dense_85/kernel:0', 'multi_head_attention_14/dense_85/bias:0', 'multi_head_attention_14/dense_86/kernel:0', 'multi_head_attention_14/dense_86/bias:0', 'multi_head_attention_14/dense_87/kernel:0', 'multi_head_attention_14/dense_87/bias:0', 'layer_normalization_30/gamma:0', 'layer_normalization_30/beta:0', 'dense_88/kernel:0', 'dense_88/bias:0', 'dense_89/kernel:0', 'dense_89/bias:0', 'layer_normalization_31/gamma:0', 'layer_normalization_31/beta:0', 'multi_head_attention_15/dense_90/kernel:0', 'multi_head_attention_15/dense_90/bias:0', 'multi_head_attention_15/dense_91/kernel:0', 'multi_head_attention_15/dense_91/bias:0', 'multi_head_attention_15/dense_92/kernel:0', 'multi_head_attention_15/dense_92/bias:0', 'multi_head_attention_15/dense_93/kernel:0', 'multi_head_attention_15/dense_93/bias:0', 'layer_normalization_32/gamma:0', 'layer_normalization_32/beta:0', 'dense_94/kernel:0', 'dense_94/bias:0', 'dense_95/kernel:0', 'dense_95/bias:0', 'layer_normalization_33/gamma:0', 'layer_normalization_33/beta:0', 'multi_head_attention_16/dense_96/kernel:0', 'multi_head_attention_16/dense_96/bias:0', 'multi_head_attention_16/dense_97/kernel:0', 'multi_head_attention_16/dense_97/bias:0', 'multi_head_attention_16/dense_98/kernel:0', 'multi_head_attention_16/dense_98/bias:0', 'multi_head_attention_16/dense_99/kernel:0', 'multi_head_attention_16/dense_99/bias:0', 'layer_normalization_34/gamma:0', 'layer_normalization_34/beta:0', 'dense_100/kernel:0', 'dense_100/bias:0', 'dense_101/kernel:0', 'dense_101/bias:0', 'layer_normalization_35/gamma:0', 'layer_normalization_35/beta:0', 'multi_head_attention_17/dense_102/kernel:0', 'multi_head_attention_17/dense_102/bias:0', 'multi_head_attention_17/dense_103/kernel:0', 'multi_head_attention_17/dense_103/bias:0', 'multi_head_attention_17/dense_104/kernel:0', 'multi_head_attention_17/dense_104/bias:0', 'multi_head_attention_17/dense_105/kernel:0', 'multi_head_attention_17/dense_105/bias:0', 'layer_normalization_36/gamma:0', 'layer_normalization_36/beta:0', 'dense_106/kernel:0', 'dense_106/bias:0', 'dense_107/kernel:0', 'dense_107/bias:0', 'layer_normalization_37/gamma:0', 'layer_normalization_37/beta:0', 'multi_head_attention_18/dense_108/kernel:0', 'multi_head_attention_18/dense_108/bias:0', 'multi_head_attention_18/dense_109/kernel:0', 'multi_head_attention_18/dense_109/bias:0', 'multi_head_attention_18/dense_110/kernel:0', 'multi_head_attention_18/dense_110/bias:0', 'multi_head_attention_18/dense_111/kernel:0', 'multi_head_attention_18/dense_111/bias:0', 'layer_normalization_38/gamma:0', 'layer_normalization_38/beta:0', 'dense_112/kernel:0', 'dense_112/bias:0', 'dense_113/kernel:0', 'dense_113/bias:0', 'layer_normalization_39/gamma:0', 'layer_normalization_39/beta:0', 'multi_head_attention_19/dense_114/kernel:0', 'multi_head_attention_19/dense_114/bias:0', 'multi_head_attention_19/dense_115/kernel:0', 'multi_head_attention_19/dense_115/bias:0', 'multi_head_attention_19/dense_116/kernel:0', 'multi_head_attention_19/dense_116/bias:0', 'multi_head_attention_19/dense_117/kernel:0', 'multi_head_attention_19/dense_117/bias:0', 'layer_normalization_40/gamma:0', 'layer_normalization_40/beta:0', 'dense_118/kernel:0', 'dense_118/bias:0', 'dense_119/kernel:0', 'dense_119/bias:0', 'layer_normalization_41/gamma:0', 'layer_normalization_41/beta:0', 'multi_head_attention_20/dense_120/kernel:0', 'multi_head_attention_20/dense_120/bias:0', 'multi_head_attention_20/dense_121/kernel:0', 'multi_head_attention_20/dense_121/bias:0', 'multi_head_attention_20/dense_122/kernel:0', 'multi_head_attention_20/dense_122/bias:0', 'multi_head_attention_20/dense_123/kernel:0', 'multi_head_attention_20/dense_123/bias:0', 'layer_normalization_42/gamma:0', 'layer_normalization_42/beta:0', 'dense_124/kernel:0', 'dense_124/bias:0', 'dense_125/kernel:0', 'dense_125/bias:0', 'layer_normalization_43/gamma:0', 'layer_normalization_43/beta:0', 'multi_head_attention_21/dense_126/kernel:0', 'multi_head_attention_21/dense_126/bias:0', 'multi_head_attention_21/dense_127/kernel:0', 'multi_head_attention_21/dense_127/bias:0', 'multi_head_attention_21/dense_128/kernel:0', 'multi_head_attention_21/dense_128/bias:0', 'multi_head_attention_21/dense_129/kernel:0', 'multi_head_attention_21/dense_129/bias:0', 'layer_normalization_44/gamma:0', 'layer_normalization_44/beta:0', 'dense_130/kernel:0', 'dense_130/bias:0', 'dense_131/kernel:0', 'dense_131/bias:0', 'layer_normalization_45/gamma:0', 'layer_normalization_45/beta:0', 'multi_head_attention_22/dense_132/kernel:0', 'multi_head_attention_22/dense_132/bias:0', 'multi_head_attention_22/dense_133/kernel:0', 'multi_head_attention_22/dense_133/bias:0', 'multi_head_attention_22/dense_134/kernel:0', 'multi_head_attention_22/dense_134/bias:0', 'multi_head_attention_22/dense_135/kernel:0', 'multi_head_attention_22/dense_135/bias:0', 'layer_normalization_46/gamma:0', 'layer_normalization_46/beta:0', 'dense_136/kernel:0', 'dense_136/bias:0', 'dense_137/kernel:0', 'dense_137/bias:0', 'layer_normalization_47/gamma:0', 'layer_normalization_47/beta:0', 'multi_head_attention_23/dense_138/kernel:0', 'multi_head_attention_23/dense_138/bias:0', 'multi_head_attention_23/dense_139/kernel:0', 'multi_head_attention_23/dense_139/bias:0', 'multi_head_attention_23/dense_140/kernel:0', 'multi_head_attention_23/dense_140/bias:0', 'multi_head_attention_23/dense_141/kernel:0', 'multi_head_attention_23/dense_141/bias:0', 'layer_normalization_48/gamma:0', 'layer_normalization_48/beta:0', 'dense_142/kernel:0', 'dense_142/bias:0', 'dense_143/kernel:0', 'dense_143/bias:0', 'layer_normalization_49/gamma:0', 'layer_normalization_49/beta:0', 'pooler_layer_1/kernel:0', 'pooler_layer_1/bias:0', 'transform/bias:0', 'lm_layer_1/transform/dense/kernel:0', 'lm_layer_1/transform/dense/bias:0', 'lm_layer_1/transform/LayerNorm/gamma:0', 'lm_layer_1/transform/LayerNorm/beta:0', 'predictions/transform/logits_1/kernel:0', 'predictions/transform/logits_1/bias:0'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-89147f218dcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1346\u001b[0m                                                     class_weight)\n\u001b[0;32m   1347\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    616\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2772\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 2774\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[0;32m   2705\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 2706\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:541 train_step  **\n        self.trainable_variables)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1804 _minimize\n        trainable_variables))\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:521 _aggregate_gradients\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\n    C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:1219 _filter_grads\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['embedding_2/embeddings:0', 'position_embedding_1/embeddings:0', 'embedding_3/embeddings:0', 'layer_normalization_25/gamma:0', 'layer_normalization_25/beta:0', 'multi_head_attention_12/dense_72/kernel:0', 'multi_head_attention_12/dense_72/bias:0', 'multi_head_attention_12/dense_73/kernel:0', 'multi_head_attention_12/dense_73/bias:0', 'multi_head_attention_12/dense_74/kernel:0', 'multi_head_attention_12/dense_74/bias:0', 'multi_head_attention_12/dense_75/kernel:0', 'multi_head_attention_12/dense_75/bias:0', 'layer_normalization_26/gamma:0', 'layer_normalization_26/beta:0', 'dense_76/kernel:0', 'dense_76/bias:0', 'dense_77/kernel:0', 'dense_77/bias:0', 'layer_normalization_27/gamma:0', 'layer_normalization_27/beta:0', 'multi_head_attention_13/dense_78/kernel:0', 'multi_head_attention_13/dense_78/bias:0', 'multi_head_attention_13/dense_79/kernel:0', 'multi_head_attention_13/dense_79/bias:0', 'multi_head_attention_13/dense_80/kernel:0', 'multi_head_attention_13/dense_80/bias:0', 'multi_head_attention_13/dense_81/kernel:0', 'multi_head_attention_13/dense_81/bias:0', 'layer_normalization_28/gamma:0', 'layer_normalization_28/beta:0', 'dense_82/kernel:0', 'dense_82/bias:0', 'dense_83/kernel:0', 'dense_83/bias:0', 'layer_normalization_29/gamma:0', 'layer_normalization_29/beta:0', 'multi_head_attention_14/dense_84/kernel:0', 'multi_head_attention_14/dense_84/bias:0', 'multi_head_attention_14/dense_85/kernel:0', 'multi_head_attention_14/dense_85/bias:0', 'multi_head_attention_14/dense_86/kernel:0', 'multi_head_attention_14/dense_86/bias:0', 'multi_head_attention_14/dense_87/kernel:0', 'multi_head_attention_14/dense_87/bias:0', 'layer_normalization_30/gamma:0', 'layer_normalization_30/beta:0', 'dense_88/kernel:0', 'dense_88/bias:0', 'dense_89/kernel:0', 'dense_89/bias:0', 'layer_normalization_31/gamma:0', 'layer_normalization_31/beta:0', 'multi_head_attention_15/dense_90/kernel:0', 'multi_head_attention_15/dense_90/bias:0', 'multi_head_attention_15/dense_91/kernel:0', 'multi_head_attention_15/dense_91/bias:0', 'multi_head_attention_15/dense_92/kernel:0', 'multi_head_attention_15/dense_92/bias:0', 'multi_head_attention_15/dense_93/kernel:0', 'multi_head_attention_15/dense_93/bias:0', 'layer_normalization_32/gamma:0', 'layer_normalization_32/beta:0', 'dense_94/kernel:0', 'dense_94/bias:0', 'dense_95/kernel:0', 'dense_95/bias:0', 'layer_normalization_33/gamma:0', 'layer_normalization_33/beta:0', 'multi_head_attention_16/dense_96/kernel:0', 'multi_head_attention_16/dense_96/bias:0', 'multi_head_attention_16/dense_97/kernel:0', 'multi_head_attention_16/dense_97/bias:0', 'multi_head_attention_16/dense_98/kernel:0', 'multi_head_attention_16/dense_98/bias:0', 'multi_head_attention_16/dense_99/kernel:0', 'multi_head_attention_16/dense_99/bias:0', 'layer_normalization_34/gamma:0', 'layer_normalization_34/beta:0', 'dense_100/kernel:0', 'dense_100/bias:0', 'dense_101/kernel:0', 'dense_101/bias:0', 'layer_normalization_35/gamma:0', 'layer_normalization_35/beta:0', 'multi_head_attention_17/dense_102/kernel:0', 'multi_head_attention_17/dense_102/bias:0', 'multi_head_attention_17/dense_103/kernel:0', 'multi_head_attention_17/dense_103/bias:0', 'multi_head_attention_17/dense_104/kernel:0', 'multi_head_attention_17/dense_104/bias:0', 'multi_head_attention_17/dense_105/kernel:0', 'multi_head_attention_17/dense_105/bias:0', 'layer_normalization_36/gamma:0', 'layer_normalization_36/beta:0', 'dense_106/kernel:0', 'dense_106/bias:0', 'dense_107/kernel:0', 'dense_107/bias:0', 'layer_normalization_37/gamma:0', 'layer_normalization_37/beta:0', 'multi_head_attention_18/dense_108/kernel:0', 'multi_head_attention_18/dense_108/bias:0', 'multi_head_attention_18/dense_109/kernel:0', 'multi_head_attention_18/dense_109/bias:0', 'multi_head_attention_18/dense_110/kernel:0', 'multi_head_attention_18/dense_110/bias:0', 'multi_head_attention_18/dense_111/kernel:0', 'multi_head_attention_18/dense_111/bias:0', 'layer_normalization_38/gamma:0', 'layer_normalization_38/beta:0', 'dense_112/kernel:0', 'dense_112/bias:0', 'dense_113/kernel:0', 'dense_113/bias:0', 'layer_normalization_39/gamma:0', 'layer_normalization_39/beta:0', 'multi_head_attention_19/dense_114/kernel:0', 'multi_head_attention_19/dense_114/bias:0', 'multi_head_attention_19/dense_115/kernel:0', 'multi_head_attention_19/dense_115/bias:0', 'multi_head_attention_19/dense_116/kernel:0', 'multi_head_attention_19/dense_116/bias:0', 'multi_head_attention_19/dense_117/kernel:0', 'multi_head_attention_19/dense_117/bias:0', 'layer_normalization_40/gamma:0', 'layer_normalization_40/beta:0', 'dense_118/kernel:0', 'dense_118/bias:0', 'dense_119/kernel:0', 'dense_119/bias:0', 'layer_normalization_41/gamma:0', 'layer_normalization_41/beta:0', 'multi_head_attention_20/dense_120/kernel:0', 'multi_head_attention_20/dense_120/bias:0', 'multi_head_attention_20/dense_121/kernel:0', 'multi_head_attention_20/dense_121/bias:0', 'multi_head_attention_20/dense_122/kernel:0', 'multi_head_attention_20/dense_122/bias:0', 'multi_head_attention_20/dense_123/kernel:0', 'multi_head_attention_20/dense_123/bias:0', 'layer_normalization_42/gamma:0', 'layer_normalization_42/beta:0', 'dense_124/kernel:0', 'dense_124/bias:0', 'dense_125/kernel:0', 'dense_125/bias:0', 'layer_normalization_43/gamma:0', 'layer_normalization_43/beta:0', 'multi_head_attention_21/dense_126/kernel:0', 'multi_head_attention_21/dense_126/bias:0', 'multi_head_attention_21/dense_127/kernel:0', 'multi_head_attention_21/dense_127/bias:0', 'multi_head_attention_21/dense_128/kernel:0', 'multi_head_attention_21/dense_128/bias:0', 'multi_head_attention_21/dense_129/kernel:0', 'multi_head_attention_21/dense_129/bias:0', 'layer_normalization_44/gamma:0', 'layer_normalization_44/beta:0', 'dense_130/kernel:0', 'dense_130/bias:0', 'dense_131/kernel:0', 'dense_131/bias:0', 'layer_normalization_45/gamma:0', 'layer_normalization_45/beta:0', 'multi_head_attention_22/dense_132/kernel:0', 'multi_head_attention_22/dense_132/bias:0', 'multi_head_attention_22/dense_133/kernel:0', 'multi_head_attention_22/dense_133/bias:0', 'multi_head_attention_22/dense_134/kernel:0', 'multi_head_attention_22/dense_134/bias:0', 'multi_head_attention_22/dense_135/kernel:0', 'multi_head_attention_22/dense_135/bias:0', 'layer_normalization_46/gamma:0', 'layer_normalization_46/beta:0', 'dense_136/kernel:0', 'dense_136/bias:0', 'dense_137/kernel:0', 'dense_137/bias:0', 'layer_normalization_47/gamma:0', 'layer_normalization_47/beta:0', 'multi_head_attention_23/dense_138/kernel:0', 'multi_head_attention_23/dense_138/bias:0', 'multi_head_attention_23/dense_139/kernel:0', 'multi_head_attention_23/dense_139/bias:0', 'multi_head_attention_23/dense_140/kernel:0', 'multi_head_attention_23/dense_140/bias:0', 'multi_head_attention_23/dense_141/kernel:0', 'multi_head_attention_23/dense_141/bias:0', 'layer_normalization_48/gamma:0', 'layer_normalization_48/beta:0', 'dense_142/kernel:0', 'dense_142/bias:0', 'dense_143/kernel:0', 'dense_143/bias:0', 'layer_normalization_49/gamma:0', 'layer_normalization_49/beta:0', 'pooler_layer_1/kernel:0', 'pooler_layer_1/bias:0', 'transform/bias:0', 'lm_layer_1/transform/dense/kernel:0', 'lm_layer_1/transform/dense/bias:0', 'lm_layer_1/transform/LayerNorm/gamma:0', 'lm_layer_1/transform/LayerNorm/beta:0', 'predictions/transform/logits_1/kernel:0', 'predictions/transform/logits_1/bias:0'].\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
