{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from QBert.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "import import_ipynb\n",
    "from QBert import qbert_model\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_pkl(file_path) :\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def save_pkl(df, file_path) :\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, key의 문장 길이)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def ind_to_weight(masked_pos, seq_len) :\n",
    "    return tf.reduce_sum(tf.one_hot(masked_pos, seq_len), axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = load_pkl('./dt/train_set_under_255.pkl')\n",
    "train = load_pkl('./dt/train_set-maksed-position-sample-10000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000\n",
    "max_seq_len = 255\n",
    "num_layers = 12\n",
    "dff = 768\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "dropout = .1\n",
    "name = 'qbert_210602'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BertModule(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name) :\n",
    "    \n",
    "    x = tf.keras.Input(shape = (None, ), name = 'MASKED_X_INPUT')\n",
    "    mask = tf.keras.Input(shape = (1, 1, None), name = 'MASKING_INPUT')\n",
    "    lm = tf.keras.Input(shape = (max_seq_len, vocab_size), name = 'WORD_LABEING_INPUT')\n",
    "    nsp = tf.keras.Input(shape = (2, ), name = 'NEXT_SENTENCE_PREDICTION_INPUT')\n",
    "    weight = tf.keras.Input(shape = (max_seq_len, ), name = 'MASKING_WEIGHT')\n",
    "    \n",
    "    \n",
    "    Bert = qbert_model(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)\n",
    "    dense_cls = tf.keras.layers.Dense(2, activation = 'softmax', use_bias = False)\n",
    "\n",
    "    bert_outputs = Bert([x, mask])\n",
    "\n",
    "    y_pred = bert_outputs['sequence_output']\n",
    "\n",
    "    decode_matrix = tf.linalg.pinv(Bert.layers[1].weights[0])\n",
    "\n",
    "    pred_lm =  tf.math.softmax(tf.matmul(y_pred, decode_matrix))\n",
    "    \n",
    "    pred_cls = dense_cls(y_pred[:, 0])\n",
    "\n",
    "    true_y_lm = tf.cast(tf.one_hot(tf.cast(lm, dtype = tf.int32), depth = vocab_size), dtype = tf.float32)\n",
    "    lm_losses = tf.reduce_mean(tf.reduce_sum(true_y_lm * -tf.math.log(pred_lm), axis = 2))\n",
    "    \n",
    "    lm_losses = lm_losses * weight\n",
    "    \n",
    "    cls_losses = tf.reduce_mean(tf.reduce_sum(nsp * -tf.math.log(pred_cls), axis = 1))\n",
    "    \n",
    "    total_loss = lm_losses + cls_losses\n",
    "    \n",
    "    return tf.keras.Model([x, mask, lm, nsp, weight], total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainBert = BertModule(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss_LM(tf.keras.losses.Loss) :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        super(BertLoss_LM, self).__init__()\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        \n",
    "        lm_y = y_true['y']\n",
    "        weight = y_true['weight']\n",
    "        \n",
    "        print(lm_y.shape)\n",
    "        print(weight.shape)\n",
    "        \n",
    "        batch_y = tf.cast(tf.one_hot(tf.cast(y_true, dtype = tf.int32), depth = vocab_size), dtype = tf.float32)\n",
    "        pred_lm = y_pred\n",
    "        \n",
    "        loss_lm = tf.reduce_mean(tf.reduce_sum(batch_y * -tf.math.log(pred_lm), axis = 2))\n",
    "#         loss_cls = tf.reduce_mean(tf.reduce_sum(batch_nsp * -tf.math.log(pred_cls), axis = 1))\n",
    "        \n",
    "        return loss_lm\n",
    "\n",
    "    \n",
    "class BertLoss_CLS(tf.keras.losses.Loss) :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        super(BertLoss_CLS, self).__init__()\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_nsp = y_true\n",
    "        pred_cls = y_pred\n",
    "        \n",
    "#         loss_lm = tf.reduce_mean(tf.reduce_sum(tf.one_hot(batch_y, depth = vocab_size) * -tf.math.log(pred_lm), axis = 2))\n",
    "        loss_cls = tf.reduce_mean(tf.reduce_sum(batch_nsp * -tf.math.log(pred_cls), axis = 1))\n",
    "        \n",
    "        return loss_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertModule(tf.keras.Model) :\n",
    "    \n",
    "#     def __init__(self, vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name) :\n",
    "#         super(BertModule, self).__init__()\n",
    "        \n",
    "#         self.max_seq_len = max_seq_len\n",
    "#         self.d_model = d_model\n",
    "        \n",
    "#         self.Bert = qbert_model(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)\n",
    "        \n",
    "#         self.dense_cls = tf.keras.layers.Dense(2, activation = 'softmax', use_bias = False)\n",
    "    \n",
    "#     def call(self, inputs) :\n",
    "        \n",
    "#         x, mask, lm, nsp, weight = inputs['input'], inputs['mask'], inputs['lm'], inputs['nsp'], inputs['weight']\n",
    "        \n",
    "#         batch_size = tf.shape(input)[0]\n",
    "        \n",
    "#         bert_outputs = self.Bert([input, mask])\n",
    "        \n",
    "#         y_pred = bert_outputs['sequence_output']\n",
    "        \n",
    "#         decode_matrix = tf.linalg.pinv(self.Bert.layers[1].weights[0])\n",
    "        \n",
    "#         pred_lm =  tf.math.softmax(tf.matmul(y_pred, decode_matrix))\n",
    "#         pred_cls = self.dense_cls(y_pred[:, 0])\n",
    "        \n",
    "#         return [ pred_lm, pred_cls]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pretrainBert = BertModule(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Batch_size = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_input = pad_sequences(tf.reshape(train[0]['x'], (1, -1)), max_seq_len, padding = 'post')\n",
    "test_mask = create_padding_mask(test_input)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_y = pad_sequences(tf.reshape(train[0]['label'], (1, -1)), max_seq_len, padding = 'post')\n",
    "test_nsp = train[0]['NSP']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred_lm, pred_cls = pretrainBert({'input' : test_input, \n",
    "                                   'mask' : test_mask})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_nsp * -tf.math.log(pred_cls)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss_lm = tf.reduce_mean(tf.reduce_sum(tf.one_hot(test_y, depth = vocab_size) * -tf.math.log(pred_lm), axis = 2))\n",
    "loss_cls = tf.reduce_mean(tf.reduce_sum(test_nsp * -tf.math.log(pred_cls), axis = 1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "losses = loss_lm + loss_cls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Batch_size = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size = 5\n",
    "\n",
    "x = pad_sequences([ x['x'] for x in train ], max_seq_len, padding = 'post')\n",
    "y = pad_sequences([ x['label'] for x in train ] , max_seq_len, padding = 'post')\n",
    "nsp = [ x['NSP'] for x in train ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_x = x[:batch_size]\n",
    "batch_y = y[:batch_size]\n",
    "batch_nsp = nsp[:batch_size]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_mask = create_padding_mask(batch_x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred_lm, pred_cls = pretrainBert({'input' : batch_x, \n",
    "                                   'mask' : batch_mask})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = BertLoss()(y_true = [batch_y, batch_nsp], y_pred = [pred_lm, pred_cls])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertLoss_LM(tf.keras.losses.Loss) :\n",
    "    \n",
    "#     def __init__(self) :\n",
    "#         super(BertLoss_LM, self).__init__()\n",
    "        \n",
    "#     def call(self, y_true, y_pred):\n",
    "        \n",
    "#         lm_y = y_true['y']\n",
    "#         weight = y_true['weight']\n",
    "        \n",
    "#         print(lm_y.shape)\n",
    "#         print(weight.shape)\n",
    "        \n",
    "#         batch_y = tf.cast(tf.one_hot(tf.cast(y_true, dtype = tf.int32), depth = vocab_size), dtype = tf.float32)\n",
    "#         pred_lm = y_pred\n",
    "        \n",
    "#         loss_lm = tf.reduce_mean(tf.reduce_sum(batch_y * -tf.math.log(pred_lm), axis = 2))\n",
    "# #         loss_cls = tf.reduce_mean(tf.reduce_sum(batch_nsp * -tf.math.log(pred_cls), axis = 1))\n",
    "        \n",
    "#         return loss_lm\n",
    "\n",
    "    \n",
    "# class BertLoss_CLS(tf.keras.losses.Loss) :\n",
    "    \n",
    "#     def __init__(self) :\n",
    "#         super(BertLoss_CLS, self).__init__()\n",
    "        \n",
    "#     def call(self, y_true, y_pred):\n",
    "#         batch_nsp = y_true\n",
    "#         pred_cls = y_pred\n",
    "        \n",
    "# #         loss_lm = tf.reduce_mean(tf.reduce_sum(tf.one_hot(batch_y, depth = vocab_size) * -tf.math.log(pred_lm), axis = 2))\n",
    "#         loss_cls = tf.reduce_mean(tf.reduce_sum(batch_nsp * -tf.math.log(pred_cls), axis = 1))\n",
    "        \n",
    "#         return loss_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(filter(lambda x: len(x['x']) <= 130, train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000\n",
    "max_seq_len = 130\n",
    "num_layers = 3\n",
    "dff = 256\n",
    "d_model = 100\n",
    "num_heads = 5\n",
    "dropout = .1\n",
    "name = 'qbert_210603'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "x = pad_sequences([ x['x'] for x in train ], max_seq_len, padding = 'post')\n",
    "y = pad_sequences([ x['label'] for x in train ] , max_seq_len, padding = 'post')\n",
    "nsp = np.asarray([ x['NSP'] for x in train ])\n",
    "\n",
    "masked_lm_weight = np.array([ ind_to_weight(x['masked_position'], max_seq_len) for x in train])\n",
    "\n",
    "mask = create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_lm = []\n",
    "\n",
    "# for i in range(len(y)) :\n",
    "#     new_lm.append({\"y\" : y[i],\n",
    "#                   \"weight\" : masked_lm_weight[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 1e-3\n",
    "batch_size = 10\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "# loss_fn = [ BertLoss_LM(), BertLoss_CLS() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainBert = BertModule(vocab_size, max_seq_len, num_layers, dff, d_model, num_heads, dropout, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainBert.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class qSeq(tf.keras.utils.Sequence) :\n",
    "    \n",
    "#     def __init__(self, x_set, mask_set, y_lm_set, y_nsp_set, masked_lm_weight, batch_size) :\n",
    "        \n",
    "#         self.x = x_set\n",
    "#         self.mask = mask_set\n",
    "#         self.y_lm = y_lm_set\n",
    "#         self.y_nsp = y_nsp_set\n",
    "#         self.masked_lm_weight = masked_lm_weight\n",
    "#         self.batch_size = batch_size\n",
    "        \n",
    "#     def __len__(self) :\n",
    "#         r = tf.math.ceil(len(self.x) / self.batch_size)\n",
    "#         return r\n",
    "    \n",
    "#     def __getitem__(self, idx) :\n",
    "#         batch_x = self.x[idx * self.batch_size : (idx+1) * batch_size]\n",
    "#         batch_mask = self.mask[idx * self.batch_size : (idx+1) * batch_size]\n",
    "#         batch_y_lm = self.y_lm[idx * self.batch_size : (idx+1) * batch_size]\n",
    "#         batch_y_nsp = self.y_nsp[idx * self.batch_size : (idx+1) * batch_size]\n",
    "#         batch_masked_lm_weight = self.masked_lm_weight[idx * self.batch_size : (idx+1) * batch_size]\n",
    "        \n",
    "#         return [{\"input\" : batch_x, \"mask\" : batch_mask},\n",
    "#                        [[batch_y_lm, batch_masked_lm_weight], batch_y_nsp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = qSeq(x, mask, y, nsp, masked_lm_weight, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 130, 32000) for input Tensor(\"WORD_LABEING_INPUT_7:0\", shape=(None, 130, 32000), dtype=float32), but it was called on an input with incompatible shape (10000, 130).\n"
     ]
    }
   ],
   "source": [
    "pretrainBert([x, mask, y, nsp, masked_lm_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9909,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(new_lm).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type dict).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-305bc164b826>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                              , x = {'input' : x,\n\u001b[0;32m      3\u001b[0m                                      'mask' : mask}\n\u001b[1;32m----> 4\u001b[1;33m                              , y = [ np.array(new_lm), nsp ])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m           model=self)\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m                **kwargs):\n\u001b[0;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1006\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1341\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    260\u001b[0m   \"\"\"\n\u001b[0;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 262\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type dict)."
     ]
    }
   ],
   "source": [
    "hist = pretrainBert.fit(batch_size = batch_size, callbacks = None, epochs = epochs\n",
    "                             , x = {'input' : x,\n",
    "                                     'mask' : mask}\n",
    "                             , y = [ np.array(new_lm), nsp ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LGCNS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model/BertPretrained-210602-130-3-100-5.pt\\assets\n"
     ]
    }
   ],
   "source": [
    "pretrainBert.save('./model/BertPretrained-210602-{}-{}-{}-{}.pt'.format(max_seq_len, num_layers, d_model, num_heads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set = train[np.random.randint(0, len(train))]\n",
    "sample_train_set = train[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_pkl('./dt/train_set_under_255.pkl')[np.random.randint(10000, 1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained('./model/BertTokenizer-6000-32000-vocab.txt'\n",
    "                                                   , strip_accents=False\n",
    "                                                   , lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 특히 국제 분쟁 조정을 위해 북한의 김일성 , 아이티 ##의 세 ##드라 ##스 장군 , 팔레 ##인 ##스타 ##인의 하마 ##스 , 보스니아 ##의 세르비아 ##계 정권 같이 미국 정부에 대해 협상을 거부 ##하면서 사태 ##의 위기를 초래 ##한 인물 및 단체를 직접 만나 분쟁 ##의 원인을 근본 ##적으로 해결하기 위해 힘썼다 . [SEP] 넓이는 46 . 80 ##10 ##km2이고 , 인구는 2015년 8월 기준으로 5 , 66 ##2명이다 . [SEP]'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_statement = ' '.join(tokenizer_for_load.convert_ids_to_tokens(sample_train_set['label']))\n",
    "train_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set = train[np.random.randint(0, len(train))]\n",
    "\n",
    "train_x = tf.reshape(sample_train_set['x'], (1, -1))\n",
    "train_x = pad_sequences(train_x, max_seq_len, padding = 'post')\n",
    "mask = create_padding_mask(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, nls = pretrainBert({\"input\" : train_x, \n",
    "                        \"mask\" : mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 130), dtype=int64, numpy=\n",
       "array([[17328, 17360, 20083,  5483,  2019, 18409,  6030, 25117,  9202,\n",
       "        14692, 26121,  3494, 27229,  4178,  2666, 25338, 13402,  1101,\n",
       "        22674, 24924,  7654, 21774,  3057, 10126, 17184, 29053,    15,\n",
       "        16914, 29689,   123, 18975,  1919, 29793, 14770, 25498, 15708,\n",
       "        22909,  9951, 27557, 13215,   965,  9987,  2350, 20155, 14333,\n",
       "         4858, 26958, 14489,  3173,  4480, 22968,  3308, 23452, 20518,\n",
       "        31073,  2973,  8616, 13132, 23579, 12193, 29040, 26558, 31106,\n",
       "        26844, 22248, 13991,  3920, 22616, 23662, 31055, 16463, 16199,\n",
       "        20434, 12297, 10795, 27195,  3488, 10549, 15916, 25133,  9855,\n",
       "         1907, 23976, 13786, 13825, 31484, 15073, 20471, 29217, 18239,\n",
       "         3095, 23133,  1123,  2396, 17538, 16958,  7579,  9146,  7860,\n",
       "        26091,  1059, 24044, 23068,  9914, 10082, 15687,  7680,  2289,\n",
       "         1450, 27952, 31394, 25124,  4905,  3505,  3409, 17645,  8407,\n",
       "        26843,   476,  1066, 17645, 30803,  2797, 20071, 31684,  3041,\n",
       "        21286,  5901, 15513, 16674]], dtype=int64)>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(lm, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.882038  , 0.11796198]], dtype=float32)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
